Directory structure:
└── agents/
    ├── README.md
    ├── BUILD
    ├── __init__.py
    ├── runners.py
    ├── telemetry.py
    ├── agents/
    │   ├── __init__.py
    │   ├── agent.py
    │   ├── base_agent.py
    │   ├── callback_context.py
    │   ├── invocation_context.py
    │   ├── langgraph_agent.py
    │   ├── live_request_queue.py
    │   ├── readonly_context.py
    │   └── remote_agent.py
    ├── artifacts/
    │   ├── __init__.py
    │   ├── base_artifact_service.py
    │   ├── gcs_artifact_service.py
    │   └── in_memory_artifact_service.py
    ├── cli/
    │   ├── __init__.py
    │   ├── __main__.py
    │   ├── cli.py
    │   ├── cli_tools_click.py
    │   ├── fast_api.py
    │   ├── web.py
    │   ├── media_streamer/
    │   │   ├── __init__.py
    │   │   └── index.html
    │   ├── ui/
    │   │   ├── __init__.py
    │   │   ├── agent_graph.py
    │   │   ├── agent_tab.py
    │   │   ├── app_context.py
    │   │   ├── artifacts_tab.py
    │   │   ├── chat.py
    │   │   ├── eval_tab.py
    │   │   ├── event_dialog.py
    │   │   ├── pending_events.py
    │   │   ├── remote_mode.py
    │   │   ├── state_tab.py
    │   │   └── streaming_component.py
    │   └── utils/
    │       ├── envs.py
    │       └── logs.py
    ├── code_executor/
    │   ├── __init__.py
    │   ├── base_code_executor.py
    │   ├── code_execution_utils.py
    │   ├── code_executor_context.py
    │   ├── unsafe_local_code_executor.py
    │   └── vertex_code_executor.py
    ├── evaluation/
    │   ├── __init__.py
    │   ├── agent_evaluator.py
    │   ├── evaluation_constants.py
    │   ├── evaluation_generator.py
    │   ├── response_evaluator.py
    │   └── trajectory_evaluator.py
    ├── events/
    │   ├── __init__.py
    │   ├── event.py
    │   └── event_actions.py
    ├── examples/
    │   ├── __init__.py
    │   ├── base_example_provider.py
    │   ├── example.py
    │   ├── example_util.py
    │   └── vertex_example_store.py
    ├── flows/
    │   ├── __init__.py
    │   ├── base_flow.py
    │   ├── loop_flow.py
    │   ├── registry.py
    │   ├── sequential_flow.py
    │   └── llm_flows/
    │       ├── __init__.py
    │       ├── _code_execution.py
    │       ├── _nl_planning.py
    │       ├── agent_transfer.py
    │       ├── auto_flow.py
    │       ├── base_llm_flow.py
    │       ├── basic.py
    │       ├── contents.py
    │       ├── examples.py
    │       ├── functions.py
    │       ├── identity.py
    │       ├── instructions.py
    │       └── single_flow.py
    ├── memory/
    │   ├── __init__.py
    │   ├── base_memory_service.py
    │   └── in_memory_memory_service.py
    ├── models/
    │   ├── __init__.py
    │   ├── anthropic_llm.py
    │   ├── base_llm.py
    │   ├── base_llm_connection.py
    │   ├── gemini_llm_connection.py
    │   ├── google_llm.py
    │   ├── llm_request.py
    │   ├── llm_response.py
    │   └── registry.py
    ├── sessions/
    │   ├── __init__.py
    │   ├── base_session_service.py
    │   ├── in_memory_session_service.py
    │   ├── postgres_session_service.py
    │   ├── session.py
    │   └── state.py
    └── tools/
        ├── __init__.py
        ├── _automatic_function_calling_util.py
        ├── agent_tool.py
        ├── async_function_tool.py
        ├── base_tool.py
        ├── crewai_tool.py
        ├── function_parameter_parse_util.py
        ├── function_tool.py
        ├── get_user_choice_tool.py
        ├── google_search_tool.py
        ├── langchain_tool.py
        ├── load_artifacts_tool.py
        ├── load_memory_tool.py
        ├── load_web_page.py
        ├── preload_memory_tool.py
        ├── tool_context.py
        ├── toolbox_tool.py
        └── retrieval/
            ├── __init__.py
            ├── base_retrieval_tool.py
            ├── files_retrieval.py
            ├── llama_index_retrieval.py
            └── vertex_rag_retrieval.py

================================================
File: README.md
================================================
# Agent Framework Developer Guide

This guide is for developers who want to contribute to the Agent Framework. You
don't have to read this if you just want to build an agent with the framework.

### General

*   Use **relative imports** for the packages and modules in the Agent
    Framework, aka in `src/agents` folder.
*   Update dependencies in `pyproject.toml`.
*   To debug in VS Code, use `Run and Debug` function. See
    [screenshot](https://screenshot.googleplex.com/7RJAop5vTcSzzhZ)

### Debug

#### Logging

In code, do the following:

```python
import logging

logger = logging.getLogger(__name__)

# ...

# prefer using lazy format, instead of f-string, which is eagerly calcualted.
logger.info("User's name: %s, email: %s", name, email)
```

Check the logs with the following command in system temp folder, e.g.
`/tmp/agent_logs` folder (the path should be printed as yellow at log setup
time, [screenshot](https://screenshot.googleplex.com/3gJRu8xxiSBT35B)):

```bash
tail -F /tmp/agent_logs/agent.debug.latest.log
```

### Testing
Testing guideline
*   Right now, we focus on integration(black box) testing.
*   Run all tests before you submit your code.
*   Add tests if it's not covered by existing tests.
*   Try to re-use existing agents for your tests. Only add new agents if the existing agents doesn't meet your needs.
*   Follow conventions that established by other tests.

#### Instructions for running tests:
run from project root dir(google3/experimental/genai/python/agents):
To run all tests,
```pytest tests/integration/```

To run one single test file:
```pytest tests/integration/test_multi_agent.py```

To run one single test method:
```pytest tests/integration/test_multi_agent.py::test_eval_agent```

(More details:
https://docs.google.com/document/d/1ecLhizMMwKdlNcRHQPboeOmRJIsHLmz1v0mBX6tqYA8/edit?tab=t.0#heading=h.by0zp5tdp0uu)

#### VS Code Setup

KEY features:

*   Auto-sort imports following google's style.
*   Auto-format on save following google's style.
*   Auto-complete for import namespaces, functions, parameters, etc.
*   Type hints.
*   Lint errors about type-mismatch, style, and other common mistakes.
*   Debugger: F5, F10 for step-by-step run.
*   Works on both gMac and Cloudtop.

TIP: this is my current setup after tuning for sometime, hoping to make it
closer to external developers. We want to polish developer experience over the
time, as well. Feel free to suggest and improve down the road.

##### Install VS Code

*   go/vscode/install
*   Google-specific extensions:
    *   go/vscode-google3
    *   go/fig-vscode (fig-vscode is already bundled with vscode-google3)

##### Start VS Code

Use `experimental/genai/python/agents` as workspace folder.

```bash
g4d -f G4_CLIENT
code experimental/genai/python/agents
```

##### Install Extensions

*   `.vscode/extensions.json` lists all recommended extension, they should
    appear in the
    [recommended section](https://screenshot.googleplex.com/6zm7UZSsYV6guid) in
    VS Code.
*   Especially remember to install
    [Gemini Code Assist + Google Cloud Code](https://marketplace.visualstudio.com/items?itemName=GoogleCloudTools.cloudcode),
    you can use `cloud-llm-preview1` as project in config.

##### User Settings

NOTE: I've configured all necessary settings in the workspace settings:
google3/experimental/genai/python/agents/.vscode/settings.json. Just in case, I
missed anything, below is my User Setting. When both configured, workspace
setting takes precedence.

```json
{
    "[jsonc]": {
        "editor.defaultFormatter": "vscode.json-language-features",
        "editor.formatOnSave": true
    },
    "[python]": {
        "editor.defaultFormatter": "ms-python.black-formatter",
        "editor.formatOnSave": true,
        "editor.formatOnSaveMode": "modifications",
        "editor.formatOnType": false
    },
    "black-formatter.args": [
        "--pyink-indentation=2",
        "--pyink-use-majority-quotes",
        "--line-length=80"
    ],
    "black-formatter.path": [
        "${env:HOME}/dev-tools/py_tools/bin/pyink"
    ],
    "cloudcode.duetAI.project": "cloud-llm-preview1",
    "cloudcode.project": "cloud-llm-preview1",
    "diffEditor.codeLens": true,
    "editor.defaultFormatter": "ms-python.black-formatter",
    "editor.insertSpaces": true,
    "editor.rulers": [
        80,
        120
    ],
    "editor.tabSize": 2,
    "explorer.autoReveal": true,
    "explorer.confirmDelete": false,
    "explorer.confirmDragAndDrop": false,
    "explorer.excludeGitIgnore": true,
    "fig.openFileIn": "codeEditor",
    "git.confirmSync": false,
    "git.enableSmartCommit": true,
    "git.openRepositoryInParentFolders": "always",
    "notebook.defaultFormatter": "ms-python.black-formatter",
    "pylint.args": [
        "--rcfile=${env:HOME}/dev-tools/google-styleguide/pylintrc"
    ],
    "pylint.path": [
        "/usr/bin/gpylint"
    ],
    "python.analysis.autoImportCompletions": true,
    "python.analysis.autoSearchPaths": false,
    "python.analysis.inlayHints.functionReturnTypes": true,
    "python.analysis.inlayHints.pytestParameters": true,
    "python.analysis.typeCheckingMode": "basic"
}
```

### Build wheel package

To build a new snapshot wheel package,

1.  Change `__version__` in
    google3/experimental/genai/python/agents/src/agents/__init__.py file. Use
    the current date after `dev`, e.g. `0.1.0.dev20241114`

1.  Run below command and the built wheel file will be in `dist/` folder:

    ```bash
    flit build
    ```

1.  Create a CL and check in.

### Deploy FastAPI Server

Under construction. Do not follow yet.

```
gcloud run deploy --source .
```

## Misc Topics

### How to connect to BigQuery in Agent

```bash
# Install bigquery package
pip install google-cloud-bigquery
# The module has to be installed to fix import error
pip install google-cloud-aiplatform
```

To access BigQuery, ensure you have "BigQuery Admin" permission. If you are not
using Vertex API, run the following:

```bash
gcloud auth login --no-launch-browser --update-adc
gcloud auth application-default set-quota-project [YOUR_PROJECT]
```

### How to set up Postgresql to store session for Agent Framework.

Set up docker

```bash
# Pull the latest Postgres container image to local image repository.
# https://hub.docker.com/_/postgres

docker pull postgres:latest

# Run a docker named `my-postgres` in detach mode with several Postgres DB environment variables and a persistent volume. Forwards traffice sent to port `5532` on host machine to port `5432` inside the container, which is the default port PostgreSQL listens.

docker run --name my-postgres -d -e POSTGRES_DB=agent  -e POSTGRES_USER=agent  -e POSTGRES_PASSWORD=agent  -e PGDATA=/var/lib/postgresql/data/pgdata -v pgvolume:/var/lib/postgresql/data -p 5532:5432 postgres
```

Verify DB connection

```bash
# Install psql client.
sudo apt install postgresql-client

# Connect to `agent` db in localhost 5532 port with username `agent`.
psql -h localhost -p 5532 -U agent -d agent
```
Create PostgresSessionService with `db_url`

The db_url should be in the format of `postgresql://<username>:<password>@domain:port/<database_name>`

e.g:

```
session_service = PostgresSessionService(db_url="postgresql://agent:agent@localhost:5532/agent")
```

## Known Issues

<!--#include file="/experimental/genai/python/agents/_known_issues.md"-->


================================================
File: BUILD
================================================
load(
    "//third_party/bazel_rules/rules_python/python:py_library.bzl",
    "py_library",
)

py_library(
    name = "agents",
    srcs = glob(
        ["**/*.py"],
        exclude = ["cli"],
    ),
    srcs_version = "PY3",
    # Doesn't work with pytype in Google3. go/pytype-unchecked-annotations#target-opt-out
    tags = ["pytype_unchecked_annotations"],
    deps = [
        "//third_party/py/PIL:pil",
        "//third_party/py/agents",
        "//third_party/py/anthropic",
        "//third_party/py/bs4",
        "//third_party/py/click",
        "//third_party/py/dotenv",
        "//third_party/py/fastapi",
        "//third_party/py/google/cloud/aiplatform/private_preview/example_stores",
        "//third_party/py/google/genai",
        "//third_party/py/graphviz",
        "//third_party/py/langchain",
        "//third_party/py/langchain_core",
        "//third_party/py/langgraph",
        "//third_party/py/llama_index",
        "//third_party/py/opentelemetry:opentelemetry_api",
        "//third_party/py/opentelemetry:opentelemetry_sdk",
        "//third_party/py/pandas:pandas_internal",
        "//third_party/py/pyaudio",
        "//third_party/py/pydantic:pydantic_v2",
        "//third_party/py/requests",
        "//third_party/py/sqlalchemy",
        "//third_party/py/tabulate",
        "//third_party/py/typing_extensions",
        "//third_party/py/tzlocal",
        "//third_party/py/uvicorn",
        "//third_party/py/websockets",
    ],
)


================================================
File: __init__.py
================================================
from .agents import Agent
from .runners import Runner

__all__ = ['Agent', 'Runner']
# version: date+base_cl
__version__ = '0.0.2.dev20250204+723246417'


================================================
File: runners.py
================================================
import random
import string
from typing import AsyncGenerator
from typing import Generator

from google.genai import types

from .agents import Agent
from .agents import InvocationContext
from .agents import LiveRequestQueue
from .artifacts import BaseArtifactService
from .artifacts import InMemoryArtifactService
from .events import Event
from .flows.llm_flows.auto_flow import AutoFlow
from .memory.base_memory_service import BaseMemoryService
from .memory.in_memory_memory_service import InMemoryMemoryService
from .sessions import BaseSessionService
from .sessions import InMemorySessionService
from .sessions import Session
from .telemetry import tracer


class Runner:

  def __init__(
      self,
      *,
      app_name: str,
      agent: Agent,
      artifact_service: BaseArtifactService | None = None,
      session_service: BaseSessionService,
      memory_service: BaseMemoryService | None = None,
      response_modalities: list[str] | None = None,
  ):
    self.app_name = app_name
    self.agent = agent
    self.artifact_service = artifact_service
    self.session_service = session_service
    self.memory_service = memory_service
    self.response_modalities = response_modalities

  @tracer.start_as_current_span('invocation')
  def run(
      self,
      *,
      session: Session,
      new_message: types.Content,
      streaming: str | None = None,
      function_call_event_id: str = '',
  ) -> Generator[Event, None, None]:
    """Runs the agent in the standard mode.

    Args:
      agent: The agent to run.
      session: The session to run the agent in.
      session_service: The session service to use to manipulate the session.
      new_message: A new message to append to the session.
      streaming: Streaming mode, None or 'server-socket'.
          None: Do not stream. 'server-socket': Stream the response with
            websockets.

    Yields:
      The events generated by the agent.
    """

    characters = string.ascii_letters + string.digits
    invocation_id = ''.join(random.choice(characters) for _ in range(8))
    root_agent = self.agent

    invocation_context = InvocationContext(
        artifact_service=self.artifact_service,
        session_service=self.session_service,
        memory_service=self.memory_service,
        invocation_id=invocation_id,
        agent=root_agent,
        session=session,
        user_content=new_message,
        streaming=streaming,
        response_modalities=self.response_modalities,
    )
    if new_message:
      # Appends only. We do not yield the event because it's not from the model.
      event = Event(
          invocation_id=invocation_id, author='user', content=new_message
      )
      if function_call_event_id:
        event.function_call_event_id = function_call_event_id
      self.session_service.append_event(session, event)
    elif root_agent.greeting_prompt:
      prompt = root_agent.greeting_prompt.format_map(session.state)
      dummy_session_service = InMemorySessionService()
      dummy_session = dummy_session_service.create(
          'dummy_app', 'dummy_user', state=session.state
      )
      dummy_session.events.append(
          Event(
              invocation_id=invocation_id,
              author='user',
              content=types.Content(
                  role='user', parts=[types.Part(text=prompt)]
              ),
          )
      )
      invocation_context.session_service = dummy_session_service
      invocation_context.session = dummy_session
      for event in root_agent.run(invocation_context):
        dummy_session_service.append_event(dummy_session, event)
        if event.author != 'user':
          event.is_greeting = True
          self.session_service.append_event(session, event)
          yield event
      return

    invocation_context.agent = self._find_agent_to_run(session, root_agent)
    for event in invocation_context.agent.run(invocation_context):
      self.session_service.append_event(session, event)
      yield event
    # TODO: move to a different place.
    if self.memory_service:
      self.memory_service.add_session(session)

  async def run_live(
      self,
      *,
      session: Session,
      live_request_queue: LiveRequestQueue,
  ) -> AsyncGenerator[Event, None]:
    """Run the server loop with a stop event."""
    # TODO: right now, only works for a single audio agent without FC.
    characters = string.ascii_letters + string.digits
    invocation_id = ''.join(random.choice(characters) for _ in range(8))
    root_agent = self.agent

    invocation_context = InvocationContext(
        artifact_service=self.artifact_service,
        session_service=self.session_service,
        memory_service=self.memory_service,
        invocation_id=invocation_id,
        agent=root_agent,
        session=session,
        user_content=None,
        streaming='bidi',
        live_request_queue=live_request_queue,
        response_modalities=self.response_modalities,
    )

    invocation_context.agent = self._find_agent_to_run(session, root_agent)
    async for event in invocation_context.agent.run_live(invocation_context):
      self.session_service.append_event(session, event)
      yield event

  def run_stateless(
      self,
      *,
      session: Session,
      new_message: types.ContentUnion,
      stream: bool = False,
  ) -> Generator[Event, None, None]:
    """Runs the agent in stateless mode.

    The agent will still yield the events but they will not be appended to the
    session.

    Args:
      agent: The agent to run.
      session: The session to run the agent in.
      new_message: A new message to append to the session.
      stream: Whether to stream the response as they are generated.

    Yields:
      The events generated by the agent.
    """
    temp_session = session.copy()
    temp_session.state = session.state.copy()
    temp_session.events = session.events.copy()

    # TODO: handle temp artifact

    temp_runner = InMemoryRunner(self.agent)
    return temp_runner.run(
        session=temp_session, new_message=new_message, stream=stream
    )

  def run_with_session_id(
      self,
      *,
      session_id: str,
      new_message: types.ContentUnion,
      stream: bool = False,
  ) -> Generator[Event, None, None]:
    """Runs the agent with the given session ID.

    Args:
      session_id: The ID of the session to run the agent in.
      new_message: A new message to append to the session.
      stream: Whether to stream the response as they are generated.

    Yields:
      The events generated by the agent.
    """
    session = self.session_service.get(session_id)
    if not session:
      raise ValueError(f'Session {session_id} not found.')
    return self.run(session=session, new_message=new_message, stream=stream)

  # When resuming from a session, we want to find the last agent in the
  # conversation to handle the new message. This is an optimization because most
  # conversations are continuous so the last agent is more suitable to continues
  # the conversation.
  #
  # We have to find an agent that has the ability to transfer to any agents in
  # the agent hierarchy. Otherwise the conversation will stuck with that agent
  # or a portion of the agent hierarchy forever. This means we need to find an
  # agent with auto flow, and all its ancestors should also have auto flow too.
  #
  # We look from bottom up in the session events to find the first agent that
  # matches the criteria. Otherwise, we fall back to the root agent.
  def _find_agent_to_run(self, session: Session, root_agent: Agent) -> Agent:
    for event in reversed(session.events):
      if event.author == 'user':
        continue
      # Found root agent.
      if event.author == root_agent.name:
        return root_agent
      agent = root_agent.find_child(event.author)
      # Agent not found, continue looking.
      if not agent:
        continue
      if self.__can_handle_new_message(agent):
        return agent
    # Falls back to root agent if no suitable agents are found in the session.
    return root_agent

  def __can_handle_new_message(self, agent: Agent) -> bool:
    while agent:
      if agent.flow != 'auto' and not isinstance(agent.flow, AutoFlow):
        return False
      agent = agent.parent_agent
    return True


class InMemoryRunner(Runner):

  def __init__(self, agent: Agent):
    super().__init__(
        app_name='InMemoryRunner',
        agent=agent,
        artifact_service=InMemoryArtifactService(),
        session_service=InMemorySessionService(),
        memory_service=InMemoryMemoryService(),
    )


================================================
File: telemetry.py
================================================
# NOTE:
#
#    We expect that the underlying GenAI SDK will provide a certain
#    level of tracing and logging telemetry aligned with Open Telemetry
#    Semantic Conventions (such as logging prompts, respones, request
#    properties, etc.) and so the information that is recorded by the
#    Vertex Agent Framework should be focused on the higher-level
#    constructs of the framework that are not observable by the SDK.

import json
from typing import Any

from google.genai import types
from opentelemetry import trace

from .agents.invocation_context import InvocationContext
from .models import LlmRequest
from .models import LlmResponse

tracer = trace.get_tracer('gcp.vertex.agent')


def trace_call_llm(
    invocation_context: InvocationContext,
    event_id: str,
    llm_request: LlmRequest,
    llm_responses: LlmResponse,
):
  span = trace.get_current_span()
  # Special standard Open Telemetry GenaI attributes that indicate
  # that this is a span related to a Generative AI system.
  span.set_attribute('gen_ai.system', 'gcp.vertex.agent')
  span.set_attribute('gen_ai.request.model', llm_request.model)
  span.set_attribute('gcp.vertex.agent.invocation_id', invocation_context.invocation_id)
  span.set_attribute('gcp.vertex.agent.event_id', event_id)
  # Consider removing once GenAI SDK provides a way to record this info.
  span.set_attribute(
      'gcp.vertex.agent.llm_request',
      json.dumps(_build_llm_request_for_trace(llm_request)),
  )
  # Consider removing once GenAI SDK provides a way to record this info.
  span.set_attribute(
      'gcp.vertex.agent.llm_response',
      llm_responses[0].model_dump_json(exclude_none=True),
  )


def trace_send_data(
    invocation_context: InvocationContext,
    event_id: str,
    data: list[types.Content],
):
  span = trace.get_current_span()
  span.set_attribute(
      'gcp.vertex.agent.invocation_id', invocation_context.invocation_id
  )
  span.set_attribute('gcp.vertex.agent.event_id', event_id)
  # Once instrumentation is added to the GenAI SDK, consider whether this
  # information still needs to be recorded by the Vertex Agent Framework.
  span.set_attribute(
      'gcp.vertex.agent.data',
      json.dumps([
          types.Content(role=content.role, parts=content.parts).model_dump(
              exclude_none=True
          )
          for content in data
      ]),
  )


def _build_llm_request_for_trace(llm_request: LlmRequest) -> dict[str, Any]:
  # Some fields in LlmRequest are function pointers and can not be serialized.
  result = {
      'model': llm_request.model,
      'config': llm_request.config.model_dump(
          exclude_none=True, exclude='response_schema'
      ),
      'contents': [],
  }
  # We do not want to send bytes data to the trace.
  for content in llm_request.contents:
    parts = [part for part in content.parts if not part.inline_data]
    result['contents'].append(
        types.Content(role=content.role, parts=parts).model_dump(
            exclude_none=True
        )
    )
  return result


================================================
File: agents/__init__.py
================================================
from .agent import Agent
from .base_agent import BaseAgent
from .invocation_context import InvocationContext
from .live_request_queue import LiveRequestQueue
from .remote_agent import RemoteAgent


================================================
File: agents/agent.py
================================================
from functools import cached_property
import logging
from typing import Any
from typing import AsyncGenerator
from typing import Callable
from typing import Generator
from typing import Literal

from google.genai import types
from pydantic import BaseModel
from pydantic import ConfigDict
from pydantic import field_validator

from ..code_executor.base_code_executor import BaseCodeExecutor
from ..events import Event
from ..examples import BaseExampleProvider
from ..examples import Example
from ..flows.base_flow import BaseFlow
from ..flows.registry import FlowRegistry
from ..models import BaseLlm
from ..models import LlmRequest
from ..models import LlmResponse
from ..telemetry import tracer
from ..tools import BaseTool
from ..tools import ToolContext
from .base_agent import BaseAgent
from .callback_context import CallbackContext
from .invocation_context import InvocationContext
from .readonly_context import ReadonlyContext

logger = logging.getLogger(__name__)


BeforeModelCallback = Callable[
    [CallbackContext, LlmRequest], LlmResponse | None
]
AfterModelCallback = Callable[
    [CallbackContext, LlmResponse],
    LlmResponse | None,
]
BeforeAgentCallback = Callable[[CallbackContext], types.Content | None]
AfterAgentCallback = Callable[[CallbackContext], types.Content | None]
BeforeToolCallback = Callable[
    [BaseTool, dict[str, Any], ToolContext],
    dict | None,
]
AfterToolCallback = Callable[
    [BaseTool, dict[str, Any], ToolContext, dict],
    dict | None,
]
InstructionProvider = Callable[[ReadonlyContext], str]

ToolUnion = Callable | BaseTool

ExamplesUnion = list[Example] | BaseExampleProvider

FlowCallable = Callable[[InvocationContext], Generator[Event, None, None]]


class Agent(BaseAgent):
  """The LLM-based agent."""

  model_config = ConfigDict(
      arbitrary_types_allowed=True,
      extra='forbid',
  )

  model: str | BaseLlm = ''
  """The model to use for the agent.

  When not set, the agent will inherit the model from its parent agent.
  """

  instruction: str | InstructionProvider = ''
  """Instructions for the LLM model, guiding the agent's behavior."""

  global_instruction: str | InstructionProvider = ''
  """Instructions for all the children agents in the entire agent tree.
  Only allowd in root agent.
  You can use this instruction to set up a stable identify or personality across all child agents."""

  flow: str | BaseFlow | FlowCallable = 'single'
  children: list[BaseAgent] = []
  tools: list[ToolUnion] = []
  generate_content_config: types.GenerateContentConfig | None = None
  examples: ExamplesUnion | None = None
  greeting_prompt: str | None = None

  planning: bool = False
  """If true, the agent will be instructed to make a plan and execute it step by
  step. Default to false.
  """

  code_executor: BaseCodeExecutor | None = None
  """If set, the agent will execute the code blocks from model responses and
  incorporate the code execution results into its final response.
  Default to None.
  """

  disable_sibling_agent_transfer: bool = False
  """If true, the agent will not be able to transfer the question to its sibling
  agents. Default to false.
  """

  input_schema: type[BaseModel] | None = None
  output_schema: type[BaseModel] | None = None
  output_key: str | None = None
  """If set, the output of the agent will be stored in the state under this key.
  """

  include_contents: Literal['default', 'none'] | None = None
  """Control the contents to include in the model request.

  'default': All contents are included.
  'none': No contents are included.
  """

  before_model_callback: BeforeModelCallback | None = None
  """Called before the model is called.

  The returned response will be returned to user and the actual model call will
  be skipped.

  Args:
    callback_context: CallbackContext
    llm_request: LlmRequest, The raw model request. Callback can mutate the request.
"""

  after_model_callback: AfterModelCallback | None = None
  """Called after the model is called.

  The returned response takes precedence over the model response.

  Args:
    callback_context: CallbackContext,
    llm_response: LlmResponse, the actual model response.
  """

  before_agent_callback: BeforeAgentCallback | None = None
  """Called before the agent is called.

  If the returned content is not None, it will be added to the session and the
  agent will not be called.

  Args:
    callback_context: CallbackContext,

  Returns:
    The content to return to the user.
  """

  after_agent_callback: AfterAgentCallback | None = None
  """Called after the agent is called.

  If the returned content is not None, it will be added to the session.

  Args:
    callback_context: CallbackContext,
  """

  before_tool_callback: BeforeToolCallback | None = None
  """Called before the tool is called.

  If you return an object, that object will be used as the tool response.
  Otherwise, the tool will be called.

  Args:
    tool: The tool to be called.
    args: The arguments to the tool.
    tool_context: ToolContext,

  Returns:
    The tool response event or None to let the framework call the tool.
  """

  after_tool_callback: AfterToolCallback | None = None
  """Called after the tool is called.

  If you return an object, that object will be used as the tool response.
  Otherwise, the tool will be called.

  Args:
    tool: The tool to be called.
    args: The arguments to the tool.
    tool_context: ToolContext,
    tool_response: The response from the tool.

  Returns:
    None
  """

  @field_validator('name', mode='after')
  @classmethod
  def validate_agent(cls, name: str) -> Any:
    if not name.isidentifier():
      raise ValueError(f'Agent name `{name}` is invalid.')
    return name

  @field_validator('generate_content_config', mode='after')
  @classmethod
  def validate_generate_content_config(
      cls, generate_content_config: types.GenerateContentConfig
  ) -> types.GenerateContentConfig:
    if not generate_content_config:
      return types.GenerateContentConfig()
    if generate_content_config.tools:
      raise ValueError('All tools must be set via Agent.tools.')
    if generate_content_config.system_instruction:
      raise ValueError('System instruction must be set via Agent.instruction.')
    if generate_content_config.response_schema:
      raise ValueError('Response schema must be set via Agent.output_schema.')
    return generate_content_config

  # Get the model name from the agent or its parent agent.
  def get_model(self):
    agent = self
    while agent:
      if agent.model:
        return agent.model
      agent = agent.parent_agent
    raise ValueError(f'No model found for agent {self.name}.')

  def find_child(self, name: str) -> BaseAgent | None:
    """Finds the child agent with the given name recursively."""
    for child in self.children:
      if child.name == name:
        return child
      # Other agent types do not have children.
      if isinstance(child, Agent):
        result = child.find_child(name)
        if result:
          return result
    return None

  def find_agent(self, name: str) -> BaseAgent | None:
    """Similar to find_child, but also check itself."""
    if self.name == name:
      return self
    return self.find_child(name)

  def model_post_init(self, __context):
    for child in self.children:
      if child.parent_agent:
        logger.error(
            'Agent %s already has a parent agent %s.',
            child.name,
            child.parent_agent.name,
        )
        raise ValueError(
            f'Agent {child.name} already has a parent agent'
            f' {child.parent_agent.name}.'
        )
      child.parent_agent = self

  def _handle_output_key(self, event, output_key, output_schema):
    """Processes the final response of an event and updates the state delta."""
    if output_key and event.is_final_response():
      result = event.content.parts[0].text
      if output_schema:
        result = output_schema.model_validate_json(result).model_dump(
            exclude_none=True
        )
      event.actions.state_delta[output_key] = result

  async def run_live(
      self,
      parent_context: InvocationContext,
  ) -> AsyncGenerator[Event, None]:
    with tracer.start_as_current_span(f'call_agent [{self.name}]'):
      # Always creates a new invocation context for the current agent.
      invocation_context = parent_context.model_copy(update={'agent': self})

      async for event in self.__resolved_flow.call_live(invocation_context):
        self._handle_output_key(event, self.output_key, self.output_schema)
        yield event
      if invocation_context.end_invocation:
        return

  def run(
      self,
      parent_context: InvocationContext,
  ) -> Generator[Event, None, None]:
    with tracer.start_as_current_span(f'call_agent [{self.name}]'):
      # Always creates a new invocation context for the current agent.
      invocation_context = parent_context.model_copy(update={'agent': self})

      # Run before_agent_callback if it exists.
      if self.before_agent_callback is not None:
        callback_context = CallbackContext(invocation_context)
        before_agent_callback_content = self.before_agent_callback(
            callback_context
        )
        if before_agent_callback_content:
          yield Event(
              invocation_id=invocation_context.invocation_id,
              author=self.name,
              content=before_agent_callback_content,
              actions=callback_context._event_actions,
          )
          return
        # TODO(wesiun): Double check below:
        #   1. UI and runner can handle this.
        #   2. session save will save this.
        if callback_context.state.has_delta():
          yield Event(
              invocation_id=invocation_context.invocation_id,
              author=self.name,
              actions=callback_context._event_actions,
          )
        if invocation_context.end_invocation:
          return

      # Run the flow.
      events = self.__resolved_flow(invocation_context)
      if events:
        for event in events:
          self._handle_output_key(event, self.output_key, self.output_schema)
          yield event
      if invocation_context.end_invocation:
        return

      # Run after_agent_callback if it exists.
      if self.after_agent_callback is not None:
        callback_context = CallbackContext(invocation_context)
        after_agent_callback_content = self.after_agent_callback(
            callback_context
        )
        if after_agent_callback_content or callback_context.state.has_delta():
          yield Event(
              invocation_id=invocation_context.invocation_id,
              author=self.name,
              content=after_agent_callback_content,
              actions=callback_context._event_actions,
          )
        if invocation_context.end_invocation:
          return

  @cached_property
  def children_dict(self) -> dict[str, BaseAgent]:
    """Returns a dict of children agents keyed by their names."""
    return {agent.name: agent for agent in self.children}

  @cached_property
  def resolved_model(self) -> str | BaseLlm:
    current_agent = self
    while current_agent:
      if current_agent.model:
        return current_agent.model
      current_agent = current_agent.parent_agent
    raise ValueError(f'No model found for agent {self.name}.')

  @cached_property
  def __resolved_flow(self) -> FlowCallable:
    if isinstance(self.flow, str):
      return FlowRegistry.new_flow(self.flow)
    if isinstance(self.flow, BaseFlow):
      return self.flow
    if isinstance(self.flow, Callable):
      return self.flow
    raise ValueError(f'Unsupported flow type: {type(self.flow)}')


================================================
File: agents/base_agent.py
================================================
from __future__ import annotations

from abc import abstractmethod
import typing
from typing import Generator

from pydantic import BaseModel
from pydantic import Field

if typing.TYPE_CHECKING:
  from ..events import Event
  from .invocation_context import InvocationContext


class BaseAgent(BaseModel):
  """Base class for all agents."""

  name: str
  """The agent's name.

  Agent name must be a unique Python identifier within the agent tree.
  """

  description: str = ''
  """One line description about the agent's capability.

  The model uses this to determine whether to delegate control to the agent.
  """

  parent_agent: BaseAgent | None = Field(default=None, init=False)
  """The parent agent in the agent tree.

  Note that one agent cannot be added to two agents' children list.
  """

  @abstractmethod
  def run(
      self,
      parent_context: InvocationContext,
  ) -> Generator[Event, None, None]:
    pass

  def get_root_agent(self) -> BaseAgent:
    """Get the root agent of the current agent."""
    root_agent = self
    while root_agent.parent_agent is not None:
      root_agent = root_agent.parent_agent
    return root_agent


================================================
File: agents/callback_context.py
================================================
from __future__ import annotations

from typing import TYPE_CHECKING

from typing_extensions import override

from .readonly_context import ReadonlyContext

if TYPE_CHECKING:
  from google.genai import types

  from ..events.event import Event
  from ..events.event_actions import EventActions
  from ..sessions.state import State
  from .invocation_context import InvocationContext


class CallbackContext(ReadonlyContext):
  """The context of various callbacks within an agent run."""

  def __init__(
      self,
      invocation_context: InvocationContext,
      *,
      event_actions: EventActions | None = None,
  ) -> None:
    super().__init__(invocation_context)

    from ..events.event_actions import EventActions
    from ..sessions.state import State

    # TODO(weisun): make this public for agent framework, but private for users.
    self._event_actions = event_actions or EventActions()
    self._state = State(
        value=invocation_context.session.state,
        delta=self._event_actions.state_delta,
    )

  @property
  @override
  def state(self) -> State:
    """The delta-aware state of the current session.

    For any state change, you can mutate this object directly,
    e.g. `ctx.state['foo'] = 'bar'`
    """
    return self._state

  @property
  def user_content(self) -> types.Content | None:
    """The user content that started this invocation. READONLY field."""
    return self._invocation_context.user_content

  @property
  def last_event(self) -> Event:
    return self._invocation_context.session.events[-1]

  def load_artifact(
      self, filename: str, version: int | None = None
  ) -> types.Part | None:
    """Loads an artifact attached to the current session.

    Args:
      filename: The filename of the artifact.
      version: The version of the artifact. If None, the latest version will be
        returned.

    Returns:
      The artifact.
    """
    if self._invocation_context.artifact_service is None:
      raise ValueError("Artifact service is not initialized.")
    return self._invocation_context.artifact_service.load(
        self._invocation_context.session.id, filename, version
    )

  def save_artifact(self, filename: str, artifact: types.Part) -> int:
    """Saves an artifact and records it as delta for the current session.

    Args:
      filename: The filename of the artifact.
      artifact: The artifact to save.

    Returns:
     The version of the artifact.
    """
    if self._invocation_context.artifact_service is None:
      raise ValueError("Artifact service is not initialized.")
    version = self._invocation_context.artifact_service.save(
        self._invocation_context.session.id, filename, artifact
    )
    self._event_actions.artifact_delta[filename] = version
    return version


================================================
File: agents/invocation_context.py
================================================
from typing import Literal

from google.genai import types
from pydantic import BaseModel
from pydantic import ConfigDict

from ..artifacts.base_artifact_service import BaseArtifactService
from ..memory.base_memory_service import BaseMemoryService
from ..sessions.base_session_service import BaseSessionService
from ..sessions.session import Session
from .base_agent import BaseAgent
from .live_request_queue import LiveRequestQueue


class InvocationContext(BaseModel):
  """An invocation context represents the data of a single invocation of an agent.

  An invocation:
    1. Starts with a user message and ends with a final response.
    2. Can contain one or multiple agent calls.
    3. Is handled by runner.run().

  An invocation runs an agent until it does not request to transfer to another
  agent.

  An agent call:
    1. Is handled by agent.run().
    2. Ends when agent.run() ends.

  An LLM agent call is an agent with a BaseLLMFlow.
  An LLM agent call can contain one or multiple steps.
  An LLM agent runs steps in a loop until:
    1. A final response is generated.
    2. The agent transfers to another agent.
    3. The end_invocation is set to true by any callbacks or tools.

  A step:
    1. Calls the LLM only once and yields its response.
    2. Calls the tools and yields their responses if requested.
  The summarization of the function response is considered another step, since
  it is another llm call.
  A step ends when it's done calling llm and tools, or if the end_invocation
  is set to true at any time.

  ```
  ┌─────────────────────── invocation ──────────────────────────┐
  ┌──────────── llm_agent_call_1 ────────────┐ ┌─ agent_call_2 ─┐
  ┌──── step_1 ────────┐ ┌───── step_2 ──────┐
  [call_llm] [call_tool] [call_llm] [transfer]
  ```
  """

  model_config = ConfigDict(
      arbitrary_types_allowed=True,
      extra='forbid',
  )

  artifact_service: BaseArtifactService | None = None
  session_service: BaseSessionService
  memory_service: BaseMemoryService | None = None

  invocation_id: str
  """The id of this invocation context. Readonly."""
  agent: BaseAgent
  """The current agent of this invocation context. Readonly."""
  user_content: types.Content | None = None
  """The user content that started this invocation. Readonly."""
  session: Session
  """The current session of this invocation context. Readonly."""
  streaming: Literal[None, '', 'server-socket', 'bidi'] = ''
  """Streaming mode, None or 'server-socket'."""

  end_invocation: bool = False
  """Whether to end this invocation.

  Set to True in callbacks or tools to terminate this invocation."""

  live_request_queue: LiveRequestQueue | None = None
  """The queue to receive live requests."""

  response_modalities: list[str] | None = None
  """The output modalities of this invocation."""

  @property
  def app_name(self) -> str:
    return self.session.app_name

  @property
  def user_id(self) -> str:
    return self.session.user_id


================================================
File: agents/langgraph_agent.py
================================================
from typing import Generator

from google.genai import types
from langgraph.graph.graph import CompiledGraph
from pydantic import ConfigDict

from ..events import Event
from .base_agent import BaseAgent
from .invocation_context import InvocationContext


class LangGraphAgent(BaseAgent):
  """Currently just a concept implementation, only supports single turn."""

  model_config = ConfigDict(
      arbitrary_types_allowed=True,
  )

  graph: CompiledGraph

  def run(
      self,
      parent_context: InvocationContext,
  ) -> Generator[Event, None, None]:
    from langchain_core.messages import HumanMessage

    messages = []
    for event in parent_context.session.events:
      if event.author == 'user':
        messages.append(HumanMessage(content=event.content.parts[0].text))

    # Use the Runnable
    final_state = self.graph.invoke({'messages': messages})
    result = final_state['messages'][-1].content

    result_event = Event(
        invocation_id=parent_context.invocation_id,
        author=self.name,
        content=types.Content(
            role='model',
            parts=[types.Part.from_text(result)],
        ),
    )
    yield result_event


================================================
File: agents/live_request_queue.py
================================================
import asyncio

from google.genai import types
from pydantic import BaseModel


class _LiveRequest(BaseModel):
  content: types.Content | None = None
  """If set, send the content to the model in turn-by-turn mode."""
  blob: types.Blob | None = None
  """If set, send the blob to the model in realtime mode."""
  close: bool = False
  """If set, close the queue. queue.shutdown() is only supported in Python 3.13+."""


class LiveRequestQueue:

  def __init__(self):
    self._queue = asyncio.Queue()

  def close(self):
    self._queue.put_nowait(_LiveRequest(close=True))

  def send_content(self, content: types.Content):
    self._queue.put_nowait(_LiveRequest(content=content))

  def send_realtime(self, blob: types.Blob):
    self._queue.put_nowait(_LiveRequest(blob=blob))

  async def get(self) -> _LiveRequest:
    return await self._queue.get()


================================================
File: agents/readonly_context.py
================================================
from __future__ import annotations

from types import MappingProxyType
from typing import Any
from typing import TYPE_CHECKING

if TYPE_CHECKING:
  from .invocation_context import InvocationContext


class ReadonlyContext:

  def __init__(
      self,
      invocation_context: InvocationContext,
  ) -> None:
    self._invocation_context = invocation_context

  @property
  def invocation_id(self) -> str:
    """The current invocation id."""
    return self._invocation_context.invocation_id

  @property
  def agent_name(self) -> str:
    """The name of the agent that is currently running."""
    return self._invocation_context.agent.name

  @property
  def state(self) -> MappingProxyType[str, Any]:
    """The state of the current session. READONLY field."""
    return MappingProxyType(self._invocation_context.session.state)


================================================
File: agents/remote_agent.py
================================================
import json
from typing import Generator

import requests

from ..events import Event
from .base_agent import BaseAgent
from .invocation_context import InvocationContext


class RemoteAgent(BaseAgent):
  url: str

  def run(
      self,
      parent_context: InvocationContext,
  ) -> Generator[Event, None, None]:
    data = {
        'invocation_id': parent_context.invocation_id,
        'session': parent_context.session.model_dump(exclude_none=True),
    }
    events = requests.post(self.url, data=json.dumps(data))
    events.raise_for_status()
    for event in events.json():
      e = Event.model_validate(event)
      e.author = self.name
      yield e


================================================
File: artifacts/__init__.py
================================================
from .base_artifact_service import BaseArtifactService
from .gcs_artifact_service import GcsArtifactService
from .in_memory_artifact_service import InMemoryArtifactService


__all__ = [
    'BaseArtifactService',
    'GcsArtifactService',
    'InMemoryArtifactService',
]


================================================
File: artifacts/base_artifact_service.py
================================================
from abc import ABC
from abc import abstractmethod

from google.genai import types


class BaseArtifactService(ABC):
  """The interface for the artifact service."""

  @abstractmethod
  def save(
      self,
      session_id: str,
      filename: str,
      artifact: types.Part,
  ) -> int:
    """Saves an artifact.

    Args:
      session_id: The session ID.
      filename: The filename of the artifact.
      artifact: The artifact to save.

    Returns:
      The revision ID.
    """

  @abstractmethod
  def load(
      self, session_id: str, filename: str, version: int | None = None
  ) -> types.Part | None:
    """Gets an artifact.

    Args:
      session_id: The session ID.
      filename: The filename of the artifact.
      version: The version of the artifact. If None, the latest version will be
        returned.

    Returns:
      The artifact or None if not found.
    """
    pass

  @abstractmethod
  def list_keys(self, session_id: str) -> list[str]:
    """Lists all the artifact filenames within a session."""
    pass

  @abstractmethod
  def delete(self, session_id: str, filename: str) -> None:
    """Deletes an artifact."""
    pass

  @abstractmethod
  def list_versions(self, session_id: str, filename: str) -> list[int]:
    """Lists all the versions of an artifact."""
    pass


================================================
File: artifacts/gcs_artifact_service.py
================================================
"""An artifact service implementation using Google Cloud Storage (GCS)."""

import logging

from google.cloud import storage
from google.genai import types

from .base_artifact_service import BaseArtifactService

logger = logging.getLogger(__name__)

# A blob name is in the format of {session_id}/{filename}/v_{version}.
_SESSION_ID_INDEX = 0
_FILENAME_INDEX = 1
_VERSION_INDEX = 2


class GcsArtifactService(BaseArtifactService):

  def __init__(self, bucket_name: str, **kwargs):
    super().__init__(**kwargs)
    self.bucket_name = bucket_name
    self.storage_client = storage.Client()
    self.bucket = self.storage_client.bucket(self.bucket_name)

  def _get_blob_name(self, session_id: str, filename: str, version: int) -> str:
    """Constructs the blob name in GCS."""
    return f"{session_id}/{filename}/v_{version}"

  def save(
      self,
      session_id: str,
      filename: str,
      artifact: types.Part,
  ) -> int:
    """Saves an artifact."""
    versions = self.list_versions(session_id, filename)
    version = 0 if not versions else max(versions) + 1

    blob_name = self._get_blob_name(session_id, filename, version)
    blob = self.bucket.blob(blob_name)

    blob.upload_from_string(
        data=artifact.inline_data.data,
        content_type=artifact.inline_data.mime_type,
    )

    return version

  def load(
      self, session_id: str, filename: str, version: int | None = None
  ) -> types.Part | None:
    """Loads an artifact from GCS."""
    if version is None:
      versions = self.list_versions(session_id, filename)
      if not versions:
        return None
      version = max(versions)

    blob_name = self._get_blob_name(session_id, filename, version)
    blob = self.bucket.blob(blob_name)

    artifact_bytes = blob.download_as_bytes()
    artifact = types.Part.from_bytes(
        data=artifact_bytes, mime_type=blob.content_type
    )
    return artifact

  def list_keys(self, session_id: str) -> list[str]:
    """Lists all artifact filenames within a session from GCS."""
    prefix = f"{session_id}/"
    blobs = self.storage_client.list_blobs(self.bucket, prefix=prefix)
    filenames = set()
    for blob in blobs:
      parts = blob.name.split("/")
      if len(parts) > _FILENAME_INDEX:
        filenames.add(parts[_FILENAME_INDEX])

    return list(filenames)

  def delete(self, session_id: str, filename: str) -> None:
    """Deletes an artifact from GCS."""
    versions = self.list_versions(session_id, filename)
    for version in versions:
      blob_name = self._get_blob_name(session_id, filename, version)
      blob = self.bucket.blob(blob_name)
      blob.delete()
    return

  def list_versions(self, session_id: str, filename: str) -> list[int]:
    """Lists all versions of an artifact from GCS."""
    prefix = self._get_blob_name(session_id, filename, "")
    blobs = self.storage_client.list_blobs(self.bucket, prefix=prefix)
    versions = []
    for blob in blobs:
      parts = blob.name.split("/")
      if len(parts) > _VERSION_INDEX:
        version = int(parts[_VERSION_INDEX].removeprefix("v_"))
        versions.append(version)
    return versions


================================================
File: artifacts/in_memory_artifact_service.py
================================================
import logging

from google.genai import types
from pydantic import BaseModel
from pydantic import Field

from ..events import Event
from .base_artifact_service import BaseArtifactService

logger = logging.getLogger(__name__)


class InMemoryArtifactService(BaseArtifactService, BaseModel):

  artifacts: dict[str, dict[str, list[types.Part]]] = Field(
      default_factory=dict
  )

  def save(
      self,
      session_id: str,
      filename: str,
      artifact: types.Part,
  ) -> int:
    if session_id not in self.artifacts:
      self.artifacts[session_id] = {}

    if filename not in self.artifacts[session_id]:
      self.artifacts[session_id][filename] = []

    version = len(self.artifacts[session_id][filename])
    self.artifacts[session_id][filename].append(artifact)
    return version

  def load(
      self, session_id: str, filename: str, version: int | None = None
  ) -> types.Part | None:
    if not self.artifacts.get(session_id):
      return None

    versions = self.artifacts[session_id].get(filename)
    if not versions:
      return None

    if version is None:
      version = -1
    return versions[version]

  def list_keys(self, session_id: str) -> list[str]:
    if not self.artifacts.get(session_id):
      return []

    return list(self.artifacts[session_id].keys())

  def delete(self, session_id: str, filename: str) -> None:
    if not self.artifacts.get(session_id):
      return None
    self.artifacts[session_id].pop(filename, None)


  def list_versions(
      self, session_id: str, filename: str
  ) -> list[int]:
    if not self.artifacts.get(session_id):
      return []

    versions = self.artifacts[session_id][filename]
    return list(range(len(versions)))


================================================
File: cli/__init__.py
================================================
from .cli_tools_click import main


================================================
File: cli/__main__.py
================================================
from .cli_tools_click import main

if __name__ == '__main__':
  main()


================================================
File: cli/cli.py
================================================
from datetime import datetime
import importlib
import os
import sys

from agents import Agent
from agents import Runner
from agents.artifacts import BaseArtifactService
from agents.artifacts import InMemoryArtifactService
from agents.sessions import BaseSessionService
from agents.sessions import InMemorySessionService
from agents.sessions import Session
from google.genai import types
from pydantic import BaseModel

from .utils import envs


class InputFile(BaseModel):
  state: dict[str, object]
  queries: list[str]


def run_input_file(
    app_name: str,
    root_agent: Agent,
    artifact_service: BaseArtifactService,
    session: Session,
    session_service: BaseSessionService,
    input_path: str,
) -> None:
  runner = Runner(
      app_name=app_name,
      agent=root_agent,
      artifact_service=artifact_service,
      session_service=session_service,
  )

  # Reads from the input json file.
  with open(input_path, 'r') as f:
    input_file = InputFile.model_validate_json(f.read())
  input_file.state['_time'] = datetime.now()

  session.state = input_file.state
  for query in input_file.queries:
    print('user: ', query)
    content = types.Content(role='user', parts=[types.Part(text=query)])
    for event in runner.run(session=session, new_message=content):
      if event.is_final_response():
        print(event.content.parts[0].text)


def run_interactively(
    app_name: str,
    root_agent: Agent,
    artifact_service: BaseArtifactService,
    session: Session,
    session_service: BaseSessionService,
) -> None:
  runner = Runner(
      app_name=app_name,
      agent=root_agent,
      artifact_service=artifact_service,
      session_service=session_service,
  )
  while True:
    query = input('user: ')
    if query == 'exit':
      break
    for event in runner.run(
        session=session,
        new_message=types.Content(role='user', parts=[types.Part(text=query)]),
    ):
      if event.is_final_response():
        print(f'[{event.author}]: {event.content.parts[0].text}')


def run_cli(
    *,
    agent_parent_dir: str,
    agent_folder_name: str,
    json_file_path: str | None,
    save_session: bool,
) -> None:
  """Runs an interactive CLI for a certain agent.

  Args:
    agent_parent_dir: str, the absolute path of the parent folder of the agent
      folder.
    agent_folder_name: str, the name of the agent folder.
    json_file_path: str | None, the absolute path to the json file, either
      *.input.json or *.session.json.
    save_session: bool, whether to save the session on exit.
  """
  if agent_parent_dir not in sys.path:
    sys.path.append(agent_parent_dir)

  artifact_service = InMemoryArtifactService()
  session_service = InMemorySessionService()
  session = session_service.create(agent_folder_name, 'test_user')

  agent_module_path = os.path.join(agent_parent_dir, agent_folder_name)
  agent_module = importlib.import_module(agent_folder_name)
  root_agent = agent_module.agent.root_agent
  envs.load_dotenv_for_agent(agent_folder_name, agent_parent_dir)
  if json_file_path:
    if json_file_path.endswith('.input.json'):
      run_input_file(
          app_name=agent_folder_name,
          root_agent=root_agent,
          artifact_service=artifact_service,
          session=session,
          session_service=session_service,
          input_path=json_file_path,
      )
    elif json_file_path.endswith('.session.json'):
      with open(json_file_path, 'r') as f:
        session = Session.model_validate_json(f.read())
      for content in session.get_contents():
        if content.role == 'user':
          print('user: ', content.parts[0].text)
        else:
          print(content.parts[0].text)
      run_interactively(root_agent, artifact_service, session, session_service)
    else:
      print(f'Unsupported file type: {json_file_path}')
      exit(1)
  else:
    print(f'Running agent {root_agent.name}, type exit to exit.')
    run_interactively(
        agent_folder_name,
        root_agent,
        artifact_service,
        session,
        session_service,
    )

  if save_session:
    if json_file_path:
      session_path = json_file_path.replace('.input.json', '.session.json')
    else:
      session_id = input('Session ID to save: ')
      session_path = f'{agent_module_path}/{session_id}.session.json'
    with open(session_path, 'w') as f:
      f.write(session.model_dump_json(indent=2, exclude_none=True))
    # TODO: Save from opentelemetry.
    # logs_path = session_path.replace('.session.json', '.logs.json')
    # with open(logs_path, 'w') as f:
    #   f.write(
    #       session.model_dump_json(
    #           indent=2, exclude_none=True, include='event_logs'
    #       )
    #   )
    print('Session saved to', session_path)


================================================
File: cli/cli_tools_click.py
================================================
import os
import shlex
import subprocess
import sys

import click
import streamlit.web.bootstrap as bootstrap
from dotenv import load_dotenv

from .cli import run_cli
from .fast_api import run_fast_api
from .utils import logs


@click.group()
def main():
  """Agent Framework CLI tools."""
  pass


@main.command("run")
@click.option(
    "--json_file",
    type=click.Path(
        exists=True, dir_okay=False, file_okay=True, resolve_path=True
    ),
    help=(
        "The json file in the agent folder, either a *.input.json or"
        " *.session.json."
    ),
)
@click.option(
    "--save_session",
    type=bool,
    is_flag=True,
    show_default=True,
    default=False,
    help="Whether to save the session to a json file on exit.",
)
@click.argument(
    "agent",
    type=click.Path(
        exists=True, dir_okay=True, file_okay=False, resolve_path=True
    ),
)
def cli_run(agent: str, json_file: str, save_session: bool):
  """Run an interactive CLI for a certain agent."""
  logs.log_to_tmp_folder()

  agent_parent_folder = os.path.dirname(agent)
  agent_folder_name = os.path.basename(agent)

  run_cli(
      agent_parent_dir=agent_parent_folder,
      agent_folder_name=agent_folder_name,
      json_file_path=json_file,
      save_session=save_session,
  )


@main.command()
@click.option(
    "--launch_browser",
    is_flag=True,
    show_default=True,
    default=False,
    help="Whether to launch browser automatically.",
)
@click.option(
    "--log_to_tmp",
    is_flag=True,
    show_default=True,
    default=False,
    help=(
        "Whether to log to system temp folder instead of console. This is"
        " useful for local debugging."
    ),
)
@click.option("--session_db_url", help="The database URL to store the session.")
@click.argument(
    "agent_dir",
    type=click.Path(
        exists=True, dir_okay=True, file_okay=False, resolve_path=True
    ),
    default=os.getcwd(),
)
def web(
    agent_dir: str,
    launch_browser: bool,
    log_to_tmp: bool,
    session_db_url: str = "",
):
  """Start the web app server to test multiple agents.

  AGENT_DIR: the directory of agents, where each sub-directory is a single
  agent. By default, it is the current working directory.
  """
  if log_to_tmp:
    logs.log_to_tmp_folder()
  else:
    logs.log_to_stderr()

  app_script_path = os.path.join(os.path.dirname(__file__), "web.py")

  flag_options = {"server.headless": not launch_browser}
  bootstrap.load_config_options(flag_options)
  bootstrap.run(
      app_script_path,
      False,
      [agent_dir, session_db_url],
      flag_options,
  )


@main.command("test")
@click.argument(
    "test_folder",
    type=click.Path(
        exists=True, dir_okay=True, file_okay=False, resolve_path=True
    ),
)
def test_folder(test_folder: str):
  """Run all tests in a specified folder recursively."""
  # Convert the test folder to a relative Python module path
  base_dir = os.getcwd()
  rel_path = os.path.relpath(test_folder, start=base_dir)
  module_name = rel_path.replace(
      os.sep, "."
  )  # Convert path to module name format
  click.echo(f"Module name derived: {module_name}")

  # Add the project root to sys.path to ensure absolute imports
  project_root = os.path.abspath(base_dir)
  if project_root not in sys.path:
    sys.path.append(project_root)

  logs.log_to_stderr()
  load_dotenv(override=True, verbose=True)

  # Find all `.test.json` files recursively in the folder
  test_paths = []
  for root, _, files in os.walk(test_folder):
    for file in files:
      if file.endswith(".test.json"):
        test_paths.append(os.path.join(root, file))

  # Add folder itself for evaluation
  if test_paths:
    test_paths.append(test_folder)

  if not test_paths:
    click.echo(f"No test files or folders found in: {test_folder}")
    return

  click.echo(f"Found {len(test_paths)} tests in the folder: {test_folder}")

  # Results list to collect pass/fail information
  results = []

  # Run tests and display results
  for path in test_paths:
    if os.path.isdir(path):
      click.echo(f"Running tests in folder: {path}")
    else:
      click.echo(f"Running test file: {path}")
    try:
      from ..evaluation.agent_evaluator import AgentEvaluator

      result = AgentEvaluator.evaluate(module_name, path)
      click.echo(f"\u2705 {path}: {result}")
      results.append({"path": path, "status": "Passed", "details": result})
    except Exception as e:
      click.echo(f"\u274c {path}: Error - {e}")
      results.append({"path": path, "status": "Failed", "details": str(e)})

  # Display a summary after all tests are run
  click.echo("\nTest Summary:")
  total_tests = len(results)
  passed_tests = len([r for r in results if r["status"] == "Passed"])
  failed_tests = total_tests - passed_tests

  click.echo(f"Total tests run: {total_tests}")
  click.echo(f"\u2705 Passed: {passed_tests}")
  click.echo(f"\u274c Failed: {failed_tests}")


@main.command("api_server")
@click.option(
    "--log_to_tmp",
    is_flag=True,
    show_default=True,
    default=False,
    help=(
        "Whether to log to system temp folder instead of console. This is"
        " useful for local debugging."
    ),
)
@click.option("--session_db_url", help="The database URL to store the session.")
@click.argument(
    "agent",
    type=click.Path(
        exists=True, dir_okay=True, file_okay=False, resolve_path=True
    ),
)
def cli_api_server(agent: str, log_to_tmp: bool, session_db_url: str = ""):
  """Start a FastAPI server for a certain agent."""
  if log_to_tmp:
    logs.log_to_tmp_folder()
  else:
    logs.log_to_stderr()

  agent_parent_folder = os.path.dirname(agent)
  agent_folder_name = os.path.basename(agent)

  run_fast_api(
      app_name=agent_folder_name,
      agent_dir=agent_parent_folder,
      agent_name=agent_folder_name,
      session_db_url=session_db_url,
  )


@main.command("deploy")
@click.option("--cloud_project_name", type=str)
@click.option("--service_name", type=str)
@click.option("--agent_name", type=str)
def cli_deploy(cloud_project_name: str, service_name: str, agent_name: str):
  """Deploy a FastAPI server for a certain agent to Cloud Run."""
  command = (
      f"gcloud run deploy {service_name} --source . --region=us-central1"
      " --no-allow-unauthenticated --set-env-vars"
      f" GOOGLE_GENAI_USE_VERTEXAI=1,GOOGLE_CLOUD_PROJECT={cloud_project_name},"
      f"AGENT_NAME={agent_name},GOOGLE_CLOUD_LOCATION=us-central1 --port=8000"
  )
  subprocess.run(shlex.split(command))


================================================
File: cli/fast_api.py
================================================
import importlib
import logging
import sys
from typing import Any
from typing import Optional

from fastapi import FastAPI
from fastapi import HTTPException
from google.genai import types
from pydantic import BaseModel
from uvicorn.main import run as uvicorn_run

from ..agents import Agent
from ..artifacts import InMemoryArtifactService
from ..events import Event
from ..runners import Runner
from ..sessions.in_memory_session_service import InMemorySessionService
from ..sessions.postgres_session_service import PostgresSessionService
from ..sessions.session import Session
from .utils import envs

logger = logging.getLogger(__name__)


class AgentRunRequest(BaseModel):
  app_name: str
  user_id: str
  session_id: str
  new_message: types.Content


def run_fast_api(
    *,
    app_name: str,
    agent_dir: str,
    agent_name: str,
    session_db_url: str = "",
) -> None:
  """Runs the FastAPI server."""
  if agent_dir not in sys.path:
    sys.path.append(agent_dir)

  agent_module = importlib.import_module(agent_name)
  root_agent: Agent = agent_module.agent.root_agent
  envs.load_dotenv_for_agent(agent_name, agent_dir)

  artifact_service = InMemoryArtifactService()
  if not session_db_url:
    session_service = InMemorySessionService()
  else:
    session_service = PostgresSessionService(db_url=session_db_url)
  app = FastAPI()

  runner = Runner(
      app_name=app_name,
      agent=root_agent,
      artifact_service=artifact_service,
      session_service=session_service,
  )

  @app.get(
      "/apps/{app_name}/users/{user_id}/sessions/{session_id}",
      response_model_exclude_none=True,
  )
  def get_session(app_name: str, user_id: str, session_id: str) -> Session:
    session = session_service.get(app_name, user_id, session_id)
    if not session:
      raise HTTPException(status_code=404, detail="Session not found")
    return session

  @app.get(
      "/apps/{app_name}/users/{user_id}/sessions",
      response_model_exclude_none=True,
  )
  def list_sessions(app_name: str, user_id: str) -> list[str]:
    return session_service.list_sessions(app_name, user_id).session_ids

  @app.post(
      "/apps/{app_name}/users/{user_id}/sessions/{session_id}",
      response_model_exclude_none=True,
  )
  def create_session(
      app_name: str,
      user_id: str,
      session_id: str,
      state: Optional[dict[str, Any]] = None,
  ) -> Session:
    if session_service.get(app_name, user_id, session_id) is not None:
      logger.warning("Session already exists: %s", session_id)
      raise HTTPException(
          status_code=400, detail=f"Session already exists: {session_id}"
      )

    logger.info("New session created: %s", session_id)
    return session_service.create(
        app_name, user_id, state, session_id=session_id
    )

  @app.delete("/apps/{app_name}/users/{user_id}/sessions/{session_id}")
  def delete_session(app_name: str, user_id: str, session_id: str):
    session_service.delete(app_name, user_id, session_id)

  @app.post("/agent/run", response_model_exclude_none=True)
  def agent_run(req: AgentRunRequest) -> list[Event]:
    # TODO: Implement SSE for event generator.
    session = session_service.get(req.app_name, req.user_id, req.session_id)
    if not session:
      raise HTTPException(status_code=404, detail="Session not found")

    events = list(runner.run(session=session, new_message=req.new_message))
    logger.info("Generated %s events in agent run: %s", len(events), events)
    return events

  uvicorn_run(app, host="0.0.0.0", log_config=None)


================================================
File: cli/web.py
================================================
import logging
import os
import typing

# from agents.cli.ui import live_tab
from agents.cli.ui import agent_tab
from agents.cli.ui import app_context
from agents.cli.ui import artifacts_tab
from agents.cli.ui import chat
from agents.cli.ui import eval_tab
from agents.cli.ui import state_tab
from opentelemetry import trace
from opentelemetry.exporter.cloud_trace import CloudTraceSpanExporter
from opentelemetry.sdk.trace import export
from opentelemetry.sdk.trace import ReadableSpan
from opentelemetry.sdk.trace import TracerProvider
import streamlit as st


class SessionStateSpanExporter(export.SpanExporter):

  def export(
      self, spans: typing.Sequence[ReadableSpan]
  ) -> export.SpanExportResult:
    for span in spans:
      if span.name == 'call_llm' or span.name == 'send_data':
        attributes = dict(span.attributes)
        attributes['trace_id'] = span.get_span_context().trace_id
        attributes['span_id'] = span.get_span_context().span_id
        st.session_state.call_llm_spans[
            attributes['gcp.vertex.agent.event_id']
        ] = attributes
    return export.SpanExportResult.SUCCESS

  def force_flush(self, timeout_millis: int = 30000) -> bool:
    return True


provider = TracerProvider()
provider.add_span_processor(
    export.SimpleSpanProcessor(SessionStateSpanExporter())
)

if os.environ.get('AF_TRACE_TO_CLOUD', '0') == '1':
  processor = export.BatchSpanProcessor(
      CloudTraceSpanExporter(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])
  )
  provider.add_span_processor(processor)

trace.set_tracer_provider(provider)

st.set_page_config(
    page_title='GenAI Agent Demos',
    layout='wide',
)


logger = logging.getLogger(__name__)


# Makes dialog bigger.
st.markdown(
    """<style>
/* Wider dialog. */
div[role="dialog"] {
    width: 80%;
}
/* Remove extra top padding in the sidebar. */
div[data-testid="stSidebarHeader"] {
    height: 16px;
    padding: 0;
}
/* Horizontal layout for function call steps. */
div[class*="st-key-row_layout_"] {
    flex-direction: row !important;
    flex-wrap: wrap;
    gap: 8px;
}
div[class*="st-key-row_layout_"] div {
    width: max-content !important;
}
/* Chat history row layout. */
div[class*="st-key-chat_row_"] {
    flex-direction: row !important;
    flex-wrap: nowrap;
}
div[class*="st-key-chat_row_"] div:last-child {
    flex: 1;
}
/* Smaller step buttons. */
div[class*="st-key-row_layout_fc_"] button {
    min-height: 0 !important;
}
div[class*="st-key-row_layout_fc_"] p {
    font-size: 12px !important;
}
/* Smaller json view. */
.react-json-view {
    font-size: 12px !important;
}
/* Fixed width font for state edit. */
.st-key-state_json textarea {
    font-family: "Source Code Pro", monospace;
    font-size: 14px;
}
/* Show agent initial on the avatar button. */
div[class*="st-key-avatar_"] button {
    position: relative;
}
div[class*="st-key-avatar_"] button > span {
    margin-right: 4px;
}
div[class*="st-key-avatar_"] button > div {
    position: absolute;
    top: 0;
    right: 6px;
    font-size: 12px;
}
</style>""",
    unsafe_allow_html=True,
)


app_context.setup_session_state()

st.sidebar.subheader('Agent Framework Dev UI')
agent_panel, state_panel, artifacts_panel, eval_panel = st.sidebar.tabs(
    ['Agent', 'State', 'Artifacts', 'Eval']
)

with agent_panel:
  agent_tab.render()
# Renders chat before other tabs so they can be updated by chat.
if (
    st.session_state.demo_name
    and st.session_state.demo_name
    != app_context.CONNECT_REMOTE_SERVER_DEMO_NAME
):
  chat.render_chat()
with state_panel:
  state_tab.render()
with artifacts_panel:
  artifacts_tab.render()
with eval_panel:
  eval_tab.render()


================================================
File: cli/media_streamer/__init__.py
================================================
import streamlit.components.v1 as components

media_streamer = components.declare_component(
    name='media_streamer', path='./media_streamer'
)


================================================
File: cli/media_streamer/index.html
================================================
<html>
  <head>
    <style type="text/css">
      body {
        font-family: sans-serif;
        padding: 10px;
      }
      button {
        font-size: 16px;
        padding: 10px 15px;
        margin-right: 10px;
      }
      #status {
        font-weight: bold;
      }
      /* Mirror the preview video */
      #videoPreview {
        border: 1px solid #ccc;
        transform: scaleX(-1);
      }
    </style>
  </head>
  <body>
    <h2>Audio &amp; Video Recorder Component</h2>
    <!-- Video preview -->
    <video id="videoPreview" autoplay muted playsinline width="320" height="240"></video>
    <br/>
    <!-- Unified start/stop buttons -->
    <button id="startButton" onclick="startRecording()">Start Streaming</button>
    <button id="stopButton" onclick="stopRecording()" disabled>Stop Streaming</button>
    <span id="status">Idle</span>

    <script type="text/javascript">
      // --- Streamlit component lifecycle constants ---
      const SET_COMPONENT_VALUE = "streamlit:setComponentValue";
      const COMPONENT_READY = "streamlit:componentReady";
      const SET_FRAME_HEIGHT = "streamlit:setFrameHeight";
      const RENDER = "streamlit:render";

      // Helper function to send messages to the Streamlit host.
      function _sendMessage(type, data) {
        const outboundData = Object.assign({ isStreamlitMessage: true, type: type }, data);
        window.parent.postMessage(outboundData, "*");
      }
      function setFrameHeight(height) {
        _sendMessage(SET_FRAME_HEIGHT, { height: height });
      }
      // This function sends the combined data.
      function notifyHost(data) {
        _sendMessage(SET_COMPONENT_VALUE, data);
      }
      function initialize() {
        _sendMessage(COMPONENT_READY, { apiVersion: 1 });
        window.addEventListener("load", function() {
          setTimeout(() => setFrameHeight(document.documentElement.clientHeight), 0);
        });
      }
      initialize();

      // --- Global Variables ---
      let isRecording = false;
      let audioContext;
      let scriptProcessor;
      let audioAccumulatedChunks = [];
      let audioFlushIntervalId = null;
      let videoCaptureIntervalId = null;
      const SAMPLE_RATE = 16000;
      const BUFFER_SIZE = 4096;
      let mediaStream = null; // for video (and optionally audio if needed)

      // Global object to hold combined recording data.
      let recordingData = {};

      // Instead of calling notifyHost directly, update our combined data and send.
      function sendCombinedUpdate(key, value) {
        recordingData[key] = value;
        notifyHost({
          value: recordingData,
          dataType: "json"
        });
      }

      const videoElement = document.getElementById("videoPreview");
      const statusSpan = document.getElementById("status");
      const startButton = document.getElementById("startButton");
      const stopButton = document.getElementById("stopButton");

      // --- Start Recording: Capture both audio and video ---
      async function startRecording() {
        if (isRecording) return;
        isRecording = true;
        startButton.disabled = true;
        stopButton.disabled = false;
        statusSpan.innerText = "Streaming...";

        // --- Setup audio recording ---
        try {
          audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: SAMPLE_RATE });
          let audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          let mediaStreamSource = audioContext.createMediaStreamSource(audioStream);
          scriptProcessor = audioContext.createScriptProcessor(BUFFER_SIZE, 1, 1);
          scriptProcessor.onaudioprocess = function(event) {
            let inputBuffer = event.inputBuffer;
            let inputData = inputBuffer.getChannelData(0);
            let pcmData = new Int16Array(inputData.length);
            for (let i = 0; i < inputData.length; i++) {
              let s = Math.max(-1, Math.min(1, inputData[i]));
              pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            audioAccumulatedChunks.push(pcmData);
          };
          mediaStreamSource.connect(scriptProcessor);
          scriptProcessor.connect(audioContext.destination);
        } catch (err) {
          console.error("Error accessing microphone:", err);
          statusSpan.innerText = "Audio Error: " + err;
        }

        // --- Setup video recording ---
        try {
          let videoStream = await navigator.mediaDevices.getUserMedia({ video: true });
          videoElement.srcObject = videoStream;
          // Save the video stream so we can stop it later.
          mediaStream = videoStream;
        } catch (err) {
          console.error("Error accessing camera:", err);
          statusSpan.innerText = "Video Error: " + err;
        }

        // --- Start interval timers for flushing audio and capturing video ---
        videoCaptureIntervalId = setInterval(captureVideoFrame, 250); // every 250ms
        audioFlushIntervalId = setInterval(flushAudioChunks, 250);  // every 250ms
      }

      // --- Stop Recording ---
      function stopRecording() {
        if (!isRecording) return;
        isRecording = false;
        startButton.disabled = false;
        stopButton.disabled = true;
        statusSpan.innerText = "Stopped.";

        // Stop audio processing
        if (scriptProcessor) {
          scriptProcessor.disconnect();
          scriptProcessor = null;
        }
        if (audioContext) {
          audioContext.close();
          audioContext = null;
        }
        if (audioFlushIntervalId) {
          clearInterval(audioFlushIntervalId);
          audioFlushIntervalId = null;
        }

        // Stop video stream
        if (mediaStream) {
          mediaStream.getTracks().forEach(track => track.stop());
          mediaStream = null;
        }
        if (videoCaptureIntervalId) {
          clearInterval(videoCaptureIntervalId);
          videoCaptureIntervalId = null;
        }

        // Flush any remaining audio chunks
        flushAudioChunks();

        // Final notification that recording has stopped.
        sendCombinedUpdate("message", "Recording stopped");
        sendCombinedUpdate("isRecording", false);
      }

      // --- Flush accumulated audio chunks ---
      function flushAudioChunks() {
        if (audioAccumulatedChunks.length === 0) return;
        // Combine all audio chunks into one array.
        let totalLength = audioAccumulatedChunks.reduce((sum, chunk) => sum + chunk.length, 0);
        let combined = new Int16Array(totalLength);
        let offset = 0;
        for (let chunk of audioAccumulatedChunks) {
          combined.set(chunk, offset);
          offset += chunk.length;
        }
        audioAccumulatedChunks = [];  // clear accumulator

        // Convert combined PCM data to a Blob.
        let blob = new Blob([combined.buffer], { type: 'application/octet-stream' });
        let reader = new FileReader();
        reader.onloadend = function() {
          // Instead of sending only audio, update the combined data.
          sendCombinedUpdate("audioChunk", reader.result);
        };
        reader.readAsDataURL(blob);
      }

      // --- Capture a single video frame ---
      function captureVideoFrame() {
        if (!videoElement.srcObject) return;
        const canvas = document.createElement("canvas");
        canvas.width = videoElement.videoWidth || 320;
        canvas.height = videoElement.videoHeight || 240;
        const ctx = canvas.getContext("2d");
        ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
        let dataUrl = canvas.toDataURL("image/jpeg");
        // Instead of a separate notifyHost call, update the combined data.
        sendCombinedUpdate("videoFrame", dataUrl);
      }
    </script>
  </body>
</html>


================================================
File: cli/ui/agent_graph.py
================================================
from typing import Callable

import graphviz

from agents.agents import BaseAgent
from agents.tools import BaseTool
from agents.tools.agent_tool import AgentTool
from agents.tools.retrieval.base_retrieval_tool import BaseRetrievalTool


def build_graph(graph, agent, highlight_pairs):
  dark_green = '#6aa84f'
  light_green = '#d9ead3'

  def get_node_name(tool_or_agent):
    if isinstance(tool_or_agent, BaseAgent):
      return tool_or_agent.name
    elif isinstance(tool_or_agent, Callable):
      return tool_or_agent.__name__
    elif isinstance(tool_or_agent, BaseTool):
      return tool_or_agent.name
    else:
      raise ValueError(f'Unsupported tool type: {tool_or_agent}')

  def get_node_caption(tool_or_agent):
    if isinstance(tool_or_agent, BaseAgent):
      return '🤖 ' + tool_or_agent.name
    elif isinstance(tool_or_agent, Callable):
      return '🔧 ' + tool_or_agent.__name__
    elif isinstance(tool_or_agent, BaseRetrievalTool):
      return '🔎 ' + tool_or_agent.name
    elif isinstance(tool_or_agent, AgentTool):
      return '🤖 ' + tool_or_agent.name
    elif isinstance(tool_or_agent, BaseTool):
      return '🔧 ' + tool_or_agent.name
    else:
      raise ValueError(f'Unsupported tool type: {type(tool)}')

  def get_node_shape(tool_or_agent):
    if isinstance(tool_or_agent, BaseAgent):
      return 'ellipse'
    elif isinstance(tool_or_agent, Callable):
      return 'box'
    elif isinstance(tool_or_agent, BaseRetrievalTool):
      return 'cylinder'
    elif isinstance(tool_or_agent, BaseTool):
      return 'box'
    else:
      raise ValueError(f'Unsupported tool type: {type(tool_or_agent)}')

  def draw_node(tool_or_agent):
    name = get_node_name(tool_or_agent)
    shape = get_node_shape(tool_or_agent)
    caption = get_node_caption(tool_or_agent)
    if highlight_pairs:
      for highlight_tuple in highlight_pairs:
        if name in highlight_tuple:
          graph.node(
              name,
              caption,
              style='filled,rounded',
              fillcolor=light_green,
              color=dark_green,
              shape=shape,
          )
          return
    # if not in highlight, draw non-highliht node
    graph.node(name, caption, shape=shape, style='rounded')

  def draw_edge(from_name, to_name):
    if highlight_pairs:
      for highlight_from, highlight_to in highlight_pairs:
        if from_name == highlight_from and to_name == highlight_to:
          graph.edge(from_name, to_name, color=dark_green)
          return
        elif from_name == highlight_to and to_name == highlight_from:
          graph.edge(from_name, to_name, color=dark_green, dir='back')
          return
    # if no need to highlight, color none
    graph.edge(from_name, to_name, arrowhead='none')

  draw_node(agent)
  if hasattr(agent, 'children'):
    for child in agent.children:
      build_graph(graph, child, highlight_pairs)
      draw_edge(agent.name, child.name)
  if hasattr(agent, 'tools'):
    for tool in agent.tools:
      draw_node(tool)
      draw_edge(agent.name, get_node_name(tool))


def get_agent_graph(root_agent, highlights_pairs):
  print('build graph')
  graph = graphviz.Digraph(graph_attr={'rankdir': 'LR'})
  build_graph(graph, root_agent, highlights_pairs)
  return graph


================================================
File: cli/ui/agent_tab.py
================================================
import copy
import importlib
import json
import logging
import os
import re

import pandas as pd
import streamlit as st

from agents import Agent
from agents.cli.utils import envs
from agents.sessions import Session

from . import agent_graph
from . import app_context
from . import pending_events
from . import remote_mode

logger = logging.getLogger(__name__)

_ENV_VARIABLES: dict[str, str] = {
    'GOOGLE_GENAI_USE_VERTEXAI': (
        'Choose model provider: 0 for Google AI, 1 for Vertex.'
    ),
    'GOOGLE_API_KEY': (
        'Google API Key, required if GOOGLE_GENAI_USE_VERTEXAI=0.'
    ),
    'GOOGLE_CLOUD_PROJECT': (
        'Google Cloud Project ID, required if GOOGLE_GENAI_USE_VERTEXAI=1.'
    ),
    'GOOGLE_CLOUD_LOCATION': (
        'Google Cloud Location, required if GOOGLE_GENAI_USE_VERTEXAI=1.'
    ),
    'AF_TRACE_TO_CLOUD': (
        'Whether to export trace to Google Cloud. 1 for yes, otherwise no.'
    ),
}
"""Key environment variables and the descriptions."""


def on_demo_name_change():
  demo_name: str | None = st.session_state.demo_name
  if app_context.is_remote_mode():
    st.query_params.app = demo_name
    reset_session()
    logger.info('remote agent is connected.')
  elif demo_name == app_context.CONNECT_REMOTE_SERVER_DEMO_NAME:
    connect_remote_server()
  elif demo_name:
    st.query_params.app = demo_name
    import_agent_module()
    reset_session()
  else:
    st.query_params.pop('app', None)


def on_streaming_change():
  if st.session_state.streaming == 'server-socket':
    st.toast('Server socket streaming is always using Gemini 2.0')
    st.session_state.response_modalities = ['TEXT']
  elif st.session_state.streaming == 'bidi':
    st.toast('Bidi streaming is always using Gemini 2.0')
    st.session_state.response_modalities = ['AUDIO']


# Using fragment so switching to server streaming doesn't rerun the page.
@st.fragment
def render_streaming_options():
  streaming_options = {
      '': 'None',
      'server-socket': 'Server Socket',
      'bidi': 'Bidi',
  }
  st.segmented_control(
      'Streaming',
      options=streaming_options.keys(),
      format_func=lambda option: streaming_options[option],
      selection_mode='single',
      key='streaming',
      on_change=on_streaming_change,
  )

  if st.session_state.streaming:
    st.multiselect(
        'Response Modalities',
        ['TEXT', 'AUDIO'],
        key='response_modalities',
    )
  if st.session_state.streaming == 'bidi':
    if not st.session_state.is_live:
      if st.button('Start Streaming'):
        st.session_state.is_live = True
        st.rerun()
    else:
      if st.button('Stop Streaming'):
        st.session_state.is_live = False
        st.rerun()


def render():
  if not st.session_state.demo_name and 'app' in st.query_params:
    st.session_state.demo_name = st.query_params.app
    if app_context.is_remote_mode():
      remote_mode.create_session(
          st.session_state.session.app_name,
          st.session_state.session.user_id,
          st.session_state.session.id,
      )
      logger.info('remote agent is connected.')
    else:
      on_demo_name_change()

  if app_context.is_remote_mode():
    # Add remote server URL so the selectbox options below can be updated.
    app_context.get_remote_server_urls().add(st.session_state.demo_name)

  st.selectbox(
      'GenAI Agent Demos',
      [app_context.CONNECT_REMOTE_SERVER_DEMO_NAME]
      + sorted(list(app_context.get_remote_server_urls()))
      + get_demo_names(),
      index=None,
      placeholder='Select a demo...',
      key='demo_name',
      on_change=on_demo_name_change,
  )

  if app_context.is_remote_mode():
    st.warning(
        'Remote Mode. Some features are not available in remote mode yet.'
    )
    return

  if st.session_state.demo_name == app_context.CONNECT_REMOTE_SERVER_DEMO_NAME:
    return

  if not st.session_state.demo_name:
    return

  col1, col2 = st.columns(2)
  if col1.button('Definitions', use_container_width=True):
    definitions_dialog()
  if col2.button('Reload Agent', use_container_width=True):
    import_agent_module(reload=True)
  st.selectbox(
      'Sessions',
      get_saved_sessions(),
      index=None,
      on_change=load_session,
      key='selected_session',
      placeholder='Load a saved session...',
  )
  col1, col2 = st.columns(2)
  if col1.button('Reset', use_container_width=True):
    reset_session()
    st.rerun()
  if col2.button('Save', use_container_width=True):
    save_session_dialog()

  render_streaming_options()

  # Enable this once fixed.
  # st.button('Rerun session', use_container_width=True, key='rerun_session')

  pending_events.render_pending_events_button()

  with st.popover('Insert files'):
    st.file_uploader(
        'Files will be added to the prompt.',
        accept_multiple_files=True,
        key=st.session_state.file_uploader_key,
    )

  if st.button('Debug Info'):
    _render_debug_info_dialog()


@st.cache_resource
def get_demo_names():
  agent_folder = app_context.get_agent_folder()
  demo_names = [
      x
      for x in os.listdir(agent_folder)
      if os.path.isdir(os.path.join(agent_folder, x))
      and not x.startswith('.')
      and x != '__pycache__'
  ]
  demo_names.sort()
  logger.info('Found demos %s demos: %s', len(demo_names), demo_names)
  return demo_names


@st.dialog('Add remote server URL')
def connect_remote_server():
  remote_server_url = st.text_input(
      'Fill remote server URL:', 'http://127.0.0.1:8000'
  )
  if st.button('Connect'):
    st.session_state.demo_name = remote_server_url
    reset_session()
    # Will only add the remote server URL if reset session is successful.
    app_context.get_remote_server_urls().add(remote_server_url)
    st.rerun()


@st.dialog('Agent Definitions')
def definitions_dialog():
  agents = {}
  get_all_agents(app_context.get_root_agent(), agents)
  agent_names = list(agents.keys())
  tab_names = ['Graph'] + agent_names
  tabs = st.tabs(tab_names)
  with tabs[0]:
    st.graphviz_chart(
        agent_graph.get_agent_graph(app_context.get_root_agent(), None)
    )

  for i in range(1, len(tab_names)):
    with tabs[i]:
      agent = agents[agent_names[i - 1]]
      if hasattr(agent, 'children'):
        children_names = [child.name for child in agent.children]
      else:
        children_names = []
      if hasattr(agent, 'tools'):
        tools_names = [
            tool.__name__ if callable(tool) else tool.name
            for tool in agent.tools
        ]
      else:
        tools_names = []
      summary = {
          'name': agent.name,
          'description': agent.description,
          'parent': agent.parent_agent.name if agent.parent_agent else '',
          'children': children_names,
          'tools': tools_names,
      }
      for key in ['model', 'flow', 'global_instruction', 'instruction']:
        if hasattr(agent, key):
          summary[key] = getattr(agent, key)
      st.json(summary)


@st.dialog('Save session')
def save_session_dialog():
  session_name = st.text_input(
      'Session name',
      value=st.session_state.selected_session,
      key='session_name_input_save_session',
  )
  save_button_clicked = st.button('Save', key='save_session_button')

  if save_button_clicked:
    # Define the session file path
    session_path = os.path.join(
        app_context.get_agent_folder(),
        st.session_state.demo_name,
        session_name + '.session.json',
    )

    # Write the session data to the file
    storage_session = st.session_state.session_service.get(
        st.session_state.session.app_name,
        st.session_state.session.user_id,
        st.session_state.session.id,
    )
    with open(session_path, 'w') as f:
      f.write(
          strip_bytes_data(copy.deepcopy(storage_session)).model_dump_json(
              indent=2, exclude_none=True
          )
      )
    logs_path = session_path.replace('.session.json', '.logs.json')
    print(st.session_state.call_llm_spans)
    with open(logs_path, 'w') as f:
      json.dump(st.session_state.call_llm_spans, f, indent=2)
    # Provide success feedback to the UI
    st.success(f'Session saved successfully as: {session_path}')


def get_all_agents(agent, result):
  result[agent.name] = agent
  if hasattr(agent, 'children'):
    for child in agent.children:
      get_all_agents(child, result)


# Use demo_name as cache key.
def get_saved_sessions():
  demo_dir = os.path.join(
      app_context.get_agent_folder(), st.session_state.demo_name
  )
  session_suffix = '.session.json'
  names = [
      x.removesuffix(session_suffix)
      for x in os.listdir(demo_dir)
      if x.endswith(session_suffix)
  ]
  names.sort()
  logger.info('Found sessions %s sessions: %s', len(names), names)
  return names


def load_session():
  selected_session = st.session_state.selected_session
  print('load session', selected_session)
  if not selected_session:
    reset_session()
    return
  session_path = os.path.join(
      app_context.get_agent_folder(),
      st.session_state.demo_name,
      selected_session + '.session.json',
  )
  with open(session_path, 'r') as f:
    st.session_state.session = Session.model_validate_json(f.read())
    print('loaded session', selected_session)
  logs_path = session_path.replace('.session.json', '.logs.json')
  if os.path.exists(logs_path):
    with open(logs_path, 'r') as f:
      st.session_state.call_llm_spans = json.load(f)
  else:
    st.session_state.call_llm_spans = {}
    st.toast('No logs found for the session.')


def reset_session():
  if app_context.is_remote_mode():
    st.session_state.session = app_context.get_session_service().create(
        'remote_test_app',
        'test_user',
    )
    remote_mode.create_session(
        st.session_state.session.app_name,
        st.session_state.session.user_id,
        st.session_state.session.id,
    )
    logger.info('New session: %s.', st.session_state.session.id)
    return

  app_name: str = st.session_state.demo_name
  st.session_state.session = app_context.get_session_service().create(
      app_name, 'test_user'
  )
  st.session_state.call_llm_spans = {}
  build_empty_state(app_context.get_root_agent(), st.session_state.session)
  logger.info(
      'For app: %s, created new session: %s.',
      app_name,
      st.session_state.session.id,
  )


# Fills context dict with the keys that the instructions need.
def build_empty_state(agent, session):
  if not agent or not isinstance(agent, Agent):
    return
  if isinstance(agent.instruction, str):
    for key in re.findall(r'{([\w]+)}', agent.instruction):
      if key not in session.state:
        session.state[key] = ''
  for child in agent.children:
    build_empty_state(child, session)


def import_agent_module(reload=False):
  # The following line is needed to make sure the agent folder is in sys.path.
  app_context.get_agent_folder()
  demo_name = st.session_state.demo_name
  envs.load_dotenv_for_agent(demo_name, app_context.get_agent_folder())
  demo_modules = app_context.get_demo_modules()
  if reload:
    importlib.reload(demo_modules[demo_name].agent)
  else:
    demo_modules[demo_name] = importlib.import_module(demo_name)
  logger.info('Demo loaded: %s', demo_name)


# TODO: temporary for stripping out parts with bytes data
def strip_bytes_data(session: Session):
  for event in session.events:
    if not event.content:
      continue
    event.content.parts = [
        part for part in event.content.parts if not part.inline_data
    ]
    event.actions.artifact_delta = None
  return session


@st.dialog('Debug Info')
def _render_debug_info_dialog():
  st.table(
      pd.DataFrame({
          'Environment Variable': _ENV_VARIABLES.keys(),
          'Value': [os.environ.get(var, '') for var in _ENV_VARIABLES.keys()],
          'Description': _ENV_VARIABLES.values(),
      })
  )


================================================
File: cli/ui/app_context.py
================================================
import sys
import time
import types

from agents.agents.agent import Agent
from agents.artifacts import InMemoryArtifactService
from agents.memory.in_memory_memory_service import InMemoryMemoryService
from agents.runners import Runner
from agents.sessions.base_session_service import BaseSessionService
from agents.sessions.in_memory_session_service import InMemorySessionService
from agents.sessions.postgres_session_service import PostgresSessionService
import streamlit as st

CONNECT_REMOTE_SERVER_DEMO_NAME = 'Connect to Remote Server'


@st.cache_resource
def get_remote_server_urls():
  remote_urls = set()
  return remote_urls


@st.cache_resource
def get_session_service() -> BaseSessionService:
  db_url = sys.argv[2]
  if db_url:
    return PostgresSessionService(db_url=db_url)
  else:
    return InMemorySessionService()


@st.cache_resource
def get_artifact_service():
  return InMemoryArtifactService()


@st.cache_resource
def get_memory_service():
  return InMemoryMemoryService()


def _get_runner(agent_name: str):
  return Runner(
      app_name=agent_name,
      agent=get_demo_modules()[agent_name].agent.root_agent,
      artifact_service=get_artifact_service(),
      session_service=get_session_service(),
      memory_service=get_memory_service(),
      response_modalities=st.session_state.response_modalities,
  )


def get_runner() -> Runner:
  """Gets the runner for the current selected agent."""
  return _get_runner(_get_root_agent_name())


@st.cache_resource
def get_agent_folder():
  agent_folder = sys.argv[1]
  if agent_folder not in sys.path:
    sys.path.append(agent_folder)
  return agent_folder


@st.cache_resource
def get_demo_modules() -> dict[str, types.ModuleType]:
  demo_modules = {}
  return demo_modules


def setup_session_state():
  if 'artifact_service' not in st.session_state:
    st.session_state.artifact_service = get_artifact_service()
  if 'session_service' not in st.session_state:
    st.session_state.session_service = get_session_service()
  if 'event_dialog_index' not in st.session_state:
    st.session_state.event_dialog_index = 0
  if 'file_uploader_key' not in st.session_state:
    st.session_state.file_uploader_key = str(time.time())
  if 'event_num' not in st.session_state:
    st.session_state.event_num = 0
  if 'edit_state' not in st.session_state:
    st.session_state.edit_state = False
  if 'demo_name' not in st.session_state:
    st.session_state.demo_name = None
  if 'streaming' not in st.session_state or st.session_state.streaming is None:
    st.session_state.streaming = ''
  if 'call_llm_spans' not in st.session_state:
    st.session_state.call_llm_spans = {}
  if 'is_live' not in st.session_state:
    st.session_state.is_live = False
  if 'response_modalities' not in st.session_state:
    st.session_state.response_modalities = None


def _get_root_agent_name() -> str:
  return st.session_state.demo_name


def get_root_agent() -> Agent:
  return get_demo_modules()[_get_root_agent_name()].agent.root_agent


def is_remote_mode() -> bool:
  """Whether it's running in remote mode via an api server."""
  demo_name: str = st.session_state.demo_name
  return bool(demo_name) and demo_name.startswith('http://')


================================================
File: cli/ui/artifacts_tab.py
================================================
import io
import logging

import streamlit as st
from PIL import Image

logger = logging.getLogger(__name__)


def render():
  if not st.session_state.demo_name:
    return

  artifact_service = st.session_state.artifact_service
  session_id = st.session_state.session.id

  filenames = artifact_service.list_keys(session_id)

  if not filenames:
    return

  for key in filenames:
    logger.info(f'Render artifacts key {key}')

    versions = artifact_service.list_versions(session_id, key)
    if not versions:
      pass

    with st.popover(key):
      tab_names = ['Latest']
      for i in range(len(versions) - 2, -1, -1):
        tab_names.append(str(i))
      version = st.segmented_control(
          'Versions',
          options=tab_names,
          key=key,
      )
      if not version:
        version = 'Latest'
      index = len(versions) - 1 - tab_names.index(version)
      artifact = artifact_service.load(session_id, key, versions[index])
      if artifact.inline_data:
        st.download_button(
            label='Download',
            data=artifact.inline_data.data,
            file_name=key,
            mime=artifact.inline_data.mime_type,
            key=f'download_{key}_{index}',
        )
      elif artifact.file_data:
        st.download_button(
            label='Download',
            data=artifact.file_data.file_uri,
            file_name=key,
            mime=artifact.file_data.mime_type,
            key=f'download_{key}_{index}',
        )
    render_artifact(artifact)


def render_artifact(artifact):
  # Renders the artifact.
  if artifact.inline_data:
    if artifact.inline_data.mime_type.startswith('image/'):
      st.image(Image.open(io.BytesIO(artifact.inline_data.data)), width=400)
    elif artifact.inline_data.mime_type.startswith('application/pdf'):
      # Don't try to render it.
      pass
    elif artifact.inline_data.mime_type.startswith('text/html'):
      st.components.v1.html(
          artifact.inline_data.data, height=500, scrolling=True
      )
    else:
      st.write(artifact.inline_data.data)
  elif artifact.file_data:
    if artifact.file_data.mime_type.startswith('text/html'):
      st.components.v1.html(
          artifact.file_data.file_uri, height=500, scrolling=True
      )
  else:
    st.code(artifact.text)


================================================
File: cli/ui/chat.py
================================================
import asyncio
from datetime import datetime
import io
import os
import logging
import mimetypes
import random
import re
import string
import time
import types
import threading
import asyncio

import base64
from agents.events import Event
from agents.sessions.session import Session
from google.genai import types as genai_types
from PIL import Image
import streamlit as st
import streamlit.components.v1 as components

from . import app_context
from . import artifacts_tab
from . import eval_tab
from . import event_dialog
from . import remote_mode
from ...agents import LiveRequestQueue


logger = logging.getLogger(__name__)


def show_event_dialog(event_index):
  st.session_state.event_dialog_index = event_index
  event_dialog.show()


# Uses fragment so button clicks won't cause full page rerun.
@st.fragment
def render_avatar(author, event_index, part_index):
  # avatar = '👤' if author == 'user' else '🤖'
  if author == 'user':
    avatar = '👤'
  elif author == 'evaluation':
    avatar = '📝'
  else:
    avatar = '🤖'
  inital = (
      ''.join(x[0] for x in author.split('_'))[:2].upper()
      if author != 'user' and author != 'root_agent'
      else ''
  )
  unique_key = f'avatar_{event_index}_{part_index}_{random.randint(0, 100000)}'

  st.button(
      inital,
      icon=avatar,
      key=unique_key,
      help=author,
      on_click=show_event_dialog,
      args=(event_index,),
  )


# Uses fragment so button clicks won't cause full page rerun.
@st.fragment
def render_function_call_button(event_index, function_calls):
  title = f"⚡ {', '.join([fc.name for fc in function_calls])}"
  with st.popover(title):
    st.button(
        'Details',
        key=f'details_{event_index}_{random.randint(0, 100000)}',
        on_click=show_event_dialog,
        args=(event_index,),
    )
    for fc in function_calls:
      title = f'**{fc.name}**'
      if fc.id:
        title += f' ({fc.id})'
      st.write(title)
      st.write(fc.args)


# Uses fragment so button clicks won't cause full page rerun.
@st.fragment
def render_function_response_button(event_index, function_responses):
  event = st.session_state.session.events[event_index]
  title = f"✔️ {', '.join([fr.name for fr in function_responses])}"
  with st.popover(title):
    st.button(
        'Details',
        key=f'details_{event_index}_{random.randint(0, 100000)}',
        on_click=show_event_dialog,
        args=(event_index,),
    )
    for fr in function_responses:
      title = f'**{fr.name}**'
      if fr.id:
        title += f' ({fr.id})'
      st.write(title)
      st.write(fr.response)
    if event.actions.state_delta:
      st.write('Updated context:')
      st.write(event.actions.state_delta)
    if event.actions.artifact_delta:
      st.write('Updated artifact and version:')
      st.write(event.actions.artifact_delta)


last_fc_container = None
last_fc_author = None


def render_chat_row(author, event_index, part_index):
  cols = st.columns([8, 92], vertical_alignment='top')
  with cols[0]:
    render_avatar(author, event_index, part_index)
  return cols[1]


def render_content(event_index, author, content, actual_tools=[]):
  global last_fc_container
  global last_fc_author
  function_calls = []
  function_responses = []
  for part_index, part in enumerate(content.parts):
    if part.function_call:
      function_calls.append(part.function_call)
    elif part.function_response:
      function_responses.append(part.function_response)
    else:
      last_fc_container = None
      last_fc_author = None
      # Sometime the model returns '\n' and it messes up the UI.
      if part.text and not part.text.strip():
        continue
      chat_container = render_chat_row(author, event_index, part_index)
      with chat_container:
        st.write(transform_part(part))

  if function_calls or function_responses:
    if not last_fc_container or last_fc_author != author:
      last_fc_author = author
      chat_container = render_chat_row(author, event_index, 0)
      with chat_container:
        last_fc_container = st.container(key=f'row_layout_fc_{event_index}_0')

    with last_fc_container:
      if function_calls:
        render_function_call_button(event_index, function_calls)
        actual_tools.extend(function_calls)
      if function_responses:
        render_function_response_button(event_index, function_responses)


def transform_part(part):
  if part.text:
    return part.text.strip()
  elif part.executable_code:
    return f"""```python
{part.executable_code.code}
```"""
  elif part.code_execution_result:
    return part.code_execution_result
  elif part.inline_data:
    if part.inline_data.mime_type.startswith('image/'):
      return Image.open(io.BytesIO(part.inline_data.data))
    data_str = str(part.inline_data.data)
    return f"""
**mime_type**: `{part.inline_data.mime_type}`

**data**: {data_str if len(data_str) < 200 else data_str[:200] + '...'}
"""


def extract_code_block(text: str):
  """Returns: the first code block in the model message and truncate everything after it."""
  pattern = re.compile(
      (
          r'(?P<prefix>.*?)```(tool_code|python)\n'
          '(?P<code>.*?)\n```(?P<suffix>.*?)$'
      ).encode(),
      re.DOTALL,
  )
  pattern_match = pattern.search(text.encode())
  if pattern_match is None:
    return text, None

  prefix = pattern_match.group('prefix').decode()
  code = pattern_match.group('code').decode()
  return prefix, code


text_column = None
# function calling container that can show multiple function calls


def render_events(index_range):
  global text_column
  session = st.session_state.session

  eval_dataset_index = 0
  actual_tools = []
  for i in index_range:
    event = session.events[i]
    if not event.partial:
      text_column = None

    if not event.partial:
      if event.error_code:
        error_message = f'**{event.error_code}**'
        if event.error_message:
          error_message += f'\n\n{event.error_message}'
        st.error(error_message)
      elif event.content:
        render_content(
            i, event.author, event.content, actual_tools=actual_tools
        )
      else:
        logger.info(
            'Event with no content: %s', event.model_dump(exclude_none=True)
        )

    if event.actions.artifact_delta:
      for key, version in event.actions.artifact_delta.items():
        artifact = app_context.get_artifact_service().load(
            session.id, key, version
        )
        artifacts_tab.render_artifact(artifact)

    if event.grounding_metadata and event.grounding_metadata.search_entry_point:
      st.markdown(
          event.grounding_metadata.search_entry_point.rendered_content,
          unsafe_allow_html=True,
      )

    eval_dataset_index = eval_tab.load_eval_into_session(
        session, i, eval_dataset_index, actual_tools
    )


# Merges all partial text streams into a single embedded text stream.
def transform_stream(stream):
  # Does not do anything in non-streaming mode.
  if not st.session_state.streaming:
    yield from stream
    return

  non_text_event = None

  def text_stream(event):
    nonlocal non_text_event
    yield event.content.parts[0].text
    while True:
      try:
        event = next(stream)
        if event.content and event.content.parts[0].text:
          # Ignores non-partial text because it's already rendered.
          if event.partial:
            yield event.content.parts[0].text
        else:
          non_text_event = event
          break
      except StopIteration:
        break

  while True:
    try:
      event = next(stream)
      if event.content and event.partial:
        yield text_stream(event)
        if non_text_event:
          yield non_text_event
          non_text_event = None
      else:
        yield event
    except StopIteration:
      break


def run_prompt(prompt: str):
  parts = []
  has_files = False
  if st.session_state[st.session_state.file_uploader_key]:
    has_files = True
    uploaded_files = st.session_state[st.session_state.file_uploader_key]
    # Uses a different key to clear the file uploader.
    st.session_state.file_uploader_key = str(time.time())
    for file in uploaded_files:
      mime_type, _ = mimetypes.guess_type(file.name)
      parts.append(
          genai_types.Part(
              inline_data=genai_types.Blob(
                  mime_type=mime_type, data=file.read()
              )
          )
      )
  parts.append(genai_types.Part(text=prompt))
  content = genai_types.Content(role='user', parts=parts)

  run_content(content)
  if has_files:
    # Rerun the page to clear the file uploader.
    st.rerun()


def run_content(content: genai_types.Content):
  session = st.session_state.session
  session_length = len(session.events)
  render_content(session_length, 'user', content)

  events = app_context.get_runner().run(
      session=session,
      new_message=content,
      streaming=st.session_state.streaming,
  )
  running_container = st.container()
  with st.spinner('Running...'):
    with running_container:
      for event in transform_stream(events):
        event_index = len(session.events) - 1
        if isinstance(event, types.GeneratorType):
          chat_container = render_chat_row('model', event_index, 0)
          with chat_container:
            st.write_stream(event)
        else:
          # Do not render non-partial text, since it's already rendered above.
          if (
              event.content
              and event.content.parts[0].text
              and st.session_state.streaming == 'server-socket'
          ):
            continue
          render_events(
              range(event_index, event_index + 1),
          )

  # Handle get_user_choice tool response
  # TODO: Do not insert function response for async function call if the
  # function returns None. So we don't have to look back 2 events.
  if session.events and len(session.events) > 1:
    function_call_event = session.events[-2]
    for function_call in function_call_event.get_function_calls():
      if function_call.name == 'get_user_choice':
        with st.container(key=f'row_layout_user_choice', border=True):
          for option in function_call.args['options']:
            st.button(option, on_click=run_prompt, args=(option,))


def render_last_event(session: Session):
  if not session.events:
    return
  render_events(range(len(session.events) - 1, len(session.events)))


def run_prompt_remote(prompt: str):
  session: Session = st.session_state.session
  content = genai_types.Content(
      role='user', parts=[genai_types.Part.from_text(text=prompt)]
  )

  characters = string.ascii_letters + string.digits
  invocation_id = ''.join(random.choice(characters) for _ in range(8))
  user_input_event = Event(
      invocation_id=invocation_id, author='user', content=content
  )
  session.events.append(user_input_event)
  render_last_event(session)

  with st.spinner('Running...'):
    remote_events = remote_mode.run(
        session.app_name, session.user_id, session.id, content
    )
    for event in remote_events:
      render_last_event(session)


def rerun_session():
  session = st.session_state.session
  old_events = session.events
  # We only clear the events and event logs, but keep the session context.
  session.events = []
  st.session_state.call_llm_spans = {}
  for event in old_events:
    if event.content and event.author == 'user':
      print(f'Rerun session: {event.content.parts[0].text}')
      run_prompt(event.content.parts[0].text)


async def run_live(runner, session, live_request_queue):
  events = runner.run_live(
      session=session,
      live_request_queue=live_request_queue,
  )
  import pyaudio

  pya_interface = pyaudio.PyAudio()
  stream = pya_interface.open(
      format=pyaudio.paInt16,
      channels=1,
      rate=24000,
      output=True,
  )

  try:
    async for event in events:
      event_index = len(session.events) - 1
      # TODO: re-enable chat row rendering
      # if isinstance(event, types.GeneratorType):
      #   chat_container = render_chat_row('model', event_index, 0)
      #   with chat_container:
      #     st.write_stream(event)
      # else:
      #   # Do not render non-partial text, since it's already rendered above.
      #   if (
      #       event.content
      #       and event.content.parts[0].text
      #       and st.session_state.streaming == 'server-socket'
      #   ):
      #     continue
      # Handle audio data
      inline_data = (
          event.content
          and event.content.parts
          and event.content.parts[0].inline_data
      )
      if inline_data and inline_data.mime_type.startswith('audio/pcm'):
        stream.write(inline_data.data)
        continue
      # TODO: re-enable chat row rendering
      # render_events(
      #     range(event_index, event_index + 1),
      # )
      await asyncio.sleep(0)
  finally:
    stream.stop_stream()
    stream.close()
    pya_interface.terminate()


def streaming_worker(runner, session, live_request_queue):
  # Create a new event loop for this thread.
  loop = asyncio.new_event_loop()
  asyncio.set_event_loop(loop)

  loop.run_until_complete(run_live(runner, session, live_request_queue))
  # Keep the event loop running to process incoming audio frames.
  loop.run_forever()


def render_chat():
  logger.info('render_chat...')

  # Initialize the saved audio buffer if it does not exist
  if 'saved_audio' not in st.session_state:
    st.session_state['saved_audio'] = b''

  if 'live_request_queue' not in st.session_state:
    st.session_state.live_request_queue = LiveRequestQueue()

  if not st.session_state.demo_name:
    return

  if app_context.is_remote_mode():
    _render_chat_remote()
    return

  session: Session = st.session_state.session
  root_agent = app_context.get_root_agent()
  session.state['_time'] = str(datetime.now())
  if not session.events and root_agent.greeting_prompt:
    # new_message=None triggers the greeting prompt.
    events = list(
        app_context.get_runner().run(
            session=session,
            new_message=None,
            streaming=st.session_state.streaming,
        )
    )

  if st.session_state.streaming != 'bidi':
    if session.events:
      render_events(range(len(session.events)))
    elif root_agent.description:
      with st.container(border=True):
        st.write(root_agent.description)

  # Check if streaming mode is set to 'server-socket-audio'
  if st.session_state.streaming == 'bidi':
    if st.session_state.is_live:
      parent_dir = os.path.dirname(os.path.abspath(__file__))
      media_streamer_path = os.path.join(parent_dir, '../media_streamer')
      media_streamer = components.declare_component(
          'media_streamer',
          path=media_streamer_path,
      )

      st.markdown('---')
      st.subheader('Gemini Bidi Live')
      st.write(
          'Press the button to start/stop streaming. Audio chunks will be'
          ' sent from the browser.'
      )

      media_data = media_streamer(
          key='media_streamer', some_prop='initial value'
      )

      if 'media_worker' not in st.session_state:
        runner = app_context.get_runner()
        session = st.session_state.session
        worker_thread = threading.Thread(
            target=streaming_worker,
            args=(
                runner,
                session,
                st.session_state.live_request_queue,
            ),
            daemon=True,
        )
        worker_thread.start()

        # Store the worker thread in session state.
        st.session_state['media_worker'] = worker_thread
        # time.sleep(2)
      else:
        # The worker is already running.
        if 'videoFrame' in media_data:
          # video data is in the following format: data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABA
          base64_str = media_data['videoFrame'].split('base64,')[1]
          decoded_image = base64.b64decode(base64_str)
          # video is sent as images
          st.session_state.live_request_queue.send_realtime(
              genai_types.Blob(data=decoded_image, mime_type='image/jpeg')
          )
        if 'audioChunk' in media_data:
          # audio data is in the following format: data:application/octet-stream;base64,CgAJAAoA
          chunk_data = media_data['audioChunk'].split('base64,', 1)[1]
          # Add missing padding if necessary
          missing_padding = len(chunk_data) % 4
          if missing_padding:
            chunk_data += '=' * (4 - missing_padding)
          decoded = base64.b64decode(chunk_data)
          st.session_state.live_request_queue.send_realtime(
              genai_types.Blob(data=decoded, mime_type='audio/pcm')
          )
  else:
    # Chat input for prompt
    prompt = st.chat_input('Say something')
    if prompt:
      parts = []

      # # TODO: enable this after we fully support audio
      # # Allows users to record/upload audio(non-streaming) to chat
      # audio_file_path = 'user_audio_streaming.wav'
      # if os.path.exists(audio_file_path):
      #   st.success('Audio file found and added to the prompt.')
      #   with open(audio_file_path, 'rb') as audio_file:
      #     audio_part = genai_types.Part.from_bytes(
      #         data=audio_file.read(),
      #         mime_type='audio/wav',
      #     )
      #     parts.append(audio_part)

      # # Add the user-entered prompt
      # parts.append(genai_types.Part(text=prompt))
      # content = genai_types.Content(role='user', parts=parts)

      # # Run content
      # run_content(content)
      run_prompt(prompt)


def _render_chat_remote():
  session: Session = st.session_state.session
  if session.events:
    render_events(range(len(session.events)))
  prompt = st.chat_input('Say something')
  if prompt:
    run_prompt_remote(prompt)


================================================
File: cli/ui/eval_tab.py
================================================
import json
import logging
import os

import pandas as pd
import streamlit as st

from agents.sessions.session import Session

from . import app_context
from . import chat

logger = logging.getLogger(__name__)


def render():
  if not st.session_state.demo_name:
    return

  # Render current agent name
  current_agent = st.session_state.get('demo_name', None)

  if not current_agent:
    st.info('Please select an agent first in Agent tab.')
    return

  if st.button('Save session as eval', use_container_width=True):
    save_session_as_test_file_dialog()

  # Render evaluation file selection
  select_evaluations()

  if not st.session_state.get('run_tests'):
    render_eval_results()
  st.session_state['run_tests'] = False

  # Load Selected Session
  load_session_with_eval()


def select_evaluations():
  col1, col2 = st.columns(2)
  if col1.button(
      'Run Select',
      use_container_width=True,
      key='open_test_files_popup',
  ):
    select_test_files_popup()

  if col2.button(
      'Run All',
      use_container_width=True,
      key='run_all_tests_button',
  ):
    eval_files_directory = os.path.abspath(
        os.path.join(app_context.get_agent_folder(), st.session_state.demo_name)
    )
    all_test_files = [
        os.path.join(root, file)
        for root, _, files in os.walk(eval_files_directory)
        for file in files
        if file.endswith('.test.json')
    ]
    st.session_state['run_tests'] = True
    st.session_state['test_file_selection'] = all_test_files

  # Run the selected test files
  if st.session_state.get('run_tests'):
    run_tests(st.session_state.get('test_file_selection', []))


def load_session_with_eval():
  if (
      'session_to_load' in st.session_state
      and st.session_state['session_to_load']
  ):

    st.write(f"✅ Loading session: {st.session_state['session_to_load']}")
    selected_eval_file = st.session_state['session_to_load']
    session_name = selected_eval_file.rsplit('.test', 1)[0]

    session_path = os.path.join(
        app_context.get_agent_folder(),
        st.session_state.demo_name,
        session_name + '.session.json',
    )

    eval_path = os.path.join(
        app_context.get_agent_folder(),
        st.session_state.demo_name,
        session_name + '.test.json',
    )

    # Check if the session file exists
    if not os.path.exists(session_path) or not os.path.exists(eval_path):
      st.error(f'❌ File not found: {session_path}')
      st.session_state['session_to_load'] = None  # Clear temporary state
      return

    with open(session_path, 'r') as f:
      st.session_state.session = Session.model_validate_json(f.read())

    with open(eval_path, 'r') as f:
      st.session_state.eval = json.load(f)
      logger.info('loaded eval file: %s', eval_path)

    st.session_state['session_to_load'] = None  # Clear temporary state
    st.rerun()


def run_tests(test_files):
  if not test_files:
    st.warning('No files selected for testing.')
    return

  progress_section = st.empty()
  with progress_section.container():
    st.write('### Running Evaluations')
    progress = st.progress(0)
  total_files = len(test_files)
  test_results = []

  col1, col2 = st.columns([8, 2])
  col1.text('File')
  col2.text('Result')

  for idx, file_path in enumerate(test_files):
    try:
      # Load evaluation criteria for the current file
      from agents.evaluation.agent_evaluator import AgentEvaluator
      from agents.evaluation.trajectory_evaluator import TrajectoryEvaluator

      evaluation_criteria = AgentEvaluator.find_config_for_test_file(file_path)

      # Use AgentEvaluator for trajectory evaluation
      dataset = AgentEvaluator.load_dataset(file_path)[0]
      agent_name = st.session_state.get('demo_name', 'unknown_agent')

      # breakpoint()
      session_name = file_path.rsplit('.test.json', 1)[0]

      session_path = os.path.join(
          app_context.get_agent_folder(),
          st.session_state.demo_name,
          session_name + '.session.json',
      )

      num_runs = 1
      # Check if the session file exists
      evaluation_response = None
      if os.path.exists(session_path):
        evaluation_response = AgentEvaluator._generate_responses_from_session(
            [dataset], session_path
        )
      else:
        evaluation_response = AgentEvaluator._generate_responses(
            agent_name, [dataset], num_runs=num_runs
        )
      score = TrajectoryEvaluator.evaluate(evaluation_response)

      # Check against criteria
      threshold = evaluation_criteria.get('tool_trajectory_avg_score', 1.0)
      result = '✅' if score >= threshold else '❌'
    except Exception as e:
      result = f'❌ Error - {str(e)}'
      logger.info('Error: %s', str(e))

    # Append results
    test_result = {
        'File Name': os.path.basename(file_path),
        'Result': result,
        'File Path': file_path,
    }
    test_results.append(test_result)
    render_eval_result(idx, test_result)
    # Update progress
    progress.progress((idx + 1) / total_files)

  st.session_state['test_results'] = test_results
  progress_section.empty()


def render_eval_results():
  if (
      'test_results' not in st.session_state
      or not st.session_state['test_results']
  ):
    st.info('Run an evaluation to see results.')
    return
  col1, col2 = st.columns([8, 2])
  col1.text('File')
  col2.text('Result')

  test_results_df = pd.DataFrame(st.session_state['test_results'])

  for index, row in test_results_df.iterrows():
    render_eval_result(index, row)


def render_eval_result(index, row):
  col1, col2 = st.columns([8, 2])
  with col1:
    st.write(row['File Name'].removesuffix('.test.json'))
  with col2:
    if st.button(row['Result'], key=f'load_session_btn_{index}'):
      file_path = row['File Path']
      st.session_state['session_to_load'] = os.path.splitext(
          os.path.basename(file_path)
      )[0]


# File Selection Dialog
@st.dialog('Select Evaluation Files to Run')
def select_test_files_popup():
  st.write('### Select Files for Evaluation')

  # Fetch test files
  eval_files_directory = os.path.abspath(
      os.path.join(app_context.get_agent_folder(), st.session_state.demo_name)
  )
  test_files = [
      os.path.join(root, file)
      for root, _, files in os.walk(eval_files_directory)
      for file in files
      if file.endswith('.test.json')
  ]

  if not test_files:
    st.warning('No test files found.')
    return

  # Prepare selection table
  test_data = {
      'Select': [
          os.path.basename(file)
          in st.session_state.get('test_file_selection', [])
          for file in test_files
      ],
      'File Name': [os.path.basename(file) for file in test_files],
  }
  test_df = pd.DataFrame(test_data)

  # Display editable table
  edited_test_df = st.data_editor(
      test_df, use_container_width=True, key='test_file_selection_table'
  )

  # Update selected test files
  st.session_state['test_file_selection'] = [
      test_files[i]
      for i, selected in enumerate(edited_test_df['Select'])
      if selected
  ]

  if st.button('Run Selected Tests'):
    st.session_state['run_tests'] = True
    st.session_state['run_all_tests'] = (
        False  # Ensure only selected tests are run
    )
    st.rerun()


@st.dialog('Save session as test file')
def save_session_as_test_file_dialog():
  session_name = st.text_input(
      'Test file name',
      value=st.session_state.selected_session,
      key='session_name_input',
  )
  save_button_clicked = st.button('Save', key='test_file_save_button')

  if save_button_clicked:
    try:
      # Convert the session data to evaluation format
      test_data = convert_session_format_to_eval_format(
          st.session_state.session.model_dump()
      )

      # Serialize the test data to JSON
      test_data_str = json.dumps(test_data, indent=2)

      # Define the file path
      test_file_path = os.path.join(
          app_context.get_agent_folder(),
          st.session_state.demo_name,
          session_name + '.test.json',
      )

      # Write the JSON string to the file
      with open(test_file_path, 'w') as f:
        f.write(test_data_str)

      # Provide feedback on the UI
      st.success(f'File saved successfully as: {test_file_path}')

    except Exception as e:
      # Display an error message if something goes wrong
      st.error(f'An error occurred while saving the file: {str(e)}')


def load_eval_into_session(session, i, eval_dataset_index, actual_tools):
  def render_eval_event(event_index, author, eval_event_index, tools_match):
    eval_event = st.session_state.eval[eval_event_index]
    expected_tools = eval_event.get('expected_tool_use', [])

    # Create a new row for the evaluation
    chat_row = st.container(key=f'chat_row_eval_{event_index}')
    with chat_row:
      # Render the avatar for the evaluation
      chat.render_avatar(author, event_index, 0)
      with st.container():
        if not tools_match:
          st.markdown('**Evaluation (❌): Mismatch Detected**')
        else:
          if expected_tools:
            st.markdown('**Evaluation (✅): Expected Tools Used**')
          else:
            return  # If no expected tools, return early

        # Render tools in a horizontal layout
        for tool_index, tool in enumerate(expected_tools):
          render_eval_tool_button(eval_event_index, tool, tool_index)

  @st.fragment
  def render_eval_tool_button(eval_event_index, tool, tool_index):
    tool_text = f"⚡ {tool.get('tool_name', 'Unknown Tool')}"
    tool_details = tool.get('tool_input', {})
    unique_key = (
        f'eval_tool_{eval_event_index}_{tool.get("tool_name")}_{tool_index}'
    )

    # Render the popup and button
    with st.popover(tool_text):
      st.write('### Tool Details')
      st.json(tool_details)
      st.button(
          tool_text,
          key=unique_key,
          help='Click to view details',  # Optional tooltip
          on_click=lambda: None,  # Trigger the popup display
      )

  if 'eval' not in st.session_state:
    return
  if i - 1 >= 0 and session.events[i - 1].author == 'user':
    eval_event = st.session_state.eval[eval_dataset_index]
    expected_tools = eval_event.get('expected_tool_use', [])
    if not expected_tools:
      eval_dataset_index += 1
      actual_tools = []
      return eval_dataset_index
    from agents.evaluation.trajectory_evaluator import TrajectoryEvaluator

    actual_processed = [
        {'tool_name': tool.name, 'tool_input': tool.args}
        for tool in actual_tools
    ]
    expected_processed = [
        {'tool_name': tool['tool_name'], 'tool_input': tool['tool_input']}
        for tool in expected_tools
    ]
    tools_match = TrajectoryEvaluator.are_tools_equal(
        actual_processed, expected_processed
    )

    render_eval_event(i, 'evaluation', eval_dataset_index, tools_match)

    eval_dataset_index += 1
    actual_tools = []
    return eval_dataset_index
  return eval_dataset_index


def convert_session_format_to_eval_format(session_file):
  """Converts a single session file in the new format into the accepted evaluation format.

  Args:
      session_file (dict): The detailed session log.

  Returns:
      list: A single evaluation dataset in the required format.
  """
  eval_case = []
  events = session_file.get('events', [])

  for event in events:
    if event.get('author') == 'user':
      # Extract user query
      content = event.get('content', {})
      parts = content.get('parts', [])
      if not parts:
        continue

      query = parts[0].get('text', '')  # Safely get the query text

      # Find the corresponding tool usage or response for the query
      expected_tool_use = []
      reference = None

      # Check subsequent events to extract tool uses or responses
      for subsequent_event in events[events.index(event) + 1 :]:
        subsequent_content = subsequent_event.get('content', {})
        subsequent_parts = subsequent_content.get('parts', [])
        if not subsequent_parts:
          continue

        # Safely check for function_call
        function_call = subsequent_parts[0].get('function_call', None)
        if function_call:
          tool_name = function_call.get('name', '')
          tool_input = function_call.get('args', {})
          expected_tool_use.append({
              'tool_name': tool_name,
              'tool_input': tool_input,
          })
        elif 'text' in subsequent_parts[0]:
          reference = subsequent_parts[0].get('text', '')
          break

      eval_case.append({
          'query': query,
          'expected_tool_use': expected_tool_use,
          'reference': reference,
      })

  return eval_case


================================================
File: cli/ui/event_dialog.py
================================================
import json
import os

import streamlit as st

from . import agent_graph
from . import app_context


@st.dialog('Event', width='large')
def show():
  root_agent = app_context.get_root_agent()
  session = st.session_state.session
  col1, col2, col3 = st.columns([90, 5, 5], vertical_alignment='center')
  has_previous = st.session_state.event_dialog_index > 0
  has_next = st.session_state.event_dialog_index < len(session.events) - 1

  with col2:
    if st.button('⇦', disabled=not has_previous):
      if has_previous:
        st.session_state.event_dialog_index -= 1

  with col3:
    if st.button('⇨', disabled=not has_next):
      if has_next:
        st.session_state.event_dialog_index += 1

  event_index = st.session_state.event_dialog_index
  event = session.events[event_index]
  text = get_event_info(event)

  with col1:
    # Uses radio buttons because they are faster than tabs.
    tabs = ['Event', 'Graph', 'Request', 'Response']
    if 'event_dialog_info_type' not in st.session_state:
      st.session_state.event_dialog_info_type = 'Event'
    info_type = st.segmented_control(
        f'Event {event_index + 1} of {len(session.events)}: {text}',
        tabs,
        selection_mode='single',
        key='event_dialog_info_type',
    )

  call_llm_span = st.session_state.call_llm_spans.get(event.id, None)
  if info_type == 'Event':
    if call_llm_span:
      project_id = os.environ.get('GOOGLE_CLOUD_PROJECT', '')
      trace_id = f"{call_llm_span['trace_id']:x}"
      span_id = f"{call_llm_span['span_id']:x}"
      st.link_button(
          'Trace',
          f'https://console.cloud.google.com/traces/explorer;traceId={trace_id};spanId={span_id}&project={project_id}',
      )
    st.write(event.model_dump(exclude_none=True))

  if info_type == 'Graph':
    function_calls = event.get_function_calls()
    function_responses = event.get_function_responses()
    if function_calls:
      function_call_highlights = []
      for function_call in function_calls:
        from_name = event.author
        to_name = function_call.name
        function_call_highlights.append((from_name, to_name))
      st.graphviz_chart(
          agent_graph.get_agent_graph(root_agent, function_call_highlights)
      )
    elif function_responses:
      function_responses_highlights = []
      for function_response in function_responses:
        from_name = function_response.name
        to_name = event.author
        function_responses_highlights.append((from_name, to_name))
      st.graphviz_chart(
          agent_graph.get_agent_graph(root_agent, function_responses_highlights)
      )
    else:
      from_name = event.author
      to_name = ''
      st.graphviz_chart(
          agent_graph.get_agent_graph(root_agent, [(from_name, to_name)])
      )

  if info_type == 'Request':
    if call_llm_span:
      if 'gcp.vertex.agent.llm_request' in call_llm_span:
        st.write(json.loads(call_llm_span['gcp.vertex.agent.llm_request']))
      if 'gcp.vertex.agent.data' in call_llm_span:
        st.write(json.loads(call_llm_span['gcp.vertex.agent.data']))
    else:
      st.write('No logs found for the event.')

  if info_type == 'Response':
    if call_llm_span and 'gcp.vertex.agent.llm_response' in call_llm_span:
      st.write(json.loads(call_llm_span['gcp.vertex.agent.llm_response']))
    else:
      st.write('No logs found for the event.')


def get_event_info(event):
  function_calls = event.get_function_calls()
  function_responses = event.get_function_responses()
  if function_calls:
    for function_call in function_calls:
      text = f'⚡ {function_call.name}'
  elif function_responses:
    for function_response in function_responses:
      icon = '⏳' if event.actions.pending else '✔️'
      text = f'{icon} {function_response.name}'
  elif event.is_final_response():
    text = event.content.role if event.content.role else 'model'
    text += ': '
    for part in event.content.parts:
      if part.text:
        text += part.text
      elif part.inline_data and part.inline_data.mime_type.startswith('image/'):
        text += ' [IMAGE] '
      elif part.inline_data:
        text += ' [File] '
  else:
    raise ValueError(f'Unsupported event: {event}')
  return text


================================================
File: cli/ui/pending_events.py
================================================
import json

from agents.events import Event
from agents.flows.llm_flows import functions
import streamlit as st

from . import app_context

run_every = None


@st.fragment(run_every=run_every)
def auto_refresh_for_pending_events():
  session = st.session_state.session
  if st.session_state.event_num != len(session.events):
    st.session_state.event_num = len(session.events)
    st.rerun()


def start_or_stop_auto_refresh_for_pending_events():
  global run_every
  session = st.session_state.session
  session_service = st.session_state.session_service
  if session_service.list_pending_events(
      session.app_name, session.user_id, session.id
  ):
    run_every = '3s'
    auto_refresh_for_pending_events()
  else:
    run_every = None


@st.dialog('Pending event')
def pending_event_dialog(
    function_call_event,
    function_call,
    function_response_events,
):
  st.write(f'Pending function call: {function_call.name}')
  st.json(function_call.args)
  with st.form('result_form'):
    result = st.text_area('Result')
    if st.form_submit_button('Submit'):
      result_value = json.loads(result) if result.startswith('{') else result
      if not isinstance(result_value, dict):
        result_value = {'result': result_value}
      function_response_event = Event.from_function_response(
          function_call_event, function_call, result_value
      )
      function_response_events.append(function_response_event)
      if len(function_response_events) == len(
          function_call_event.get_function_calls()
      ):
        # when all the function responses are generated
        session = st.session_state.session
        session_service = st.session_state.session_service
        for event in function_response_events:
          session_service.append_event(session, event)
        list(
            app_context.get_runner().run(
                session=session,
                new_message=None,
                streaming=st.session_state.get('streaming', False),
            )
        )
      st.rerun()


def render_pending_events_button():
  session = st.session_state.session
  session_service = st.session_state.session_service
  pending_events = session_service.list_pending_events(
      session.app_name, session.user_id, session.id
  )
  start_or_stop_auto_refresh_for_pending_events()

  with st.popover(
      f'Pending events ({len(pending_events)})', use_container_width=True
  ):
    for pending_event in pending_events:
      function_call_event = pending_event.function_call_event
      function_response_events = pending_event.function_response_events
      function_calls = function_call_event.get_function_calls()
      for function_call in function_calls:
        name = (
            f"{function_call.name}({','.join([str(x) for x in function_call.args.values()])})"
        )
        if st.button(
            name,
            key=f'pending_{function_call_event.id}_{name}',
            use_container_width=True,
        ):
          pending_event_dialog(
              function_call_event,
              function_call,
              function_response_events,
          )


================================================
File: cli/ui/remote_mode.py
================================================
import logging
from typing import Generator

from agents.events import Event
from google.genai import types
import requests
import streamlit as st

logger = logging.getLogger(__name__)


def create_session(app_name: str, user_id: str, session_id: str):
  server_url: str = st.session_state.demo_name
  url = f'{server_url}/apps/{app_name}/users/{user_id}/sessions/{session_id}'
  response = requests.post(url, json={}, timeout=120)
  logger.info('Creating session, status: %s', response.status_code)
  # TODO: refresh st.session_state.session from the response.


def run(
    app_name: str, user_id: str, session_id: str, new_message: types.Content
) -> Generator[Event, None, None]:
  server_url: str = st.session_state.demo_name
  url = f'{server_url}/agent/run'
  headers = {'Content-Type': 'application/json'}
  payload = {
      'app_name': app_name,
      'user_id': user_id,
      'session_id': session_id,
      'new_message': new_message.model_dump(),
  }
  response = requests.post(url, headers=headers, json=payload, timeout=120)
  logger.info('Running agent, status: %s', response.status_code)

  if response.status_code == 200:
    event_dict_list = response.json()
    logger.info(
        'Got %s events from server: %s', len(event_dict_list), event_dict_list
    )
    # TODO: Fix this hack by change elsewhere to read session from runner.
    for event_dict in event_dict_list:
      event = Event.model_validate(event_dict)
      st.session_state.session_service.append_event(
          st.session_state.session, event
      )
      yield event
  else:
    raise RuntimeError(f'Failed to run agent, status: {response.status_code}')


================================================
File: cli/ui/state_tab.py
================================================
import json

import streamlit as st


def save_state():
  session = st.session_state.session
  session.state = json.loads(st.session_state.get('state_json', ''))
  st.session_state.edit_state = False


def cancel_edit_state():
  st.session_state.edit_state = False


def start_edit_state():
  st.session_state.edit_state = True


# st.fragment is needed to avoid rerun the whole, in which case the state
# wont'be updated.
@st.fragment
def render():
  if not st.session_state.demo_name:
    return

  session = st.session_state.session

  if st.session_state.edit_state:
    with st.container(key='row_layout_state'):
      st.button('Save', on_click=save_state)
      st.button('Cancel', on_click=cancel_edit_state)
    state_json = json.dumps(session.state, indent=2)
    st.text_area(
        'Edit',
        state_json,
        height=500,
        key='state_json',
    )
  else:
    st.button('Edit', on_click=start_edit_state)
    st.write(session.state)


================================================
File: cli/ui/streaming_component.py
================================================
import asyncio
import logging
import wave

from google.genai import types
import pyaudio
import streamlit as st

from ...agents import LiveRequestQueue
from . import app_context

logger = logging.getLogger(__name__)
FORMAT = pyaudio.paInt16
CHANNELS = 1
SEND_SAMPLE_RATE = 16000
CHUNK_SIZE = 1024
AUDIO_FILE_PATH = "user_audio_streaming.wav"


class AudioStreamingComponent:

  def __init__(self, sample_rate=16000, channels=1, chunk_size=1024):
    self.sample_rate = sample_rate
    self.channels = channels
    self.chunk_size = chunk_size
    self.audio_interface = pyaudio.PyAudio()
    self.audio_stream = None
    self.live_request_queue = LiveRequestQueue()
    # for debugging or replay audio
    self.recorded_frames = []
    self.is_streaming = False

    self.audio_data = []
    self.streaming_audio_data = []

  async def start_streaming(self):
    """Start audio streaming using PyAudio."""
    self.is_streaming = True
    self.recorded_frames = []
    mic_info = self.audio_interface.get_default_input_device_info()
    self.audio_stream = self.audio_interface.open(
        format=FORMAT,
        channels=self.channels,
        rate=self.sample_rate,
        input=True,
        input_device_index=mic_info["index"],
        frames_per_buffer=self.chunk_size,
    )
    while True:
      # TODO: temoporary fix for input overflow.
      # exception_on_overflow=False may cause minior frame loss
      frame = self.audio_stream.read(
          self.chunk_size, exception_on_overflow=False
      )
      if frame:
        self._buffer_streaming_data(frame)
        await asyncio.sleep(0)

  def stop_streaming(self):
    """Stop audio streaming and close resources."""
    if self.audio_stream and self.is_streaming:
      self.is_streaming = False
      self.audio_stream.stop_stream()
      self.audio_stream.close()

  def save_to_file(self, file_path):
    """Save the recorded audio to a WAV file."""
    if not self.recorded_frames:
      return False
    with wave.open(file_path, "wb") as wf:
      wf.setnchannels(self.channels)
      wf.setsampwidth(self.audio_interface.get_sample_size(FORMAT))
      wf.setframerate(self.sample_rate)
      wf.writeframes(b"".join(self.recorded_frames))
    return True

  def _buffer_streaming_data(self, in_data):
    """Callback to buffer audio chunks and store them."""
    self.live_request_queue.send_realtime(
        types.Blob(data=in_data, mime_type="audio/pcm")
    )
    self.recorded_frames.append(in_data)
    return (None, pyaudio.paContinue)

  def close(self):
    self.audio_interface.terminate()


def render():
  # Initialize Streamlit session state variables
  if "audio_data_queue" not in st.session_state:
    st.session_state.audio_data_queue = (
        asyncio.Queue()
    )  # Queue for streaming audio input
  if "runner_output_queue" not in st.session_state:
    st.session_state.runner_output_queue = (
        asyncio.Queue()
    )  # Queue for model server responses
  if "stop_event" not in st.session_state:
    st.session_state.stop_event = asyncio.Event()  # Event to stop streaming
  if "audio_streaming_component" not in st.session_state:
    st.session_state.audio_streaming_component = AudioStreamingComponent()


def stop_runner():
  st.session_state.stop_event.set()  # Signal stop to the runner
  st.session_state.audio_streaming_component.stop_streaming()


================================================
File: cli/utils/envs.py
================================================
import logging
import os
from types import ModuleType

from dotenv import load_dotenv

logger = logging.getLogger(__file__)


def _walk_to_root_until_found(folder, filename) -> str:
  checkpath = os.path.join(folder, filename)
  if os.path.exists(checkpath) and os.path.isfile(checkpath):
    return checkpath

  parent_folder = os.path.dirname(folder)
  if parent_folder == folder:  # reached the root
    return ''

  return _walk_to_root_until_found(parent_folder, filename)


def _log_critical_env_variables():
  """Logs all critical environment variables."""

  critical_env_vars = [
      'GOOGLE_GENAI_USE_VERTEXAI',
      'GOOGLE_API_KEY',
      'GOOGLE_CLOUD_PROJECT',
      'GOOGLE_CLOUD_LOCATION',
  ]
  env_vars = []
  for env_var in critical_env_vars:
    if env_var in os.environ:
      env_vars.append(f'{env_var}={os.environ[env_var]}')
  logger.info(
      'Critical environment variables: \n%s',
      '\n'.join(env_vars),
  )


def load_dotenv_for_agent(
    agent_name: str, agent_parent_folder: str, filename: str = '.env'
):
  """Lods the .env file for the agent module."""

  # Gets the folder of agent_module as starting_folder
  starting_folder = os.path.abspath(
      os.path.join(agent_parent_folder, agent_name)
  )
  dotenv_file_path = _walk_to_root_until_found(starting_folder, filename)
  if dotenv_file_path:
    load_dotenv(dotenv_file_path, override=True, verbose=True)
    logger.info(
        'Loaded %s file for %s at %s',
        filename,
        agent_name,
        dotenv_file_path,
    )
    _log_critical_env_variables()
  else:
    logger.info('No %s file found for %s', filename, agent_name)


================================================
File: cli/utils/logs.py
================================================
import logging
import os
import tempfile
import time

LOGGING_FORMAT = (
    '%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
)


def log_to_stderr(level=logging.INFO):
  logging.basicConfig(
      level=level,
      format=LOGGING_FORMAT,
  )


def log_to_tmp_folder(
    level=logging.INFO,
    *,
    sub_folder: str = 'agents_log',
    log_file_prefix: str = 'agent',
    log_file_timestamp: str = time.strftime('%Y%m%d_%H%M%S'),
):
  """Logs to system temp folder, instead of logging to stderr.

  Args
    sub_folder: str = 'agents_log',
    log_file_prefix: str = 'agent',
    log_file_timestamp: str = time.strftime('%Y%m%d_%H%M%S'),

  Returns
    the log file path.
  """
  log_dir = os.path.join(tempfile.gettempdir(), sub_folder)
  log_filename = f'{log_file_prefix}.{log_file_timestamp}.log'
  log_filepath = os.path.join(log_dir, log_filename)

  os.makedirs(log_dir, exist_ok=True)

  file_handler = logging.FileHandler(log_filepath, mode='w')
  file_handler.setLevel(level)
  file_handler.setFormatter(logging.Formatter(LOGGING_FORMAT))

  root_logger = logging.getLogger()
  root_logger.setLevel(level)
  root_logger.handlers = []  # Clear handles to disable logging to stderr
  root_logger.addHandler(file_handler)

  print(f'Log setup complete: {log_filepath}')

  latest_log_link = os.path.join(log_dir, f'{log_file_prefix}.latest.log')
  if os.path.islink(latest_log_link):
    os.unlink(latest_log_link)
  os.symlink(log_filepath, latest_log_link)

  print(f'To access latest log: tail -F {latest_log_link}')
  return log_filepath


================================================
File: code_executor/__init__.py
================================================
import logging
from .base_code_executor import BaseCodeExecutor
from .code_execution_utils import CodeExecutionInput
from .code_execution_utils import CodeExecutionResult
from .code_execution_utils import CodeExecutionUtils
from .code_execution_utils import File
from .code_executor_context import CodeExecutorContext
from .unsafe_local_code_executor import UnsafeLocalCodeExecutor

logger = logging.getLogger(__name__)

__all__ = []

try:
  from .vertex_code_executor import VertexCodeExecutor

  __all__.append('vertex_code_executor')
except ImportError:
  logger.warning(
      'The Vertex sdk is not installed. If you want to use the Vertex Code'
      ' Interpreter with agents, please install it. If not, you can ignore this'
      ' warning.'
  )


================================================
File: code_executor/base_code_executor.py
================================================
import abc
import base64
from google.genai import types
from pydantic import BaseModel
from ..agents.invocation_context import InvocationContext
from .code_execution_utils import CodeExecutionInput
from .code_execution_utils import CodeExecutionResult
from .code_execution_utils import File


class BaseCodeExecutor(BaseModel):
  """Base class for all code executors."""

  optimize_data_file: bool = False
  """
  If true, extract and process data files from the model request
  and attach them to the code executor.
  Supported data file MimeTypes are [text/csv].

  Default to False.
  """

  stateful: bool = True
  """
  Whether the code executor is stateful. Default to False.
  """

  error_retry_attempts: int = 2
  """
  The number of attempts to retry on consecutive code execution errors. Default to 2.
  """

  @abc.abstractmethod
  def execute_code(
      self,
      invocation_context: InvocationContext,
      code_execution_input: CodeExecutionInput,
  ) -> CodeExecutionResult:
    """Executes code and return the result."""
    pass

  def _save_output_files(
      self, invocation_context: InvocationContext, output_files: list[File]
  ):
    """Saves the output files to Artifact Service."""
    if invocation_context.artifact_service is None:
      raise ValueError("Artifact service is not initialized.")
    for output_file in output_files:
      invocation_context.artifact_service.save(
          invocation_context.session.id,
          output_file.name,
          types.Part.from_bytes(
              data=base64.b64decode(output_file.content),
              mime_type=output_file.mime_type,
          ),
      )


================================================
File: code_executor/code_execution_utils.py
================================================
"""Utility functions for code execution."""

import base64
import binascii
import dataclasses
import re

from google.genai import types


@dataclasses.dataclass(frozen=True)
class File:
  """A structure that contains a file name and its content."""

  name: str
  """
  The name of the file with file extension (e.g., "file.csv").
  """

  content: str
  """
  The base64-encoded bytes of the file content.
  """

  mime_type: str = 'text/plain'
  """
  The mime type of the file (e.g., "image/png").
  """


@dataclasses.dataclass
class CodeExecutionInput:
  """A structure that contains the input of code execution."""

  code: str
  """
  The code to execute.
  """

  input_files: list[File] = dataclasses.field(default_factory=list)
  """
  The input files available to the code.
  """

  execution_id: str | None = None
  """
  The execution ID for the stateful code execution.
  """


@dataclasses.dataclass
class CodeExecutionResult:
  """A structure that contains the result of code execution."""

  stdout: str = ''
  """
  The standard output of the code execution.
  """

  stderr: str = ''
  """
  The standard error of the code execution.
  """

  output_artifacts: list[str] = dataclasses.field(default_factory=list)
  """
  The output artifact names from the code execution.
  """


class CodeExecutionUtils:
  """Utility functions for code execution."""

  @staticmethod
  def get_encoded_file_content(data: bytes) -> bytes:
    """Returns the file content as a base64-encoded bytes."""

    def _is_base64_encoded(data: bytes) -> bool:
      try:
        return base64.b64encode(base64.b64decode(data)) == data
      except binascii.Error:
        return False

    return data if _is_base64_encoded(data) else base64.b64encode(data)

  @staticmethod
  def extract_code_and_truncate_content(content: types.Content) -> str | None:
    """Returns the first code block and truncate everything after it."""
    response_text = '\n'.join(
        [part.text for part in content.parts if part.text]
    )
    pattern = re.compile(
        (
            r'(?P<prefix>.*?)```(tool_code|python)\n'
            '(?P<code>.*?)\n```(?P<suffix>.*?)$'
        ).encode(),
        re.DOTALL,
    )
    pattern_match = pattern.search(response_text.encode())
    if pattern_match is None:
      return

    code_str = pattern_match.group('code').decode()
    if not code_str:
      return

    content.parts = []
    if pattern_match.group('prefix'):
      content.parts.append(types.Part(text=pattern_match.group('prefix')))
    content.parts.append(
        CodeExecutionUtils.build_executable_code_part(code_str)
    )
    return pattern_match.group('code').decode()

  @staticmethod
  def build_executable_code_part(code: str) -> types.Part:
    """Returns the code part."""
    return types.Part.from_executable_code(
        code=code,
        language='PYTHON',
    )

  @staticmethod
  def build_code_execution_result_part(
      code_execution_result: CodeExecutionResult,
  ) -> types.Part:
    """Returns the code execution result part."""
    if code_execution_result.stderr:
      return types.Part.from_code_execution_result(
          outcome='OUTCOME_FAILED',
          output=code_execution_result.stderr,
      )
    final_result = []
    if (
        code_execution_result.stdout
        or not code_execution_result.output_artifacts
    ):
      final_result.append(
          'Code execution result:\n'
          + '```tool_outputs\n%s\n```' % code_execution_result.stdout
      )
    if code_execution_result.output_artifacts:
      final_result.append(
          'Saved artifacts:\n'
          + ','.join(
              ['`%s`' % f for f in code_execution_result.output_artifacts]
          )
      )
    return types.Part.from_code_execution_result(
        outcome='OUTCOME_OK',
        output='\n\n'.join(final_result),
    )

  @staticmethod
  def convert_code_execution_parts(content: types.Content):
    """Converts the code execution parts to text parts."""
    if not content.parts:
      return

    if content.parts[-1].executable_code:
      content.parts[-1] = types.Part(
          text='```tool_code\n%s\n```' % content.parts[-1].executable_code.code,
      )
    elif len(content.parts) == 1 and content.parts[-1].code_execution_result:
      content.parts[-1] = types.Part(
          text=content.parts[-1].code_execution_result.output,
      )
      content.role = 'user'


================================================
File: code_executor/code_executor_context.py
================================================
"""The persistent context used to configure the code executor."""

import copy
import dataclasses
from typing import Any

from ..sessions import State
from .code_execution_utils import File


_CONTEXT_KEY = '_code_execution_context'
_SESSION_ID_KEY = 'execution_session_id'
_PROCESSED_FILE_NAMES_KEY = 'processed_input_files'
_INPUT_FILE_KEY = '_code_executor_input_files'
_ERROR_COUNT_KEY = '_code_executor_error_counts'


class CodeExecutorContext:
  """The persistent context used to configure the code executor."""

  _context: dict[str, Any]

  def __init__(self, session_state: State):
    """Initializes the code executor context.

    Args:
      session_state: The session state to get the code executor context from.
    """
    self._context = self._get_code_executor_context(session_state)
    self._session_state = session_state

  def get_state_delta(self) -> dict[str, Any]:
    """Returns the state delta to update in the persistent session state."""
    context_to_update = copy.deepcopy(self._context)
    return {_CONTEXT_KEY: context_to_update}

  def get_execution_id(self) -> str | None:
    """Returns the session ID for the code executor."""
    if _SESSION_ID_KEY not in self._context:
      return None
    return self._context[_SESSION_ID_KEY]

  def set_execution_id(self, session_id: str):
    """Sets the session ID for the code executor."""
    self._context[_SESSION_ID_KEY] = session_id

  def get_processed_file_names(self) -> list[str]:
    """Returns the processed file names from the session state."""
    if _PROCESSED_FILE_NAMES_KEY not in self._context:
      return []
    return self._context[_PROCESSED_FILE_NAMES_KEY]

  def add_processed_file_names(self, file_names: [str]):
    """Adds the processed file name to the session state."""
    if _PROCESSED_FILE_NAMES_KEY not in self._context:
      self._context[_PROCESSED_FILE_NAMES_KEY] = []
    self._context[_PROCESSED_FILE_NAMES_KEY].extend(file_names)

  def get_input_files(self) -> list[File]:
    """Returns the input file names from the session state."""
    if _INPUT_FILE_KEY not in self._session_state:
      return []
    return [File(**file) for file in self._session_state[_INPUT_FILE_KEY]]

  def add_input_files(
      self,
      input_files: list[File],
  ):
    """Adds the input files to the code executor context."""
    if _INPUT_FILE_KEY not in self._session_state:
      self._session_state[_INPUT_FILE_KEY] = []
    for input_file in input_files:
      self._session_state[_INPUT_FILE_KEY].append(
          dataclasses.asdict(input_file)
      )

  def get_error_count(self, invocation_id: str) -> int:
    """Returns the error count from the session state."""
    if _ERROR_COUNT_KEY not in self._session_state:
      return 0
    return self._session_state[_ERROR_COUNT_KEY].get(invocation_id, 0)

  def increment_error_count(self, invocation_id: str):
    """Increments the error count from the session state."""
    if _ERROR_COUNT_KEY not in self._session_state:
      self._session_state[_ERROR_COUNT_KEY] = {}
    self._session_state[_ERROR_COUNT_KEY][invocation_id] = (
        self.get_error_count(invocation_id) + 1
    )

  def reset_error_count(self, invocation_id: str):
    """Resets the error count from the session state."""
    if _ERROR_COUNT_KEY not in self._session_state:
      return
    if invocation_id in self._session_state[_ERROR_COUNT_KEY]:
      del self._session_state[_ERROR_COUNT_KEY][invocation_id]

  def _get_code_executor_context(self, session_state: State) -> dict[str, Any]:
    """Returns the code executor context from the session state."""
    if _CONTEXT_KEY not in session_state:
      session_state[_CONTEXT_KEY] = {}
    return session_state[_CONTEXT_KEY]


================================================
File: code_executor/unsafe_local_code_executor.py
================================================
"""A tool that unsafely execute code directly in the current local context."""

from contextlib import redirect_stdout
import io

from pydantic import Field
from typing_extensions import override

from ..agents.invocation_context import InvocationContext
from .base_code_executor import BaseCodeExecutor
from .code_execution_utils import CodeExecutionInput
from .code_execution_utils import CodeExecutionResult


class UnsafeLocalCodeExecutor(BaseCodeExecutor):
  """A tool that unsafely execute code directly in the current local context."""

  # Overrides the BaseCodeExecutor attribute: this executor cannot be stateful.
  stateful: bool = Field(default=False, frozen=True, exclude=True)

  def __init__(self, **data):
    if 'stateful' in data and data['stateful']:
      raise ValueError('Cannot set `stateful=True` in UnsafeLocalCodeExecutor.')
    super().__init__(**data)

  @override
  def execute_code(
      self,
      invocation_context: InvocationContext,
      code_execution_input: CodeExecutionInput,
  ) -> CodeExecutionResult:
    # Execute the code.
    output = ''
    error = ''
    try:
      globals_ = {}
      locals_ = {}
      stdout = io.StringIO()
      with redirect_stdout(stdout):
        exec(code_execution_input.code, globals_, locals_)
      output = stdout.getvalue()
    except Exception as e:
      error = str(e)

    # Collect the final result.
    return CodeExecutionResult(
        stdout=output,
        stderr=error,
        output_artifacts=[],
    )


================================================
File: code_executor/vertex_code_executor.py
================================================
"""A code executor that uses Vertex Code Interpreter Extension to execute code."""

import datetime
import os
from typing import Optional

from google.genai import types
from typing_extensions import override
from vertexai.preview.extensions import Extension

from ..agents.invocation_context import InvocationContext
from .base_code_executor import BaseCodeExecutor
from .code_execution_utils import CodeExecutionInput
from .code_execution_utils import CodeExecutionResult
from .code_execution_utils import File


_SUPPORTED_IMAGE_TYPES = ['png', 'jpg', 'jpeg']
_SUPPORTED_DATA_FILE_TYPES = ['csv', 'json']

_IMPORTED_LIBRARIES = '''
import io
import math
import re

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy

def crop(s: str, max_chars: int = 64) -> str:
  """Crops a string to max_chars characters."""
  return s[: max_chars - 3] + '...' if len(s) > max_chars else s


def explore_df(df: pd.DataFrame) -> None:
  """Prints some information about a pandas DataFrame."""

  with pd.option_context(
      'display.max_columns', None, 'display.expand_frame_repr', False
  ):
    # Print the column names to never encounter KeyError when selecting one.
    df_dtypes = df.dtypes

    # Obtain information about data types and missing values.
    df_nulls = (len(df) - df.isnull().sum()).apply(
        lambda x: f'{x} / {df.shape[0]} non-null'
    )

    # Explore unique total values in columns using `.unique()`.
    df_unique_count = df.apply(lambda x: len(x.unique()))

    # Explore unique values in columns using `.unique()`.
    df_unique = df.apply(lambda x: crop(str(list(x.unique()))))

    df_info = pd.concat(
        (
            df_dtypes.rename('Dtype'),
            df_nulls.rename('Non-Null Count'),
            df_unique_count.rename('Unique Values Count'),
            df_unique.rename('Unique Values'),
        ),
        axis=1,
    )
    df_info.index.name = 'Columns'
    print(f"""Total rows: {df.shape[0]}
Total columns: {df.shape[1]}

{df_info}""")
'''


def _get_code_interpreter_extension(resource_name: str = None):
  """Returns: Load or create the code interpreter extension."""
  if not resource_name:
    resource_name = os.environ.get('CODE_INTERPRETER_EXTENSION_NAME')
  if resource_name:
    new_code_interpreter = Extension(resource_name)
  else:
    print('No CODE_INTERPRETER_ID found in the environment. Create a new one.')
    new_code_interpreter = Extension.from_hub('code_interpreter')
    os.environ['CODE_INTERPRETER_EXTENSION_NAME'] = (
        new_code_interpreter.gca_resource.name
    )
  return new_code_interpreter


class VertexCodeExecutor(BaseCodeExecutor):
  """A tool that uses Vertex Code Interpreter Extension to execute code."""

  _code_interpreter_extension: Extension

  resource_name: str = None
  """
  If set, load the existing resource name of the code interpreter extension
  instead of creating a new one.
  Format: projects/123/locations/us-central1/extensions/456
  """

  def __init__(
      self,
      resource_name: str = None,
      **data,
  ):
    super().__init__(**data)
    self.resource_name = resource_name
    self._code_interpreter_extension = _get_code_interpreter_extension(
        self.resource_name
    )

  @override
  def execute_code(
      self,
      invocation_context: InvocationContext,
      code_execution_input: CodeExecutionInput,
  ) -> CodeExecutionResult:
    # Execute the code.
    code_execution_result = self._execute_code_interpreter(
        self._get_code_with_imports(code_execution_input.code),
        code_execution_input.input_files,
        code_execution_input.execution_id,
    )

    # Save output file as artifacts.
    current_timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    file_name_prefix = '%s_' % str(current_timestamp)
    saved_files = []
    file_count = 0
    for output_file in code_execution_result['output_files']:
      file_type = output_file['name'].split('.')[-1]
      file_name = file_name_prefix + '%d.%s' % (file_count, file_type)
      if file_type in _SUPPORTED_IMAGE_TYPES:
        file_count += 1
        saved_files.append(
            File(
                name='plot_' + file_name,
                content=output_file['contents'],
                mime_type=f'image/{file_type}',
            )
        )
      elif file_type in _SUPPORTED_DATA_FILE_TYPES:
        file_count += 1
        saved_files.append(
            File(
                name='data_' + file_name,
                content=output_file['contents'],
                mime_type=f'text/{file_type}',
            )
        )
      else:
        print(f'***ERROR:*** Unsupported file type: {file_type}')
    self._save_output_files(invocation_context, saved_files)

    # Collect the final result.
    return CodeExecutionResult(
        stdout=code_execution_result.get('execution_result', ''),
        stderr=code_execution_result.get('execution_error', ''),
        output_artifacts=[f.name for f in saved_files],
    )

  def _execute_code_interpreter(
      self,
      code: str,
      input_files: Optional[list[File]] = None,
      session_id: Optional[str] = None,
  ):
    operation_params = {'code': code}
    if input_files:
      operation_params['files'] = [
          {'name': f.name, 'contents': f.content} for f in input_files
      ]
    if session_id:
      operation_params['session_id'] = session_id
    response = self._code_interpreter_extension.execute(
        operation_id='execute',
        operation_params=operation_params,
    )
    return response

  def _get_code_with_imports(self, code: str) -> str:
    return f"""
{_IMPORTED_LIBRARIES}

{code}
"""


================================================
File: evaluation/__init__.py
================================================
import logging

logger = logging.getLogger(__name__)

__all__ = ['AgentEvaluator']

try:
  from .agent_evaluator import AgentEvaluator

  __all__.append('AgentEvaluator')
except ImportError:
  logger.warning(
      'The Vertex[eval] sdk is not installed. If you want to use the Vertex'
      ' Evaluation with agents, please install it(pip install'
      ' "google-cloud-aiplatform[evaluation]). If not, you can ignore this'
      ' warning.'
  )


================================================
File: evaluation/agent_evaluator.py
================================================
import json
import os
from typing import Dict
from typing import List
from typing import Union

from .evaluation_generator import EvaluationGenerator
from .response_evaluator import ResponseEvaluator
from .trajectory_evaluator import TrajectoryEvaluator

# Constants for default runs and evaluation criteria
NUM_RUNS = 2
TOOL_TRAJECTORY_SCORE_KEY = "tool_trajectory_avg_score"
# This evaluation is not very stable.
# This is always optional unless explicitly specified.
RESPONSE_EVALUATION_SCORE_KEY = "response_evaluation_score"
RESPONSE_MATCH_SCORE_KEY = "response_match_score"

ALLOWED_CRITERIA = [
    TOOL_TRAJECTORY_SCORE_KEY,
    RESPONSE_EVALUATION_SCORE_KEY,
    RESPONSE_MATCH_SCORE_KEY,
]


QUERY_COLUMN = "query"
REFERENCE_COLUMN = "reference"
EXPECTED_TOOL_USE_COLUMN = "expected_tool_use"


DEFAULT_CRITERIA = {
    TOOL_TRAJECTORY_SCORE_KEY: 1.0,  # 1-point scale; 1.0 is perfect.
    RESPONSE_MATCH_SCORE_KEY: 0.8,  # Rouge-1 text match; 0.8 is default.
}


def load_json(file_path: str) -> Union[Dict, List]:
  with open(file_path, "r") as f:
    return json.load(f)


class AgentEvaluator:

  @staticmethod
  def find_config_for_test_file(test_file: str):
    """Find the test_config.json file in the same folder as the test file."""
    test_folder = os.path.dirname(test_file)
    config_path = os.path.join(test_folder, "test_config.json")
    if os.path.exists(config_path):
      config_data = load_json(config_path)
      if "criteria" in config_data and isinstance(
          config_data["criteria"], dict
      ):
        return config_data["criteria"]
      else:
        raise ValueError(
            f"Invalid format for test_config.json at {config_path}. Expected a"
            " 'criteria' dictionary."
        )
    return DEFAULT_CRITERIA

  @staticmethod
  def load_dataset(
      input_data: Union[str, List[str], List[Dict], List[List[Dict]]],
  ) -> List[List[Dict]]:
    def load_json_file(file_path: str) -> List[Dict]:
      data = load_json(file_path)
      if not isinstance(data, list) or not all(
          isinstance(d, dict) for d in data
      ):
        raise ValueError(f"{file_path} must contain a list of dictionaries.")
      return data

    if isinstance(input_data, str):
      if os.path.isdir(input_data):
        test_files = []
        for root, _, files in os.walk(input_data):
          for file in files:
            if file.endswith(".test.json"):
              test_files.append(os.path.join(root, file))
        return [load_json_file(f) for f in test_files]
      elif os.path.isfile(input_data):
        return [load_json_file(input_data)]
      else:
        raise ValueError(f"Input path {input_data} is invalid.")
    elif isinstance(input_data, list):
      if all(isinstance(i, str) and os.path.isfile(i) for i in input_data):
        return [load_json_file(i) for i in input_data]
      raise TypeError("Input list must contain valid file paths.")
    raise TypeError("Invalid input type for dataset loading.")

  @staticmethod
  def evaluate(
      agent_module,
      eval_dataset,
      num_runs=NUM_RUNS,
      evaluation_criteria=None,
      agent_name=None,
  ):
    evaluation_criteria = evaluation_criteria or {}
    test_files = []
    if isinstance(eval_dataset, str) and os.path.isdir(eval_dataset):
      for root, _, files in os.walk(eval_dataset):
        for file in files:
          if file.endswith(".test.json"):
            test_files.append(os.path.join(root, file))
    else:
      test_files = [eval_dataset]

    for test_file in test_files:
      dataset = AgentEvaluator.load_dataset(test_file)[0]
      criteria = AgentEvaluator.find_config_for_test_file(test_file)

      AgentEvaluator._validate_input([dataset], criteria)

      evaluation_response = AgentEvaluator._generate_responses(
          agent_module, [dataset], num_runs, agent_name
      )

      if AgentEvaluator._response_evaluation_required(criteria, [dataset]):
        AgentEvaluator._evaluate_response_scores(
            agent_module, evaluation_response, criteria
        )

      if AgentEvaluator._trajectory_evaluation_required(criteria, [dataset]):
        AgentEvaluator._evaluate_tool_trajectory(
            agent_module, evaluation_response, criteria
        )

  @staticmethod
  def _validate_input(eval_dataset, criteria):
    """Validates that the evaluation criteria align with the provided dataset.

    For efficiency, we only use first row to validate input.
    """
    if not eval_dataset:
      raise ValueError("The evaluation dataset is None or empty.")

    for key in criteria:
      if key not in ALLOWED_CRITERIA:
        raise ValueError(
            f"Invalid criteria key: {key}. Expected one of {ALLOWED_CRITERIA}."
        )

    if not eval_dataset:
      raise ValueError("The evaluation dataset is empty.")
    sample = eval_dataset[0]
    first_query = sample[0]

    if not isinstance(sample, list) and not isinstance(first_query, dict):
      raise ValueError(
          "Each evaluation dataset sample must be list of dictionary. But it's"
          f" {eval_dataset}"
      )

    if TOOL_TRAJECTORY_SCORE_KEY in criteria:
      if (
          QUERY_COLUMN not in first_query
          or EXPECTED_TOOL_USE_COLUMN not in first_query
      ):
        raise ValueError(
            f"Samples for {TOOL_TRAJECTORY_SCORE_KEY} must include"
            f" '{QUERY_COLUMN}' and '{EXPECTED_TOOL_USE_COLUMN}' keys. The"
            f" sample is {sample}."
        )

    if RESPONSE_EVALUATION_SCORE_KEY in criteria:
      if QUERY_COLUMN not in first_query:
        raise ValueError(
            f"Samples for {RESPONSE_EVALUATION_SCORE_KEY} must include"
            f" '{QUERY_COLUMN}' key. The sample is {sample}."
        )

    if RESPONSE_MATCH_SCORE_KEY in criteria:
      if QUERY_COLUMN not in first_query or REFERENCE_COLUMN not in first_query:
        raise ValueError(
            f"Samples for {RESPONSE_MATCH_SCORE_KEY} must include"
            f" '{QUERY_COLUMN}' and '{REFERENCE_COLUMN}' keys. The sample is"
            f" {sample}."
        )

  @staticmethod
  def _get_infer_criteria(eval_dataset):
    """Infers evaluation criteria based on the provided dataset.

    Args:
        eval_dataset (list): A list of evaluation samples.

    Returns:
        dict: Inferred evaluation criteria based on dataset fields.
    """
    inferred_criteria = {}
    sample = eval_dataset[0][0]

    if QUERY_COLUMN in sample and EXPECTED_TOOL_USE_COLUMN in sample:
      inferred_criteria[TOOL_TRAJECTORY_SCORE_KEY] = DEFAULT_CRITERIA[
          TOOL_TRAJECTORY_SCORE_KEY
      ]

    if QUERY_COLUMN in sample and REFERENCE_COLUMN in sample:
      inferred_criteria[RESPONSE_MATCH_SCORE_KEY] = DEFAULT_CRITERIA[
          RESPONSE_MATCH_SCORE_KEY
      ]

    return inferred_criteria

  @staticmethod
  def _generate_responses(
      agent_module, eval_dataset, num_runs, agent_name=None
  ):
    """Generates evaluation responses by running the agent module multiple times."""
    return EvaluationGenerator.generate_responses(
        eval_dataset, agent_module, repeat_num=num_runs, agent_name=agent_name
    )

  @staticmethod
  def _generate_responses_from_session(eval_dataset, session_path):
    """Generates evaluation responses by running the agent module multiple times."""
    return EvaluationGenerator.generate_responses_from_session(
        session_path, eval_dataset
    )

  @staticmethod
  def _response_evaluation_required(criteria, eval_dataset):
    """Checks if response evaluation are needed."""
    return REFERENCE_COLUMN in eval_dataset[0][0] and any(
        key in criteria
        for key in [RESPONSE_EVALUATION_SCORE_KEY, RESPONSE_MATCH_SCORE_KEY]
    )

  @staticmethod
  def _trajectory_evaluation_required(evaluation_criteria, eval_dataset):
    """Checks if response evaluation are needed."""
    return (
        EXPECTED_TOOL_USE_COLUMN in eval_dataset[0][0]
        and TOOL_TRAJECTORY_SCORE_KEY in evaluation_criteria
    )

  @staticmethod
  def _evaluate_response_scores(agent_module, evaluation_response, criteria):
    """Evaluates response scores and raises an assertion error if they don't meet the criteria."""
    metrics = ResponseEvaluator.evaluate(evaluation_response, criteria)

    AgentEvaluator._assert_score(
        metrics,
        "coherence/mean",
        criteria.get(RESPONSE_EVALUATION_SCORE_KEY),
        "Average response evaluation score",
        agent_module,
    )

    AgentEvaluator._assert_score(
        metrics,
        "rouge_1/mean",
        criteria.get(RESPONSE_MATCH_SCORE_KEY),
        "Average response match score",
        agent_module,
    )

  @staticmethod
  def _evaluate_tool_trajectory(agent_module, evaluation_response, criteria):
    """Evaluates tool trajectory scores and raises an assertion error if they don't meet the criteria."""
    score = TrajectoryEvaluator.evaluate(evaluation_response)
    AgentEvaluator._assert_score(
        {TOOL_TRAJECTORY_SCORE_KEY: score},
        TOOL_TRAJECTORY_SCORE_KEY,
        criteria[TOOL_TRAJECTORY_SCORE_KEY],
        "Average tool trajectory evaluation score",
        agent_module,
    )

  @staticmethod
  def _assert_score(metrics, metric_key, threshold, description, agent_module):
    """Asserts that a metric meets the specified threshold."""
    if metric_key in metrics:
      actual_score = metrics[metric_key]
      assert actual_score >= threshold, (
          f"{description} for {agent_module} is lower than expected. "
          f"Expected >= {threshold}, but got {actual_score}."
      )


================================================
File: evaluation/evaluation_constants.py
================================================
class EvalConstants:
  """Holds constants for evaluation file constants."""

  QUERY = "query"
  EXPECTED_TOOL_USE = "expected_tool_use"
  RESPONSE = "response"
  REFERENCE = "reference"
  TOOL_NAME = "tool_name"
  TOOL_INPUT = "tool_input"
  MOCK_TOOL_OUTPUT = "mock_tool_output"


================================================
File: evaluation/evaluation_generator.py
================================================
import importlib
from typing import Any
from typing import Callable
from typing import Dict
from typing import Optional

from agents.artifacts import InMemoryArtifactService
from agents.sessions import InMemorySessionService
from agents.sessions import Session
from google.genai import types

from ..agents.agent import BaseAgent
from ..agents.agent import BeforeToolCallback
from ..runners import Runner
from .evaluation_constants import EvalConstants


class EvaluationGenerator:
  """Generates evaluation responses for agents."""

  @staticmethod
  def generate_responses(
      eval_dataset, agent_module_path, repeat_num=3, agent_name=None
  ):

    module_name = agent_module_path.split("/")[
        -1
    ]  # e.g., 'home_automation_agent'
    results = []

    for _ in range(repeat_num):
      for data in eval_dataset:
        results.append(
            EvaluationGenerator._process_query(
                data, agent_module_path, agent_name
            )
        )

    return results

  @staticmethod
  def generate_responses_from_session(session_path, eval_dataset):
    results = []

    with open(session_path, "r") as f:
      session_data = Session.model_validate_json(f.read())
      print("loaded session", session_path)

    for data in eval_dataset:
      # load session data from session_path
      results.append(
          EvaluationGenerator._process_query_with_session(
              session_data,
              data,
          )
      )

    return results

  @staticmethod
  def _process_query(data, module_name, agent_name=None):
    """Process a query using the agent and evaluation dataset."""
    module_path = f"{module_name}"
    agent_module = importlib.import_module(module_path)
    root_agent = agent_module.agent.root_agent

    # we don't know which tools belong to which agent
    # so we just apply to any agents that has certain tool outputs
    all_mock_tools = set()
    for eval_entry in data:
      expected_tool_use = eval_entry.get(EvalConstants.EXPECTED_TOOL_USE, [])
      for expected in expected_tool_use:
        if EvalConstants.MOCK_TOOL_OUTPUT in expected:
          all_mock_tools.add(expected[EvalConstants.TOOL_NAME])

    eval_data_copy = data.copy()
    EvaluationGenerator.apply_before_tool_callback(
        root_agent,
        lambda *args: EvaluationGenerator.before_tool_callback(
            *args, eval_dataset=eval_data_copy
        ),
        all_mock_tools,
    )

    session_service = InMemorySessionService()
    session = session_service.create("test", "test_user_id")
    artifact_service = InMemoryArtifactService()
    runner = Runner(
        app_name="EvaluationGenerator",
        agent=root_agent,
        artifact_service=artifact_service,
        session_service=session_service,
    )

    # Reset agent state for each query
    reset_func = getattr(agent_module.agent, "reset_data", None)
    if callable(reset_func):
      reset_func()

    responses = data.copy()

    for index, eval_entry in enumerate(responses):
      response = None
      query = eval_entry["query"]
      content = types.Content(role="user", parts=[types.Part(text=query)])
      turn_actual_tool_uses = []

      for event in runner.run(session=session, new_message=content):
        if event.is_final_response():
          response = event.content.parts[0].text
        elif event.get_function_calls():
          for call in event.get_function_calls():
            turn_actual_tool_uses.append({
                EvalConstants.TOOL_NAME: call.name,
                EvalConstants.TOOL_INPUT: call.args,
            })

      responses[index]["actual_tool_use"] = turn_actual_tool_uses
      responses[index]["response"] = response

    return responses

  @staticmethod
  def _process_query_with_session(session_data, data):
    """Process the queries using the existing session data without invoking the runner."""
    responses = data.copy()

    # Iterate through the provided queries and align them with the session events
    for index, eval_entry in enumerate(responses):
      query = eval_entry["query"]
      actual_tool_uses = []
      response = None

      # Search for the corresponding session events
      for event in session_data.events:
        # Match the query to a user event
        if event.author == "user" and event.content.parts[0].text == query:
          # Look for subsequent tool usage or model responses
          for subsequent_event in session_data.events:
            if subsequent_event.invocation_id == event.invocation_id:
              # Extract tool usage
              if subsequent_event.content.parts[0].function_call:
                call = subsequent_event.content.parts[0].function_call
                actual_tool_uses.append(
                    {"tool_name": call.name, "tool_input": call.args}
                )
              # Extract final response
              elif subsequent_event.author != "user":
                response = subsequent_event.content.parts[0].text

      # Update the results for the current query
      responses[index]["actual_tool_use"] = actual_tool_uses
      responses[index]["response"] = response
    return responses

  @staticmethod
  def before_tool_callback(tool, args, tool_context, eval_dataset):
    """Intercept specific tool calls and return predefined outputs

    from eval_dataset.
    """
    for index, eval_entry in enumerate(eval_dataset):
      expected_tool_use = eval_entry.get("expected_tool_use", [])
      for expected in expected_tool_use:
        if (
            EvalConstants.MOCK_TOOL_OUTPUT in expected
            and tool.name == expected[EvalConstants.TOOL_NAME]
            and args == expected.get(EvalConstants.TOOL_INPUT, {})
        ):
          # pop the matched entry so we don't rematch again
          eval_dataset.pop(index)
          return {"result": expected[EvalConstants.MOCK_TOOL_OUTPUT]}

    return None

  @staticmethod
  def apply_before_tool_callback(
      agent: BaseAgent, callback: BeforeToolCallback, all_mock_tools: set[str]
  ):
    """Recursively apply the before_tool_callback to the root agent and all its subagents."""
    # check if the agent has tools that defined by evalset
    # We use function name to check if tools match
    for tool in agent.tools:
      if tool.__name__ in all_mock_tools:
        agent.before_tool_callback = callback

    # Apply recursively to subagents if they exist
    if hasattr(agent, "children") and isinstance(agent.children, list):
      for child_agent in agent.children:
        EvaluationGenerator.apply_before_tool_callback(
            child_agent, callback, all_mock_tools
        )


================================================
File: evaluation/response_evaluator.py
================================================
import pandas as pd
from tabulate import tabulate
from vertexai.preview.evaluation import EvalTask
from vertexai.preview.evaluation import MetricPromptTemplateExamples


class ResponseEvaluator:
  """Runs response evaluation for agents."""

  @staticmethod
  def evaluate(raw_eval_dataset, evaluation_criteria):
    if not raw_eval_dataset:
      raise ValueError("The evaluation dataset is empty.")

    metrics = ResponseEvaluator._get_metrics(
        raw_eval_dataset, evaluation_criteria
    )
    flattened_queries = [
        item for sublist in raw_eval_dataset for item in sublist
    ]
    eval_dataset = pd.DataFrame(flattened_queries).rename(
        columns={"query": "prompt", "expected_tool_use": "reference_trajectory"}
    )
    eval_task = EvalTask(dataset=eval_dataset, metrics=metrics)

    eval_result = eval_task.evaluate()
    ResponseEvaluator._print_results(eval_result)
    return eval_result.summary_metrics

  @staticmethod
  def _get_metrics(raw_eval_dataset, criteria):
    metrics = []
    if (
        "response_evaluation_score" in criteria
        and "query" in raw_eval_dataset[0][0]
        and "expected_tool_use" in raw_eval_dataset[0][0]
    ):
      metrics.append(MetricPromptTemplateExamples.Pointwise.COHERENCE)
    if (
        "response_match_score" in criteria
        and "reference" in raw_eval_dataset[0][0]
    ):
      metrics.append("rouge_1")
    return metrics

  @staticmethod
  def _print_results(eval_result):
    print("Evaluation Summary Metrics:", eval_result.summary_metrics)
    print(tabulate(eval_result.metrics_table, headers="keys", tablefmt="grid"))


================================================
File: evaluation/trajectory_evaluator.py
================================================
import pandas as pd
from tabulate import tabulate

from .evaluation_constants import EvalConstants


class TrajectoryEvaluator:
  """Evaluates tool trajectories for accuracy."""

  @staticmethod
  def evaluate(eval_dataset):
    results_df = pd.DataFrame(
        columns=[
            "query",
            "response",
            "actual_tool_use",
            "expected_tool_use",
            "tool_use_accuracy",
        ]
    )
    failures = []

    for conversation in eval_dataset:
      for row in conversation:
        new_row, failure = TrajectoryEvaluator._evaluate_row(row)
        results_df = pd.concat(
            [results_df, pd.DataFrame([new_row])], ignore_index=True
        )
        if failure:
          failures.append(failure)

    TrajectoryEvaluator._report_failures(failures)
    TrajectoryEvaluator._print_results(results_df)

    return results_df["tool_use_accuracy"].mean()

  @staticmethod
  def _evaluate_row(row):
    # We don't evaluate the mock tool outputs.
    expected = TrajectoryEvaluator._remove_tool_outputs(
        row["expected_tool_use"]
    )
    actual = row["actual_tool_use"]
    tool_use_accuracy = (
        1.0 if TrajectoryEvaluator.are_tools_equal(actual, expected) else 0.0
    )

    new_row = {
        "query": row["query"],
        "response": row["response"],
        "actual_tool_use": actual,
        "expected_tool_use": expected,
        "tool_use_accuracy": tool_use_accuracy,
    }
    failure = (
        None
        if tool_use_accuracy == 1.0
        else {"query": row["query"], "actual": actual, "expected": expected}
    )
    return new_row, failure

  @staticmethod
  def are_tools_equal(list_a_original, list_b_original):
    # Remove other entries that we don't want to evaluate
    list_a = [
        {"tool_name": tool["tool_name"], "tool_input": tool["tool_input"]}
        for tool in list_a_original
    ]

    list_b = [
        {"tool_name": tool["tool_name"], "tool_input": tool["tool_input"]}
        for tool in list_b_original
    ]

    return list_a == list_b

  @staticmethod
  def _remove_tool_outputs(tool_use_list):
    """Removes 'mock_tool_output' from each dictionary in the list."""
    result = []
    for tool_use in tool_use_list:
      new_tool_use = (
          tool_use.copy()
      )  # Create a copy to avoid modifying the original
      new_tool_use.pop(
          EvalConstants.MOCK_TOOL_OUTPUT, None
      )  # Remove 'tool_output' if it exists
      result.append(new_tool_use)
    return result

  @staticmethod
  def _report_failures(failures):
    if failures:
      print("Failures:")
      for failure in failures:
        print(f"""{{
                    "query": '{failure["query"]}',
                    "actual": {failure["actual"]},
                    "expected_tool_use": {failure["expected"]},
                }}""")

  @staticmethod
  def _print_results(results_df):
    print(tabulate(results_df, headers="keys", tablefmt="grid"))


================================================
File: events/__init__.py
================================================
from .event import Event
from .event_actions import EventActions


================================================
File: events/event.py
================================================
import random
import string
from datetime import datetime
from typing import Optional

from google.genai import types
from pydantic import ConfigDict
from pydantic import Field

from ..models.llm_response import LlmResponse
from .event_actions import EventActions


class Event(LlmResponse):
  model_config = ConfigDict(extra='forbid')

  # TODO: Change to required field after session files are all migrated.
  invocation_id: str | None = None
  # The name of the agent that sent the event, or user.
  author: str
  actions: EventActions = Field(default_factory=EventActions)
  # Whether this is a greeting message. When this is true, the content is not
  # sent to the model for future conversation. This is because we found that
  # the agent transfer rate is low when the greeting message is present. Perhaps
  # because the root_agent thinks it can handle all requests based on the
  # greeting message.
  is_greeting: bool | None = None

  function_call_event_id: Optional[str] = None
  """The ID of the function call event that this event is responding to."""

  # The following are computed fields.
  # No not assign the ID. It will be assigned by the session.
  id: str = ''
  timestamp: float = Field(default_factory=lambda: datetime.now().timestamp())

  def model_post_init(self, __context):
    # Generates a random ID for the event.
    if not self.id:
      self.id = Event.new_id()

  def is_final_response(self) -> bool:
    if self.actions.skip_summarization:
      return True
    return (
        not self.get_function_calls()
        and not self.get_function_responses()
        and not self.partial
        and not self.has_trailing_code_exeuction_result()
    )

  def get_function_calls(self) -> list[types.FunctionCall]:
    func_calls = []
    if self.content and self.content.parts:
      for part in self.content.parts:
        if part.function_call:
          func_calls.append(part.function_call)
    return func_calls

  def get_function_responses(self) -> list[types.FunctionResponse]:
    func_response = []
    if self.content and self.content.parts:
      for part in self.content.parts:
        if part.function_response:
          func_response.append(part.function_response)
    return func_response

  def has_trailing_code_exeuction_result(
      self,
  ) -> bool:
    if self.content:
      if self.content.parts:
        return self.content.parts[-1].code_execution_result is not None
    return False

  @staticmethod
  def new_id():
    characters = string.ascii_letters + string.digits
    return ''.join(random.choice(characters) for _ in range(8))

  @classmethod
  def from_function_response(
      cls, function_call_event, function_call, function_result
  ):
    content = types.Content(
        role='user',
        parts=[
            types.Part(
                function_response=types.FunctionResponse(
                    name=function_call.name, response=function_result
                )
            )
        ],
    )
    return cls(
        invocation_id=function_call_event.invocation_id,
        author=function_call_event.author,
        content=content,
        function_call_event_id=function_call_event.id,
    )


================================================
File: events/event_actions.py
================================================
from __future__ import annotations

from typing import Optional

from pydantic import BaseModel
from pydantic import ConfigDict
from pydantic import Field


class EventActions(BaseModel):
  model_config = ConfigDict(extra='forbid')

  skip_summarization: Optional[bool] = None
  """If true, it won't call model to summarize function response.

  Only used for function_response event.
  """

  state_delta: dict[str, object] = Field(default_factory=dict)
  """Indicates that the event is updating the state with the given delta."""

  artifact_delta: dict[str, int] = Field(default_factory=dict)
  """Indicates that the event is updating an artifact. key is the filename,
  value is the version."""

  pending: Optional[bool] = None
  """If true, the tool call is pending and the result is not yet available.

  Only used for function_call event.
  """

  transfer_to_agent: Optional[str] = None
  """If set, the event transfers to the specified agent."""

  escalate: Optional[bool] = None
  """The agent is escalating to a higher level agent."""


================================================
File: examples/__init__.py
================================================
import logging
from . import example_util
from .base_example_provider import BaseExampleProvider
from .example import Example

logger = logging.getLogger(__name__)

__all__ = [
    'BaseExampleProvider',
    'Example',
    'example_util',
]

try:
  from .vertex_example_store import VertexExampleStore

  __all__.append('VertexExampleStore')
except ImportError:
  logger.warning(
      'The Vertex private preview sdk for example store is not installed. If'
      ' you want to use the Vertex example store with agents, please install'
      ' it. If not, you can ignore this warning.'
  )


================================================
File: examples/base_example_provider.py
================================================
from .example import Example


# A class that provides examples for a given query.
class BaseExampleProvider:
  def get_examples(self, query: str) -> list[Example]:
    raise NotImplementedError()


================================================
File: examples/example.py
================================================
from google.genai import types
from pydantic import BaseModel


# An few-shot example.
class Example(BaseModel):
  input: types.Content
  output: list[types.Content]


================================================
File: examples/example_util.py
================================================
"""Utility functions for converting examples to a string that can be used in system instructions in the prompt."""

import logging
from typing import TYPE_CHECKING

from ..agents.invocation_context import InvocationContext
from .base_example_provider import BaseExampleProvider
from .example import Example

if TYPE_CHECKING:
  from ..sessions.session import Session

logger = logging.getLogger(__name__)

# Constant parts of the example string
_EXAMPLES_INTRO = (
    "<EXAMPLES>\nBegin few-shot\nThe following are examples of user queries and"
    " model responses using the available python libraries.\n\n"
)
_EXAMPLES_END = (
    "End few-shot\nNow, try to follow these examples and complete the following"
    " conversation\n<EXAMPLES>"
)
_EXAMPLE_START = "EXAMPLE {}:\nBegin example\n"
_EXAMPLE_END = "End example\n\n"
_USER_PREFIX = "[user]\n"
_MODEL_PREFIX = "[model]\n"
_FUNCTION_CALL_PREFIX = "```tool_code\n"
_FUNCTION_CALL_SUFFIX = "))\n```\n"
_FUNCTION_RESPONSE_PREFIX = "```tool_outputs\n"
_FUNCTION_RESPONSE_SUFFIX = "\n```\n"


def convert_examples_to_text(examples: list[Example]) -> str:
  """Converts a list of examples to a string that can be used in a system instruction."""
  examples_str = ""
  for example_num, example in enumerate(examples):
    output = (
        f"{_EXAMPLE_START.format(example_num + 1)}{_USER_PREFIX}{example.input.parts[0].text}\n\n"
    )

    for content in example.output:
      role_prefix = _MODEL_PREFIX if content.role == "model" else _USER_PREFIX
      for part in content.parts:
        if part.function_call:
          args = []
          # Convert function call part to python-like function call
          for k, v in part.function_call.args.items():
            if isinstance(v, str):
              args.append(f"{k}='{v}'")
            else:
              args.append(f"{k}={v}")
          output += (
              f"{role_prefix}{_FUNCTION_CALL_PREFIX}{part.function_call.name}({', '.join(args)}){_FUNCTION_CALL_SUFFIX}"
          )
        # Convert function response part to json string
        elif part.function_response:
          output += f"{_FUNCTION_RESPONSE_PREFIX}{part.function_response.__dict__}{_FUNCTION_RESPONSE_SUFFIX}"
        elif part.text:
          output += f"{role_prefix}{part.text}\n"

    output += _EXAMPLE_END
    examples_str += output

  return f"{_EXAMPLES_INTRO}{examples_str}{_EXAMPLES_END}"


def _get_latest_message_from_user(session: "Session") -> str:
  """Gets the latest message from the user.

  Returns:
    The latest message from the user. If not found, returns an empty string.
  """
  events = session.events
  if not events:
    return ""

  event = events[-1]
  if event.author == "user" and not event.get_function_responses():
    if event.content.parts and event.content.parts[0].text:
      return event.content.parts[0].text
    else:
      logger.warning("No message from user for fetching example.")

  return ""


def build_example_si(invocation_context: InvocationContext) -> str:
  """Builds the example string for the system instruction."""
  agent = invocation_context.agent
  # User content for greeting prompt is empty.
  if not agent.examples or not invocation_context.user_content:
    return ""

  user_content_text = invocation_context.user_content.parts[0].text
  if not user_content_text:
    return ""

  if isinstance(agent.examples, list):
    return convert_examples_to_text(agent.examples)
  if isinstance(agent.examples, BaseExampleProvider):
    return convert_examples_to_text(
        agent.examples.get_examples(user_content_text)
    )

  logger.error("Invalid example configuration for agent: %s.", agent.name)
  return ""


================================================
File: examples/vertex_example_store.py
================================================
"""Provides examples from vertex example store."""

from google.cloud.aiplatform.private_preview import example_stores
from google.genai import types
from .base_example_provider import BaseExampleProvider
from .example import Example


class VertexExampleStore(BaseExampleProvider):

  def __init__(self, examples_store_name: str):
    self.examples_store_name = examples_store_name

  def get_examples(self, query: str) -> list[Example]:
    example_store = example_stores.ExampleStore(self.examples_store_name)
    # Retrieve relevant examples.
    request = {
        "stored_contents_example_parameters": {
            "content_search_key": {
                "contents": [{"role": "user", "parts": [{"text": query}]}],
                "search_key_generation_method": {"last_entry": {}},
            }
        },
        "top_k": 10,
        "example_store": self.examples_store_name,
    }
    response = example_store.api_client.search_examples(request)

    # Convert results to genai formats
    for result in response.results:
      if result.similarity_score < 0.5:
        continue
      expected_contents = [
          content.content
          for content in result.example.stored_contents_example.contents_example.expected_contents
      ]
      expected_output = []
      for content in expected_contents:
        expected_parts = []
        for part in content.parts:
          if part.text:
            expected_parts.append(types.Part.from_text(text=part.text))
          elif part.function_call:
            expected_parts.append(
                types.Part.from_function_call(
                    name=part.function_call.name,
                    args={
                        key: value
                        for key, value in part.function_call.args.items()
                    },
                )
            )
          elif part.function_response:
            expected_parts.append(
                types.Part.from_function_response(
                    name=part.function_response.name,
                    response={
                        key: value
                        for key, value in part.function_response.response.items()
                    },
                )
            )
        expected_output.append(
            types.Content(role=content.role, parts=expected_parts)
        )

      yield Example(
          input=types.Content(
              role="user",
              parts=[
                  types.Part.from_text(
                      text=result.example.stored_contents_example.search_key
                  )
              ],
          ),
          output=expected_output,
      )


================================================
File: flows/__init__.py
================================================
from .base_flow import BaseFlow
from .llm_flows.auto_flow import AutoFlow
from .llm_flows.single_flow import SingleFlow
from .loop_flow import LoopFlow
from .registry import FlowRegistry
from .sequential_flow import SequentialFlow

__all__ = [
    'AutoFlow',
    'BaseFlow',
    'FlowRegistry',
    'LoopFlow',
    'SequentialFlow',
    'SingleFlow',
]


FlowRegistry.register('loop', LoopFlow)
FlowRegistry.register('sequential', SequentialFlow)
FlowRegistry.register('unit', SingleFlow)
FlowRegistry.register('auto', AutoFlow)
FlowRegistry.register('single', SingleFlow)


================================================
File: flows/base_flow.py
================================================
from abc import ABC
from abc import abstractmethod
from typing import Generator

from ..events import Event
from ..agents.invocation_context import InvocationContext


class BaseFlow(ABC):
  """Interface for the execution flows to run a group of agents."""

  @abstractmethod
  def __call__(
      self,
      invocation_context: InvocationContext
  ) -> Generator[Event, None, None]:
    """Run this flow.

    To extend, the flow should follow the below requirements:

    1. `session` should be treated as immutable, DO NOT change it.

    2. The caller who trigger the flow is responsible for updating the session
    as the events being generated. The subclass implentation will assume
    session is updated after each yield event statement.

    3. A flow may spawn children flows dependeing on the agent definition.
    """
    pass


================================================
File: flows/loop_flow.py
================================================
from typing import Generator

from ..agents.invocation_context import InvocationContext
from ..events import Event
from .base_flow import BaseFlow


class LoopFlow(BaseFlow):
  def __init__(self, max_iterations: int = 0):
    self.max_iterations = max_iterations

  # Runs children in sequence unless one agent sets escalate action.
  def __call__(
      self,
      invocation_context: InvocationContext,
  ) -> Generator[Event, None, None]:
    times_looped = 0
    while True:
      for child in invocation_context.agent.children:
        for event in child.run(invocation_context):
          yield event
          # Escalating means ending the loop.
          if event.actions.escalate:
            return
      times_looped += 1
      if self.max_iterations and times_looped >= self.max_iterations:
        return


================================================
File: flows/registry.py
================================================
"""The registry class for flows."""

import typing
from abc import ABC

if typing.TYPE_CHECKING:
  from .base_flow import BaseFlow


_flow_registry_dict: dict[str, type['BaseFlow']] = {}


class FlowRegistry(ABC):
  """Registry for the flow."""

  @staticmethod
  def new_flow(flow_name: str) -> 'BaseFlow':
    if flow_name not in _flow_registry_dict:
      raise ValueError(f'Flow {flow_name} not found.')

    return _flow_registry_dict[flow_name]()

  @staticmethod
  def register(flow_name: str, subclass: type['BaseFlow']):
    if flow_name in _flow_registry_dict:
      raise ValueError(f'Flow {flow_name} already exists.')

    _flow_registry_dict[flow_name] = subclass

  @staticmethod
  def resolve(flow_name: str) -> type['BaseFlow']:
    if flow_name not in _flow_registry_dict:
      raise ValueError(f'Flow {flow_name} not found.')
    return _flow_registry_dict[flow_name]


================================================
File: flows/sequential_flow.py
================================================
from typing import Generator

from ..agents.invocation_context import InvocationContext
from ..events import Event
from .base_flow import BaseFlow


class SequentialFlow(BaseFlow):

  def __call__(
      self,
      invocation_context: InvocationContext,
  ) -> Generator[Event, None, None]:
    for child in invocation_context.agent.children:
      yield from child.run(invocation_context)


================================================
File: flows/llm_flows/__init__.py
================================================
from . import _code_execution
from . import _nl_planning
from . import contents
from . import examples
from . import functions
from . import identity
from . import instructions


================================================
File: flows/llm_flows/_code_execution.py
================================================
"""Handles Code Execution related logic."""

import dataclasses
import os
import re
from typing import Generator

from google.genai import types

from ...agents.invocation_context import InvocationContext
from ...code_executor import CodeExecutionInput
from ...code_executor import CodeExecutionResult
from ...code_executor import CodeExecutionUtils
from ...code_executor import CodeExecutorContext
from ...code_executor import File
from ...events import Event
from ...events import EventActions
from ...models import LlmRequest
from ...models import LlmResponse


@dataclasses.dataclass
class DataFileUtil:
  """A structure that contains a data file name and its content."""

  extension: str
  """
  The file extension (e.g., ".csv").
  """

  loader_code_template: str
  """
  The code template to load the data file.
  """


_DATA_FILE_UTIL_MAP = {
    'text/csv': DataFileUtil(
        extension='.csv',
        loader_code_template="pd.read_csv('{filename}')",
    ),
}

_DATA_FILE_HELPER_LIB = '''
import pandas as pd

def explore_df(df: pd.DataFrame) -> None:
  """Prints some information about a pandas DataFrame."""

  with pd.option_context(
      'display.max_columns', None, 'display.expand_frame_repr', False
  ):
    # Print the column names to never encounter KeyError when selecting one.
    df_dtypes = df.dtypes

    # Obtain information about data types and missing values.
    df_nulls = (len(df) - df.isnull().sum()).apply(
        lambda x: f'{x} / {df.shape[0]} non-null'
    )

    # Explore unique total values in columns using `.unique()`.
    df_unique_count = df.apply(lambda x: len(x.unique()))

    # Explore unique values in columns using `.unique()`.
    df_unique = df.apply(lambda x: crop(str(list(x.unique()))))

    df_info = pd.concat(
        (
            df_dtypes.rename('Dtype'),
            df_nulls.rename('Non-Null Count'),
            df_unique_count.rename('Unique Values Count'),
            df_unique.rename('Unique Values'),
        ),
        axis=1,
    )
    df_info.index.name = 'Columns'
    print(f"""Total rows: {df.shape[0]}
Total columns: {df.shape[1]}

{df_info}""")
'''


def process_llm_request(
    invocation_context: InvocationContext,
    llm_request: LlmRequest,
):
  # Convert the code execution parts to text parts.
  for content in llm_request.contents:
    CodeExecutionUtils.convert_code_execution_parts(content)

  for event in _run_pre_processor(invocation_context, llm_request):
    yield event


def process_llm_response(
    invocation_context: InvocationContext,
    llm_response: LlmResponse,
):
  for event in _run_post_processor(invocation_context, llm_response):
    yield event


def _run_pre_processor(
    invocation_context: InvocationContext,
    llm_request: LlmRequest,
) -> Generator[Event, None, None]:
  """Pre-process the user message by adding the user message to the Colab notebook."""
  agent = invocation_context.agent
  code_executor = agent.code_executor

  if not code_executor or not code_executor.optimize_data_file:
    return

  code_executor_context = CodeExecutorContext(invocation_context.session.state)

  # Skip if the error count exceeds the max retry attempts.
  if (
      code_executor_context.get_error_count(invocation_context.invocation_id)
      >= code_executor.error_retry_attempts
  ):
    return

  # [Step 1] Extract data files from the session_history and store them in
  # memory. Meanwhile, mutate the inline data file to text part in session
  # history from all turns.
  all_input_files = _extrac_and_replace_inline_files(
      code_executor_context, llm_request
  )

  # [Step 2] Run Explore_Df code on the data files from the current turn. We
  # only need to explore the new data files because the previous data files
  # should already be explored and cached in the code execution runtime.
  processed_file_names = set(code_executor_context.get_processed_file_names())
  files_to_process = [
      f for f in all_input_files if f.name not in processed_file_names
  ]
  for file in files_to_process:
    code_str = _get_data_file_preprocessing_code(file)
    # Skip for unsupported file or executor types.
    if not code_str:
      return

    yield Event(
        invocation_id=invocation_context.invocation_id,
        author=agent.name,
        content=types.Content(
            role='model',
            parts=[
                types.Part(text=f'Processing input file: `{file.name}`'),
                CodeExecutionUtils.build_executable_code_part(code_str),
            ],
        ),
    )

    execute_code_result = code_executor.execute_code(
        invocation_context,
        CodeExecutionInput(
            code=code_str,
            input_files=[file],
            execution_id=_get_or_set_execution_id(
                invocation_context, code_executor_context
            ),
        ),
    )
    # Add the processed file name to the code executor context.
    code_executor_context.add_processed_file_names([file.name])

    yield Event(
        invocation_id=invocation_context.invocation_id,
        author=agent.name,
        content=types.Content(
            role='model',
            parts=[
                CodeExecutionUtils.build_code_execution_result_part(
                    execute_code_result
                ),
            ],
        ),
        actions=EventActions(
            state_delta=code_executor_context.get_state_delta()
        ),
    )

    if execute_code_result.stderr:
      code_executor_context.increment_error_count(
          invocation_context.invocation_id
      )
    else:
      code_executor_context.reset_error_count(invocation_context.invocation_id)


def _run_post_processor(
    invocation_context: InvocationContext,
    llm_response,
) -> Generator[Event, None, None]:
  """Post-process the model response by extracting and executing the first code block."""
  agent = invocation_context.agent
  code_executor = agent.code_executor

  if not code_executor or not llm_response or not llm_response.content:
    return

  code_executor_context = CodeExecutorContext(invocation_context.session.state)
  # Skip if the error count exceeds the max retry attempts.
  if (
      code_executor_context.get_error_count(invocation_context.invocation_id)
      >= code_executor.error_retry_attempts
  ):
    return

  # [Step 1] Extract code from the model predict response and truncate the
  # content to the part with the first code block.
  response_content = llm_response.content
  code_str = CodeExecutionUtils.extract_code_and_truncate_content(
      response_content
  )
  # Terminal state: no code to execute.
  if not code_str:
    return

  # [Step 2] Executes the code and emit 2 Events for code and execution result.
  yield Event(
      invocation_id=invocation_context.invocation_id,
      author=agent.name,
      content=response_content,
      actions=EventActions(),
  )

  execute_code_result = code_executor.execute_code(
      invocation_context,
      CodeExecutionInput(
          code=code_str,
          input_files=code_executor_context.get_input_files(),
          execution_id=_get_or_set_execution_id(
              invocation_context, code_executor_context
          ),
      ),
  )

  yield Event(
      invocation_id=invocation_context.invocation_id,
      author=agent.name,
      content=types.Content(
          role='model',
          parts=[
              CodeExecutionUtils.build_code_execution_result_part(
                  execute_code_result
              ),
          ],
      ),
      actions=EventActions(state_delta=code_executor_context.get_state_delta()),
  )

  if execute_code_result.stderr:
    code_executor_context.increment_error_count(
        invocation_context.invocation_id
    )
  else:
    code_executor_context.reset_error_count(invocation_context.invocation_id)

  # [Step 3] Skip processing the original model response
  # to continue code generation loop.
  llm_response.content = None


def _extrac_and_replace_inline_files(
    code_executor_context: CodeExecutorContext,
    llm_request: LlmRequest,
) -> list[File]:
  """Extracts and replaces inline files with file names in the LLM request."""
  all_input_files = code_executor_context.get_input_files()
  saved_file_names = set(f.name for f in all_input_files)

  # [Step 1] Process input files from LlmRequest and cache them in CodeExecutor.
  for i in range(len(llm_request.contents)):
    content = llm_request.contents[i]
    # Only process the user message.
    if content.role != 'user':
      continue

    for j in range(len(content.parts)):
      part = content.parts[j]
      # Skip if the inline data is not supported.
      if (
          not part.inline_data
          or part.inline_data.mime_type not in _DATA_FILE_UTIL_MAP
      ):
        continue

      # Replace the inline data file with a file name placeholder.
      mime_type = part.inline_data.mime_type
      file_name = f'data_{i+1}_{j+1}' + _DATA_FILE_UTIL_MAP[mime_type].extension
      llm_request.contents[i].parts[j] = types.Part(
          text='\nAvailable file: `%s`\n' % file_name
      )

      # Add the inlne data as input file to the code executor context.
      file = File(
          name=file_name,
          content=CodeExecutionUtils.get_encoded_file_content(
              part.inline_data.data
          ).decode(),
          mime_type=mime_type,
      )
      if file_name not in saved_file_names:
        code_executor_context.add_input_files([file])
        all_input_files.append(file)

  return all_input_files


def _get_or_set_execution_id(
    invocation_context: InvocationContext,
    code_executor_context: CodeExecutorContext,
) -> str | None:
  """Returns the ID for stateful code execution or None if not stateful."""
  if not invocation_context.agent.code_executor.stateful:
    return None

  execution_id = code_executor_context.get_execution_id()
  if not execution_id:
    execution_id = invocation_context.session.id
    code_executor_context.set_execution_id(execution_id)
  return execution_id


def _get_data_file_preprocessing_code(file: File) -> str | None:
  """Returns the code to explore the data file."""

  def _get_normalized_file_name(file_name: str) -> str:
    var_name, _ = os.path.splitext(file_name)
    # Replace non-alphanumeric characters with underscores
    var_name = re.sub(r'[^a-zA-Z0-9_]', '_', var_name)

    # If the filename starts with a digit, prepend an underscore
    if var_name[0].isdigit():
      var_name = '_' + var_name
    return var_name

  if file.mime_type not in _DATA_FILE_UTIL_MAP:
    return

  var_name = _get_normalized_file_name(file.name)
  loader_code = _DATA_FILE_UTIL_MAP[file.mime_type].loader_code_template.format(
      filename=file.name
  )
  return f"""
{_DATA_FILE_HELPER_LIB}

# Load the dataframe.
{var_name} = {loader_code}

# Use `explore_df` to guide my analysis.
explore_df({var_name})
"""


================================================
File: flows/llm_flows/_nl_planning.py
================================================
"""Handles NL planning related logic."""

from google.genai import types
from ...agents.invocation_context import InvocationContext
from ...models import LlmRequest
from ...models import LlmResponse


def process_llm_request(
    invocation_context: InvocationContext,
    llm_request: LlmRequest,
):
  if not invocation_context.agent.planning:
    return
  llm_request.append_instructions([get_nl_planner_instruction()])


def process_llm_response(
    invocation_context: InvocationContext,
    llm_response: LlmResponse,
):
  if not invocation_context.agent.planning:
    return
  post_process_planner_response(llm_response.content)


def get_nl_planner_instruction():
  """Returns: NL planner instruction."""

  high_level_preamble = """
You are an intelligent tool use agent built upon the Gemini large language model. When answering the question, try to leverage the available tools to gather the information instead of your memorized knowledge.

Follow this process when answering the question: (1) first come up with a plan in natural language text format; (2) Then use tools to execute the plan and provide reasoning between tool code snippets to make a summary of current state and next step. Tool code snippets and reasoning should be interleaved with each other. (3) In the end, return one final answer.

Follow this format when answering the question: (1) The planning part should be under /*PLANNING*/. (2) The tool code snippets should be under /*ACTION*/, and the reasoning parts should be under /*REASONING*/. (3) The final answer part should be under /*FINAL_ANSWER*/.
"""

  planning_preamble = """
Below are the requirements for the planning:
The plan is made to answer the user query if following the plan. The plan is coherent and covers all aspects of information from user query, and only involves the tools that are accessible by the agent. The plan contains the decomposed steps as a numbered list where each step should use one or multiple available tools. By reading the plan, you can intuitively know which tools to trigger or what actions to take.
If the initial plan cannot be successfully executed, you should learn from previous execution results and revise your plan. The revised plan should be be under /*REPLANNING*/. Then use tools to follow the new plan.
"""

  reasoning_preamble = """
Below are the requirements for the reasoning:
The reasoning makes a summary of the current trajectory based on the user query and tool outputs. Based on the tool outputs and plan, the reasoning also comes up with instructions to the next steps, making the trajectory closer to the final answer.
"""

  final_answer_preamble = """
Below are the requirements for the final answer:
The final answer should be precise and follow query formatting requirements. Some queries may not be answerable with the available tools and information. In those cases, inform the user why you cannot process their query and ask for more information.
"""

  # Only contains the requirements for custom tool/libraries.
  tool_code_without_python_libraries_preamble = """
Below are the requirements for the tool code:

**Custom Tools:** The available tools are described in the context and can be directly used.
- Code must be valid self-contained Python snippets with no imports and no references to tools or Python libraries that are not in the context.
- You cannot use any parameters or fields that are not explicitly defined in the APIs in the context.
- Use "print" to output execution results for the next step or final answer that you need for responding to the user. Never generate ```tool_outputs yourself.
- The code snippets should be readable, efficient, and directly relevant to the user query and reasoning steps.
- When using the tools, you should use the library name together with the function name, e.g., vertex_search.search().
- If Python libraries are not provided in the context, NEVER write your own code other than the function calls using the provided tools.
"""

  user_input_preamble = """
VERY IMPORTANT instruction that you MUST follow in addition to the above instructions:

You should ask for clarification if you need more information to answer the question.
You should prefer using the information available in the context instead of repeated tool use.

You should ONLY generate code snippets prefixed with "```tool_code" if you need to use the tools to answer the question.

If you are asked to write code by user specifically,
- you should ALWAYS use "```python" to format the code.
- you should NEVER put "tool_code" to format the code.
- Good example:
```python
print('hello')
```
- Bad example:
```tool_code
print('hello')
```
"""

  return '\n\n'.join([
      high_level_preamble,
      planning_preamble,
      reasoning_preamble,
      final_answer_preamble,
      tool_code_without_python_libraries_preamble,
      user_input_preamble,
  ])


def post_process_planner_response(
    response_content: types.Content | None,
) -> None:
  """Post-process the model response by truncting everything after the first

  function call block.
  """
  if not response_content or not response_content.parts:
    return None

  preserved_parts = []
  for i in range(len(response_content.parts)):
    # Stop at the first (group of) function calls.
    # Ignore and filter out function calls with empty names.
    if (
        not response_content.parts[i].function_call
        or response_content.parts[i].function_call.name
    ):
      preserved_parts.append(response_content.parts[i])

    if preserved_parts and preserved_parts[-1].function_call:
      j = i + 1
      while j < len(response_content.parts):
        if response_content.parts[j].function_call:
          preserved_parts.append(response_content.parts[j])
          j += 1
        else:
          break
      break

  response_content.parts = preserved_parts


================================================
File: flows/llm_flows/agent_transfer.py
================================================
"""Handles agent transfer for LLM flow."""

import typing

from ...agents.invocation_context import InvocationContext
from ...models import LlmRequest
from ...tools import transfer_to_agent

if typing.TYPE_CHECKING:
  from ...agents import Agent
  from ...agents import BaseAgent


def process_llm_request(
    invocation_context: InvocationContext,
    llm_request: LlmRequest,
):
  transfer_targets = __get_transfer_targets(invocation_context.agent)
  if not transfer_targets:
    return

  llm_request.append_instructions([
      __build_target_agents_instructions(
          invocation_context.agent, transfer_targets
      )
  ])

  llm_request.append_tools([transfer_to_agent])


def __build_target_agents_info(target_agent: 'BaseAgent') -> str:
  return f"""
Agent name: {target_agent.name}
Agent description: {target_agent.description}
"""


line_break = '\n'


def __build_target_agents_instructions(
    agent: 'Agent', target_agents: list['BaseAgent']
) -> str:
  si = f"""
You have a list of other agents to transfer to:

{line_break.join([
    __build_target_agents_info(target_agent) for target_agent in target_agents
])}

If you are the best to answer the question according to your description, you
can answer it.

If another agent is better for answering the question according to its
description, call `{_TRANSFER_TO_AGENT_FUNCTION_NAME}` function to transfer the
question to that agent. When transfering, do not generate any text other than
the function call.
"""

  if agent.parent_agent:
    si += f"""
Your parent agent is {agent.parent_agent.name}. If neither the other agents nor
you are best for answering the question according to the descriptions, transfer
to your parent agent. If you don't have parent agent, try answer by yourself.
"""
  return si


_TRANSFER_TO_AGENT_FUNCTION_NAME = transfer_to_agent.__name__


def __get_transfer_targets(agent: 'Agent') -> list['BaseAgent']:
  result = []
  result.extend(agent.children)

  if agent.parent_agent and not agent.disable_sibling_agent_transfer:
    result.append(agent.parent_agent)
    for sibling in agent.parent_agent.children:
      if sibling.name != agent.name:
        result.append(sibling)

  return result


================================================
File: flows/llm_flows/auto_flow.py
================================================
"""Implementation of AutoFlow."""

from . import agent_transfer
from .single_flow import SingleFlow


class AutoFlow(SingleFlow):
  """AutoFlow is SingleFlow with agent transfer capability.

  Agent transfer is allowed in the following direction:

  1. from parent to child;
  2. from child to parent;
  3. from child to its sibling;

  For sibling transfers, it's only enabled when all below conditions are met:

  - The parent agent is also of AutoFlow;
  - `disable_sibling_agent_transfer` option of this agent is False (default).

  Depending on the target agent flow type, the transfer may be automatically
  reversed. The condition is as below:

  - If the flow type of the tranferee agent is also auto, transfee agent will
    remain as the active agent. The transfee agent will respond to the user's
    next message directly.
  - If the flow type of the transfere agent is not auto, the active agent will
    be reversed back to previous agent.

  TODO: allow user to config auto-reverse function.
  """

  def __init__(self):
    super().__init__()
    self.request_processors += [agent_transfer]


================================================
File: flows/llm_flows/base_llm_flow.py
================================================
from __future__ import annotations

import asyncio
import logging
from types import ModuleType
from typing import AsyncGenerator
from typing import cast
from typing import Generator
from typing import TYPE_CHECKING

from websockets.exceptions import ConnectionClosedOK

from ...agents.callback_context import CallbackContext
from ...agents.invocation_context import InvocationContext
from ...agents.live_request_queue import LiveRequestQueue
from ...events import Event
from ...models import LlmRequest
from ...models import LlmResponse
from ...models.base_llm_connection import BaseLlmConnection
from ...models.registry import LLMRegistry
from ...telemetry import trace_call_llm
from ...telemetry import trace_send_data
from ...telemetry import tracer
from ...tools import BaseTool
from ...tools import ToolContext
from ..base_flow import BaseFlow
from . import functions

if TYPE_CHECKING:
  from ...agents.agent import Agent


logger = logging.getLogger(__name__)


def iter_over_async(ait, loop):
  ait = ait.__aiter__()

  async def get_next():
    try:
      obj = await ait.__anext__()
      return False, obj
    except StopAsyncIteration:
      return True, None

  while True:
    done, obj = loop.run_until_complete(get_next())
    if done:
      break
    yield obj


def sync_generator(async_gen):
  loop = asyncio.get_event_loop()
  sync_gen = iter_over_async(async_gen, loop)
  return sync_gen


class BaseLlmFlow(BaseFlow):
  """A basic flow that calls the LLM in a loop until a final response is generated.

  This flow ends when it transfer to another agent.
  """

  def __init__(self):
    self.request_processors: list[ModuleType] = []
    self.response_processors: list[ModuleType] = []

  async def call_live(
      self,
      invocation_context: InvocationContext,
  ) -> AsyncGenerator[Event, None]:
    """Process input asynchronously and yield events."""
    llm_request = LlmRequest()
    event_id = Event.new_id()

    # Preprocess before calling the LLM.
    for event in self._preprocess(invocation_context, llm_request):
      yield event
    if invocation_context.end_invocation:
      return

    llm = self.__get_llm(invocation_context)

    async with llm.connect(llm_request) as llm_connection:
      if llm_request.contents:
        # Sends the conversation history to the model.
        with tracer.start_as_current_span('send_data'):
          trace_send_data(invocation_context, event_id, llm_request.contents)
          await llm_connection.send_history(llm_request.contents)

      async with asyncio.TaskGroup() as tg:
        tg.create_task(
            self._send_to_model(
                llm_connection, invocation_context.live_request_queue
            )
        )
        async for event in self._receive_from_model(
            llm_connection,
            event_id,
            invocation_context,
            llm_request,
        ):
          # Empty event means the queue is closed.
          if not event:
            break
          yield event
          # In response to 'yield LlmResponse', there will be a new event that
          # contains the function responses.
          # TODO: Use a better way to get the response.
          last_event = invocation_context.session.events[-1]
          if last_event.get_function_responses():
            invocation_context.live_request_queue.send_content(
                last_event.content
            )

  async def _send_to_model(self, llm_connection, live_request_queue):
    while True:
      try:
        # Streamlit's execution model doesn't preemptively yield to the event
        # loop. Therefore, we must explicitly introduce timeouts to allow the
        # event loop to process events.
        # TODO: revert back(remove timeout) once we move off streamlit.
        live_request = await asyncio.wait_for(
            live_request_queue.get(), timeout=0.25
        )
        await asyncio.sleep(0)
      except asyncio.TimeoutError:
        continue
      if live_request.close:
        await llm_connection.close()
        return
      if live_request.blob:
        await llm_connection.send_realtime(live_request.blob)
      if live_request.content:
        await llm_connection.send_content(live_request.content)

  async def _receive_from_model(
      self,
      llm_connection: BaseLlmConnection,
      event_id: str,
      invocation_context: InvocationContext,
      llm_request: LlmRequest,
  ) -> AsyncGenerator[Event, None]:
    """Receive data from model and process events using BaseLlmConnection."""
    assert invocation_context.live_request_queue
    try:
      while True:
        async for llm_response in llm_connection.receive():
          model_response_event = Event(
              id=Event.new_id(),
              invocation_id=invocation_context.invocation_id,
              author=invocation_context.agent.name,
          )
          for event in self._postprocess(
              invocation_context,
              llm_request,
              llm_response,
              model_response_event,
          ):
            yield event
        # Give opportunity for other tasks to run.
        await asyncio.sleep(0)
    except ConnectionClosedOK:
      pass

  def __call__(
      self,
      invocation_context: InvocationContext,
  ) -> Generator[Event, None, None]:
    if invocation_context.streaming == 'server-socket':
      # Server-socket streaming mode.
      loop = asyncio.new_event_loop()
      asyncio.set_event_loop(loop)
      invocation_context.live_request_queue = LiveRequestQueue()
      for event in sync_generator(self.call_live(invocation_context)):
        yield event
        if event.turn_complete:
          invocation_context.live_request_queue.close()
    else:
      # Non-streaming mode.
      while True:
        last_event = None
        for event in self.__run_one_step(invocation_context):
          last_event = event
          yield event
        if not last_event or last_event.is_final_response():
          break

  def __run_one_step(
      self,
      invocation_context: InvocationContext,
  ) -> Generator[Event, None, None]:
    """One step means one LLM call."""
    llm_request = LlmRequest()

    # Preprocess before calling the LLM.
    for event in self._preprocess(invocation_context, llm_request):
      yield event
    if invocation_context.end_invocation:
      return

    # Calls the LLM.
    model_response_event = Event(
        id=Event.new_id(),
        invocation_id=invocation_context.invocation_id,
        author=invocation_context.agent.name,
    )
    llm_response = self._call_llm(
        invocation_context, llm_request, model_response_event
    )

    # Postprocess after calling the LLM.
    yield from self._postprocess(
        invocation_context, llm_request, llm_response, model_response_event
    )

  def _preprocess(
      self, invocation_context: InvocationContext, llm_request: LlmRequest
  ) -> Generator[Event, None, None]:
    # Runs processors.
    for processor in self.request_processors:
      events = processor.process_llm_request(invocation_context, llm_request)
      if events:
        yield from events

    # Run processors for tools.
    for tool in invocation_context.agent.tools:
      if isinstance(tool, BaseTool):
        tool_context = ToolContext(invocation_context)
        tool.process_llm_request(tool_context, llm_request)

  def _postprocess(
      self,
      invocation_context: InvocationContext,
      llm_request: LlmRequest,
      llm_response: LlmResponse,
      model_response_event: Event,
  ) -> Generator[Event, None, None]:
    """Postprocess after calling the LLM.

    Args:
      invocation_context: The invocation context.
      llm_request: The original LLM request.
      llm_response: The LLM response from the LLM call.
      model_response_event: A mutable event for the LLM response.

    Returns:
      A generator of events.
    """

    # Runs processors.
    for processor in self.response_processors:
      events = processor.process_llm_response(invocation_context, llm_response)
      if events:
        yield from events

    # Skip the model response event if there is no content and no error code.
    # This is needed for the code executor to trigger another loop.
    if not llm_response.content and not llm_response.error_code:
      return

    # Builds the event.
    model_response_event = self._finalize_model_response_event(
        llm_request, llm_response, model_response_event
    )
    yield model_response_event

    # Handles function calls.
    if model_response_event.get_function_calls():
      function_response_event = functions.handle_function_calls(
          invocation_context, model_response_event, llm_request.tools_dict
      )
      yield function_response_event

      transfer_to_agent = function_response_event.actions.transfer_to_agent
      if transfer_to_agent:
        from ...agents.agent import Agent

        root_agent = cast(Agent, invocation_context.agent.get_root_agent())
        agent_to_run = root_agent.find_agent(transfer_to_agent)
        if not agent_to_run:
          raise ValueError(
              f'Agent {transfer_to_agent} not found in the agent tree.'
          )
        yield from agent_to_run.run(invocation_context)

  @tracer.start_as_current_span('call_llm')
  def _call_llm(
      self,
      invocation_context: InvocationContext,
      llm_request: LlmRequest,
      model_response_event: Event,
  ) -> LlmResponse:
    """Calls the LLM and returns the response.

    Args:
      invocation_context: The invocation context.
      llm_request: The LLM request.
      model_response_event: A mutable event object for the LLM response.

    Returns:
      A tuple containing the LLM response and a list of callback events for
      session state delta.
    """
    from ...agents.agent import Agent

    agent = cast(Agent, invocation_context.agent)

    # Runs before_model_callback if it exists.
    if agent.before_model_callback:
      callback_context = CallbackContext(
          invocation_context, event_actions=model_response_event.actions
      )
      response = agent.before_model_callback(callback_context, llm_request)
      if response:
        return response

    # Calls the LLM.
    llm = self.__get_llm(invocation_context)
    llm_responses = list(llm.generate_content(llm_request))
    trace_call_llm(
        invocation_context, model_response_event.id, llm_request, llm_responses
    )
    llm_response = llm_responses[0]

    # Runs after_model_callback if it exists.
    if agent.after_model_callback:
      callback_context = CallbackContext(
          invocation_context, event_actions=model_response_event.actions
      )
      response = agent.after_model_callback(callback_context, llm_response)
      if response:
        return response

    return llm_response

  def _finalize_model_response_event(
      self,
      llm_request: LlmRequest,
      llm_response: LlmResponse,
      model_response_event: Event,
  ) -> Event:
    model_response_event = Event.model_validate({
        **model_response_event.model_dump(exclude_none=True),
        **llm_response.model_dump(exclude_none=True),
    })

    if model_response_event.content:
      function_calls = model_response_event.get_function_calls()
      if function_calls and functions.has_async_function_calls(
          function_calls, llm_request.tools_dict
      ):
        model_response_event.actions.pending = True

    return model_response_event

  def __get_llm(self, invocation_context: InvocationContext):
    from ...agents.agent import Agent

    resolved_model = cast(Agent, invocation_context.agent).resolved_model
    return (
        LLMRegistry.new_llm(resolved_model)
        if isinstance(resolved_model, str)
        else resolved_model
    )


================================================
File: flows/llm_flows/basic.py
================================================
"""Handles basic information to build the LLM request."""

from google.genai import types

from ...agents.invocation_context import InvocationContext
from ...models import LlmRequest


def process_llm_request(
    invocation_context: InvocationContext,
    llm_request: LlmRequest,
):
  agent = invocation_context.agent
  llm_request.model = (
      agent.resolved_model
      if isinstance(agent.resolved_model, str)
      else agent.resolved_model.model
  )
  llm_request.config = (
      agent.generate_content_config.model_copy(deep=True)
      if agent.generate_content_config
      else types.GenerateContentConfig()
  )
  # Initialize the tools list.
  llm_request.config.tools = []
  llm_request.append_tools(invocation_context.agent.tools)
  if agent.output_schema:
    llm_request.set_output_schema(agent.output_schema)
  if invocation_context.response_modalities:
    llm_request.config.response_modalities = (
        invocation_context.response_modalities
    )


================================================
File: flows/llm_flows/contents.py
================================================
"""Builds the contents for the LLM request."""

import copy

from google.genai import types

from ...agents.invocation_context import InvocationContext
from ...events import Event
from ...models import LlmRequest


def process_llm_request(
    invocation_context: InvocationContext,
    llm_request: LlmRequest,
):
  agent = invocation_context.agent
  if agent.include_contents != 'none':
    llm_request.contents = _get_contents(
        invocation_context.session.events, agent.name
    )


def _rearrange_events_for_async_function_responses_in_history(
    events: list[Event],
) -> list[Event]:
  """Rearrange the async function_response events in the history."""

  function_response_events_dict: dict[str, list[Event]] = {}
  for event in events:
    if event.get_function_responses():
      function_call_event_id: str = event.function_call_event_id  # type: ignore
      function_response_events = function_response_events_dict.get(
          function_call_event_id, []
      )
      function_response_events.append(event)
      function_response_events_dict[function_call_event_id] = (
          function_response_events
      )

  result_events: list[Event] = []
  for event in events:
    if event.get_function_responses():
      # function_response should be handled together with function_call below.
      continue
    elif event.get_function_calls():
      function_call_event_id = event.id
      function_response_events = function_response_events_dict.get(
          function_call_event_id, []
      )
      if not function_response_events:
        raise ValueError(
            'No matching function_response event found for id:'
            f' {function_call_event_id}'
        )
      result_events.append(event)
      if len(function_response_events) == 1:
        result_events.append(function_response_events[0])
      else:  # Merge all async function_response as one response event
        result_events.append(
            __merge_function_response_events(function_response_events)
        )
      continue
    else:
      result_events.append(event)

  return result_events


def _rearrange_events_for_latest_function_response(
    events: list[Event],
) -> list[Event]:
  """Rearrange the events for the latest function_response.

  If the latest function_response is for an async function_call, all events
  bewteen the initial function_call and the latest function_response will be
  removed.
  """
  if not events:
    return events
  if not events[-1].get_function_responses():
    # No need to process, since the latest event is not fuction_response.
    return events

  function_call_event_id = events[-1].function_call_event_id
  if events[-2].id == function_call_event_id:
    # The latest function_response is already matched
    return events

  reversed_function_response_events: list[Event] = []
  function_call_event_idx = -1
  for revered_idx, event in enumerate(reversed(events)):
    if event.id == function_call_event_id:
      function_call_event_idx = len(events) - 1 - revered_idx
      break
    elif (
        event.get_function_responses()
        and event.function_call_event_id == function_call_event_id
    ):
      reversed_function_response_events.append(event)

  if function_call_event_idx == -1:
    raise ValueError(
        f'No function call event found for id: {function_call_event_id}'
    )

  result_events = events[: function_call_event_idx + 1]
  result_events.append(
      __merge_function_response_events(
          list(reversed(reversed_function_response_events))
      )
  )
  return result_events


def _get_contents(
    events: list[Event], agent_name: str = ''
) -> list[types.Content]:
  filtered_events = []
  # Filter the events, leaving the contents and the function calls and
  # responses from the current agent.
  for event in events:
    if event.is_greeting:
      # Does not include the greeting message because it results in a low
      # agent transfer rate.
      continue
    if event.content:
      # Filter out function calls and responses that belong to other agents.
      # Otherwise the current agent will hallucinate and call the function.
      # TODO: Handle the case if both text and function call/response are
      # present.
      if (
          agent_name
          and event.author != agent_name
          and event.author != 'user'
          and (event.get_function_calls() or event.get_function_responses())
      ):
        continue
      filtered_events.append(event)
  result_events = _rearrange_events_for_latest_function_response(
      filtered_events
  )
  result_events = _rearrange_events_for_async_function_responses_in_history(
      result_events
  )
  return [copy.deepcopy(e.content) for e in result_events]


def __merge_function_response_events(
    function_response_events: list['Event'],
) -> 'Event':
  """Merges a list of function_response events into one event.

  The key goal is to ensure:
  1. function_call and function_response are always of the same number.
  2. The function_call and function_response are consecutively in the content.

  Args:
    function_response_events: A list of function_response events.
      NOTE: function_response_events must fulfill these requirements: 1. The
        list is in increasing order of timestamp; 2. the first event is the
        initial function_reponse event; 3. all later events should contain at
        least one function_response part that related to the function_call
        event.
      Caveat: This implementation doesn't support when a parallel function_call
        event contains async function_call of the same name.

  Returns:
    A merged event, that is
      1. All later function_response will replace function_response part in
          the initial function_response event.
      2. All non-function_response parts will be appended to the part list of
          the initial function_response event.
  """
  if len(function_response_events) <= 1:
    raise ValueError('At least two function_response events are required.')

  merged_event = function_response_events[0].model_copy(deep=True)
  parts_in_merged_event: list[types.Part] = merged_event.content.parts  # type: ignore

  if not parts_in_merged_event:
    raise ValueError('There should be at least one function_response part.')

  part_indices_in_merged_event: dict[str, int] = {}
  for idx, part in enumerate(parts_in_merged_event):
    if part.function_response:
      func_name: str = part.function_response.name  # type: ignore
      part_indices_in_merged_event[func_name] = idx

  for event in function_response_events[1:]:
    if not event.content.parts:
      raise ValueError('There should be at least one function_response part.')

    for part in event.content.parts:
      if part.function_response:
        func_name: str = part.function_response.name  # type: ignore
        parts_in_merged_event[part_indices_in_merged_event[func_name]] = part
      else:
        parts_in_merged_event.append(part)

  return merged_event


================================================
File: flows/llm_flows/examples.py
================================================
"""Handles examples for LLM flow."""

from ...agents.invocation_context import InvocationContext
from ...examples import example_util
from ...models import LlmRequest


def process_llm_request(
    invocation_context: InvocationContext,
    llm_request: LlmRequest,
):
  if not invocation_context.agent.examples:
    return
  llm_request.append_instructions(
      [example_util.build_example_si(invocation_context)]
  )


================================================
File: flows/llm_flows/functions.py
================================================
"""Handles function callings for LLM flow."""

from typing import cast

from google.genai import types

from ...agents.invocation_context import InvocationContext
from ...events import Event
from ...events import EventActions
from ...telemetry import tracer
from ...tools import BaseTool
from ...tools import ToolContext


def has_async_function_calls(
    function_calls: list[types.FunctionCall],
    tools_dict: dict[str, BaseTool],
) -> bool:
  for function_call in function_calls:
    if (
        function_call.name in tools_dict
        and tools_dict[function_call.name].is_async
    ):
      return True
  return False


def handle_function_calls(
    invocation_context: InvocationContext,
    function_call_event: Event,
    tools_dict: dict[str, BaseTool],
) -> Event:
  """Calls the functions and returns the function response event."""
  from ...agents.agent import Agent

  agent = cast(Agent, invocation_context.agent)
  function_calls = function_call_event.get_function_calls()

  function_response_events: list[Event] = []
  for function_call in function_calls:
    if function_call.name not in tools_dict:
      raise ValueError(
          f'Function {function_call.name} is not found in the tools_dict.'
      )

    # TODO: refactor the option for passing in function_call_event_id via a
    #   public interface.
    tool_context = ToolContext(
        invocation_context=invocation_context,
        function_call_event_id=function_call_event.id,
        function_call_id=function_call.id,
    )
    tool = tools_dict[function_call.name]
    if tool.is_async:
      tool_context.actions.pending = True

    # do not use "args" as the variable name, because it is a reserved keyword
    # in python debugger.
    function_args = function_call.args or {}
    function_response = None
    # Calls before_tool_callback if it exists.
    if agent.before_tool_callback:
      function_response = agent.before_tool_callback(
          tool, function_args, tool_context
      )

    # Calls the tool if before_tool_callback does not exist or returns None.
    if not function_response:
      function_response = __call_tool(
          tool, args=function_args, tool_context=tool_context
      )

    # Calls after_tool_callback if it exists.
    if agent.after_tool_callback:
      new_response = agent.after_tool_callback(
          tool,
          function_args,
          tool_context,
          function_response,
      )
      if new_response:
        function_response = new_response

    # Builds the function response event.
    function_response_event = __build_response_event(
        tool, function_response, tool_context, invocation_context
    )
    function_response_events.append(function_response_event)

  merged_event = merge_parallel_function_response_events(
      function_response_events
  )
  return merged_event


def __call_tool(
    tool: BaseTool, args: dict[str, object], tool_context: ToolContext
) -> object:
  """Calls the tool."""
  with tracer.start_as_current_span(f'call_tool [{tool.name}]'):
    return tool.call(args=args, tool_context=tool_context)


def __build_response_event(
    tool: BaseTool,
    function_result: dict[str, object],
    tool_context: ToolContext,
    invocation_context: InvocationContext,
) -> Event:
  # Specs requires the result to be a dict.
  if not isinstance(function_result, dict):
    function_result = {'result': function_result}

  part_function_response = types.Part.from_function_response(
      name=tool.name, response=function_result
  )
  part_function_response.function_response.id = tool_context.function_call_id

  content = types.Content(
      role='user',
      parts=[part_function_response],
  )
  return Event(
      invocation_id=invocation_context.invocation_id,
      author=invocation_context.agent.name,
      content=content,
      actions=tool_context.actions,
      function_call_event_id=tool_context.function_call_event_id,
  )


def merge_parallel_function_response_events(
    function_response_events: list['Event'],
) -> 'Event':

  if len(function_response_events) == 1:
    return function_response_events[0]
  merged_parts = []
  for event in function_response_events:
    if event.content:
      for part in event.content.parts or []:
        merged_parts.append(part)

  # Use the first event as the "base" for common attributes
  base_event = function_response_events[0] if function_response_events else None

  # Merge actions from all events
  # TODO: validate that pending actions are not cleared away
  merged_actions = EventActions()
  for event in function_response_events:
    merged_actions = merged_actions.model_copy(
        update=event.actions.model_dump()
    )

  # Create the new merged event
  merged_event = Event(
      invocation_id=Event.new_id(),
      author=base_event.author,
      content=types.Content(role='user', parts=merged_parts),
      actions=merged_actions,  # Optionally merge actions if required
      function_call_event_id=base_event.function_call_event_id,
  )

  # Use the base_event as the timestamp
  merged_event.timestamp = base_event.timestamp
  return merged_event


================================================
File: flows/llm_flows/identity.py
================================================
"""Gives the agent identity from the framework."""

from ...agents.invocation_context import InvocationContext
from ...models import LlmRequest


def process_llm_request(
    invocation_context: InvocationContext,
    llm_request: LlmRequest,
):
  agent = invocation_context.agent
  si = [f'You are an agent. Your internal name is "{agent.name}".']
  if agent.description:
    si.append(f' The description about you is "{agent.description}"')
  llm_request.append_instructions(si)


================================================
File: flows/llm_flows/instructions.py
================================================
"""Handles instructions and global instructions for LLM flow."""

from __future__ import annotations

import re
from typing import TYPE_CHECKING
from typing import Callable

from ...agents.readonly_context import ReadonlyContext

if TYPE_CHECKING:

  from ...agents.agent import Agent
  from ...agents.agent import InstructionProvider
  from ...agents.invocation_context import InvocationContext
  from ...models import LlmRequest


def process_llm_request(
    invocation_context: InvocationContext,
    llm_request: LlmRequest,
) -> None:
  agent: Agent = invocation_context.agent

  # Appends global instructions if set.
  root_agent: Agent = invocation_context.agent.get_root_agent()
  if root_agent.global_instruction:
    si = _build_system_instruction(
        invocation_context, root_agent.global_instruction
    )
    llm_request.append_instructions([si])

  # Appends agent instructions if set.
  if agent.instruction:
    si = _build_system_instruction(invocation_context, agent.instruction)
    llm_request.append_instructions([si])


def _build_system_instruction(
    invocation_context: InvocationContext,
    instruction: str | InstructionProvider,
) -> str:
  """Builds system instruction from instruction and session."""
  if isinstance(instruction, Callable):

    si = instruction(ReadonlyContext(invocation_context))
  else:
    si = instruction
  return _populate_values(si, invocation_context)


def _populate_values(
    instruction_template: str,
    context: InvocationContext,
) -> str:
  def _replace_match(match) -> str:
    var_name = match.group().lstrip('{').rstrip('}').strip()
    optional = False
    if var_name.endswith('?'):
      optional = True
      var_name = var_name.removesuffix('?')
    if var_name.startswith('artifact.'):
      var_name = var_name.removeprefix('artifact.')
      if context.artifact_service is None:
        raise ValueError('Artifact service is not initialized.')
      artifact = context.artifact_service.load(context.session.id, var_name)
      if not var_name:
        raise KeyError(f'Artifact {var_name} not found.')
      return str(artifact)
    else:
      if not var_name.isidentifier():
        return match.group()
      if var_name in context.session.state:
        return str(context.session.state[var_name])
      else:
        if optional:
          return ''
        else:
          raise KeyError(f'Context variable {var_name} not found.')

  return re.sub(r'{+[^{}]*}+', _replace_match, instruction_template)


================================================
File: flows/llm_flows/single_flow.py
================================================
"""Implementation of single flow."""

import logging
from . import _code_execution
from . import _nl_planning
from . import basic
from . import contents
from . import examples
from . import identity
from . import instructions
from .base_llm_flow import BaseLlmFlow

logger = logging.getLogger(__name__)


class SingleFlow(BaseLlmFlow):
  """SingleFlow is the LLM flows that handles tools calls.

  A single flow only consider an agent itself and tools.
  No children agents are allowed for unit flow.
  """

  def __init__(self):
    super().__init__()
    self.request_processors += [
        basic,
        instructions,
        identity,
        examples,
        _nl_planning,
        contents,
        # Code execution should be after the contents as it mutates the contents
        # to optimize data files.
        _code_execution,
    ]
    self.response_processors += [
        _nl_planning,
        _code_execution,
    ]


================================================
File: memory/__init__.py
================================================
from .base_memory_service import BaseMemoryService
from .in_memory_memory_service import InMemoryMemoryService


================================================
File: memory/base_memory_service.py
================================================
from pydantic import BaseModel
from pydantic import Field
from ..events.event import Event
from ..sessions import Session


class MemoryResult(BaseModel):
  session_id: str
  events: list[Event]


class SearchMemoryResponse(BaseModel):
  memories: list[MemoryResult] = Field(default_factory=list)


class BaseMemoryService:
  """Base class for memory services."""

  def add_session(self, session: Session):
    """Adds a session to the memory service.

    A session may be added multiple times during its lifetime.

    Args:
        session: The session to add.
    """
    raise NotImplementedError()

  def search(
      self, app_name: str, user_id: str, query: str
  ) -> SearchMemoryResponse:
    """Searches for sessions that match the query."""
    raise NotImplementedError()


================================================
File: memory/in_memory_memory_service.py
================================================
from pydantic import Field
from ..events.event import Event
from ..sessions import Session
from .base_memory_service import BaseMemoryService
from .base_memory_service import MemoryResult
from .base_memory_service import SearchMemoryResponse


class InMemoryMemoryService(BaseMemoryService):
  """An in-memory memory service for prototyping purpose only.

  Uses keyword matching instead of semantic search.
  """

  def __init__(self):
    self.session_events: dict[str, list[Event]] = {}
    """keys are app_name/user_id/session_id"""

  def add_session(self, session: Session):
    key = f'{session.app_name}/{session.user_id}/{session.id}'
    self.session_events[key] = [
        event for event in session.events if event.content
    ]

  def search(
      self, app_name: str, user_id: str, query: str
  ) -> SearchMemoryResponse:
    """Prototyping purpose only."""
    keywords = set(query.lower().split())
    response = SearchMemoryResponse()
    for key, events in self.session_events.items():
      if not key.startswith(f'{app_name}/{user_id}/'):
        continue
      matched_events = []
      for event in events:
        parts = event.content.parts
        text = '\n'.join([part.text for part in parts if part.text]).lower()
        for keyword in keywords:
          if keyword in text:
            matched_events.append(event)
            break
      if matched_events:
        session_id = key.split('/')[-1]
        response.memories.append(
            MemoryResult(session_id=session_id, events=matched_events)
        )
    return response


================================================
File: models/__init__.py
================================================
"""Defines the interface to support a model."""

from .base_llm import BaseLlm
from .google_llm import Gemini
from .llm_request import LlmRequest
from .llm_response import LlmResponse
from .registry import LLMRegistry

__all__ = [
    'BaseLlm',
    'Gemini',
    'LLMRegistry',
]


for regex in Gemini.supported_models():
  LLMRegistry.register(Gemini)


================================================
File: models/anthropic_llm.py
================================================
"""Anthropic integration for Claude models."""

from __future__ import annotations

import logging
import os
from functools import cached_property
from typing import TYPE_CHECKING
from typing import Generator
from typing import Iterable
from typing import Literal
from typing import Union

from anthropic import NOT_GIVEN
from anthropic import AnthropicVertex
from anthropic import types as anthropic_types
from google.genai import types
from pydantic import BaseModel
from typing_extensions import override

from .base_llm import BaseLlm
from .llm_response import LlmResponse

if TYPE_CHECKING:
  from .llm_request import LlmRequest

__all__ = ["Claude"]

logger = logging.getLogger(__name__)

MAX_TOKEN = 1024


class ClaudeRequest(BaseModel):
  system_instruction: str
  messages: Iterable[anthropic_types.MessageParam]
  tools: list[anthropic_types.ToolParam]


def to_claude_role(role: str | None) -> Literal["user", "assistant"]:
  if role in ["model", "assistant"]:
    return "assistant"
  return "user"


def to_google_genai_finish_reason(
    anthropic_stop_reason: str | None,
) -> types.FinishReason:
  if anthropic_stop_reason in ["end_turn", "stop_sequence", "tool_use"]:
    return "STOP"
  if anthropic_stop_reason == "max_tokens":
    return "MAX_TOKENS"
  return "FINISH_REASON_UNSPECIFIED"


def part_to_message_block(
    part: types.Part,
) -> Union[
    anthropic_types.TextBlockParam,
    anthropic_types.ImageBlockParam,
    anthropic_types.ToolUseBlockParam,
    anthropic_types.ToolResultBlockParam,
]:
  if part.text:
    return anthropic_types.TextBlockParam(text=part.text, type="text")
  if part.function_call:
    assert part.function_call.name

    return anthropic_types.ToolUseBlockParam(
        id=part.function_call.id or "",
        name=part.function_call.name,
        input=part.function_call.args,
        type="tool_use",
    )
  if part.function_response:
    return anthropic_types.ToolResultBlockParam(
        tool_use_id=part.function_response.id or "",
        type="tool_result",
        content=part.function_response.response["result"],
        is_error=False,
    )
  raise NotImplementedError("Not supported yet.")


def content_to_message_param(
    content: types.Content,
) -> anthropic_types.MessageParam:
  return {
      "role": to_claude_role(content.role),
      "content": [part_to_message_block(part) for part in content.parts or []],
  }


def content_block_to_part(
    content_block: anthropic_types.ContentBlock,
) -> types.Part:
  if isinstance(content_block, anthropic_types.TextBlock):
    return types.Part.from_text(text=content_block.text)
  if isinstance(content_block, anthropic_types.ToolUseBlock):
    assert isinstance(content_block.input, dict)
    part = types.Part.from_function_call(
        name=content_block.name, args=content_block.input
    )
    part.function_call.id = content_block.id
    return part
  raise NotImplementedError("Not supported yet.")


def message_to_generate_content_response(
    message: anthropic_types.Message,
) -> LlmResponse:

  return LlmResponse(
      content=types.Content(
          role="model",
          parts=[content_block_to_part(cb) for cb in message.content],
      ),
      # TODO: Deal with these later.
      # finish_reason=to_google_genai_finish_reason(message.stop_reason),
      # usage_metadata=types.GenerateContentResponseUsageMetadata(
      #     prompt_token_count=message.usage.input_tokens,
      #     candidates_token_count=message.usage.output_tokens,
      #     total_token_count=(
      #         message.usage.input_tokens + message.usage.output_tokens
      #     ),
      # ),
  )


def function_declaration_to_tool_param(
    function_declaration: types.FunctionDeclaration,
) -> anthropic_types.ToolParam:
  assert function_declaration.name

  properties = {}
  if (
      function_declaration.parameters
      and function_declaration.parameters.properties
  ):
    for key, value in function_declaration.parameters.properties.items():
      value_dict = value.model_dump(exclude_none=True)
      if "type" in value_dict:
        value_dict["type"] = value_dict["type"].lower()
      properties[key] = value_dict

  return anthropic_types.ToolParam(
      name=function_declaration.name,
      description=function_declaration.description or "",
      input_schema={
          "type": "object",
          "properties": properties,
      },
  )


class Claude(BaseLlm):
  model: str = "claude-3-5-sonnet-v2@20241022"

  @staticmethod
  @override
  def supported_models() -> list[str]:
    return [r"claude-3-.*"]

  @override
  def generate_content(
      self, llm_request: LlmRequest, stream: bool = False
  ) -> Generator[LlmResponse, None, None]:
    """Generates one content from the given contents and tools.

    Args:
      llm_request: LlmRequest, the request to send to the LLM.
      stream: bool = False, whether to do streaming call.

    Returns a generator of types.Content. For non-streaming call, it will only
      yield one Content. For streaming call, it may yield more than one content,
      but all yielded contents should be treated as one content by merging the
      parts list.
    """
    # TODO: Supports output_schema.
    messages = [
        content_to_message_param(content)
        for content in llm_request.contents or []
    ]
    tools = NOT_GIVEN
    if (
        llm_request.config
        and llm_request.config.tools
        and llm_request.config.tools[0].function_declarations
    ):
      tools = [
          function_declaration_to_tool_param(tool)
          for tool in llm_request.config.tools[0].function_declarations
      ]
    tool_choice = (
        anthropic_types.ToolChoiceAutoParam(
            type="auto",
            # TODO: allow parallel tool use.
            disable_parallel_tool_use=True,
        )
        if llm_request.tools_dict
        else NOT_GIVEN
    )
    message = self._anthropic_client.messages.create(
        model=llm_request.model,
        system=llm_request.config.system_instruction,
        messages=messages,
        tools=tools,
        tool_choice=tool_choice,
        max_tokens=MAX_TOKEN,
    )
    logger.info(
        "Claude response: %s",
        message.model_dump_json(indent=2, exclude_none=True),
    )
    yield message_to_generate_content_response(message)

  @cached_property
  def _anthropic_client(self) -> AnthropicVertex:
    if (
        "GOOGLE_CLOUD_PROJECT" not in os.environ
        or "GOOGLE_CLOUD_LOCATION" not in os.environ
    ):
      raise ValueError(
          "GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_LOCATION must be set for using"
          " Anthropic on Vertex."
      )

    return AnthropicVertex(
        project_id=os.environ["GOOGLE_CLOUD_PROJECT"],
        region=os.environ["GOOGLE_CLOUD_LOCATION"],
    )


================================================
File: models/base_llm.py
================================================
import abc
from abc import abstractmethod
from typing import Generator

from pydantic import BaseModel
from pydantic import ConfigDict

from .base_llm_connection import BaseLlmConnection
from .llm_request import LlmRequest
from .llm_response import LlmResponse


class BaseLlm(BaseModel):
  """The BaseLLM class."""

  model_config = ConfigDict(
      # This allows us to use arbitrary types in the model. E.g. PIL.Image.
      arbitrary_types_allowed=True,
  )

  model: str
  """The name of the LLM, e.g. gemini-1.5-flash or gemini-1.5-flash-001."""

  @staticmethod
  @abstractmethod
  def supported_models() -> list[str]:
    """Returns a list of supported models in regex."""
    pass

  @abc.abstractmethod
  def generate_content(
      self, llm_request: LlmRequest, stream: bool = False
  ) -> Generator[LlmResponse, None, None]:
    """Generates one content from the given contents and tools.

    Args:
      llm_request: LlmRequest, the request to send to the LLM.
      stream: bool = False, whether to do streaming call.

    Returns a generator of types.Content. For non-streaming call, it will only
      yield one Content. For streaming call, it may yield more than one content,
      but all yielded contents should be treated as one content by merging the
      parts list.
    """
    pass

  def connect(self, llm_request: LlmRequest) -> BaseLlmConnection:
    """Creates a live connection to the LLM."""
    raise NotImplementedError(
        f'Live connection is not supported for {self.model}.'
    )


================================================
File: models/base_llm_connection.py
================================================
from abc import abstractmethod
from typing import AsyncGenerator
from google.genai import types
from .llm_response import LlmResponse


class BaseLlmConnection:
  """The base class for a live model connection."""

  @abstractmethod
  async def send_history(self, history: list[types.Content]):
    """Sends the conversation history to the model.

    You call this method right after setting up the model connection.
    The model will respond if the last content is from user, otherwise it will
    wait for new user input before responding.
    """
    pass

  @abstractmethod
  async def send_content(self, content: types.Content):
    """Sends a user content to the model.

    The model will respond immediately upon receiving the content.
    If you send function responses, all parts in the content should be function
    responses.
    """
    pass

  @abstractmethod
  async def send_realtime(self, blob: types.Blob):
    """Sends a chunk of audio or a frame of video to the model in realtime.

    The model may not respond immediately upon receiving the blob. It will do
    voice activity detection and decide when to respond.
    """
    pass

  @abstractmethod
  async def receive(self) -> AsyncGenerator[LlmResponse, None]:
    pass

  @abstractmethod
  async def close(self):
    pass


================================================
File: models/gemini_llm_connection.py
================================================
from typing import AsyncGenerator

from google.genai import live
from google.genai import types

from .base_llm_connection import BaseLlmConnection
from .llm_response import LlmResponse


class GeminiLlmConnection(BaseLlmConnection):
  """The Gemini model connection."""

  def __init__(self, gemini_session: live.AsyncSession):
    self._gemini_session = gemini_session

  async def send_history(self, history: list[types.Content]):
    # TODO: Remove this filter and translate unary contents to streaming
    # contents properly.
    contents = [content for content in history if content.parts[0].text]
    await self._gemini_session.send(
        input=types.LiveClientContent(
            turns=contents,
            turn_complete=contents[-1].role == 'user',
        ),
    )

  async def send_content(self, content: types.Content):
    assert content.parts
    if content.parts[0].function_response:
      # All parts have to be function responses.
      function_responses = [part.function_response for part in content.parts]
      await self._gemini_session.send(
          input=types.LiveClientToolResponse(
              function_responses=function_responses
          ),
      )
    else:
      await self._gemini_session.send(
          input=types.LiveClientContent(
              turns=[content],
              turn_complete=True,
          )
      )

  async def send_realtime(self, blob: types.Blob):
    await self._gemini_session.send(input=blob.model_dump())

  def __build_text_response(self, text: str):
    return LlmResponse(
        content=types.Content(
            role='model',
            parts=[types.Part.from_text(text=text)],
        ),
    )

  async def receive(self) -> AsyncGenerator[LlmResponse, None]:
    text = ''
    async for message in self._gemini_session.receive():
      if message.server_content:
        content = message.server_content.model_turn
        if content:
          llm_response = LlmResponse(content=content)
          if content.parts[0].text:
            text += content.parts[0].text
            llm_response.partial = True
          else:
            if text:
              yield self.__build_text_response(text)
              text = ''
          yield llm_response

        if message.server_content.turn_complete:
          if text:
            yield self.__build_text_response(text)
            text = ''
          yield LlmResponse(turn_complete=True)
          break
      if message.tool_call:
        if text:
          yield self.__build_text_response(text)
          text = ''
        parts = [
            types.Part(function_call=function_call)
            for function_call in message.tool_call.function_calls
        ]
        yield LlmResponse(content=types.Content(role='model', parts=parts))

  async def close(self):
    await self._gemini_session.close()


================================================
File: models/google_llm.py
================================================
import contextlib
import logging
from functools import cached_property
from typing import Generator
from typing import cast

from google.genai import Client
from google.genai import types
from typing_extensions import override

from .base_llm import BaseLlm
from .base_llm import LlmRequest
from .base_llm import LlmResponse
from .base_llm_connection import BaseLlmConnection
from .gemini_llm_connection import GeminiLlmConnection

logger = logging.getLogger(__name__)
line_break = '\n'
_EXCLUDED_PART_FIELD = {'inline_data': {'data'}}


class Gemini(BaseLlm):
  """Integration for Gemini models."""

  model: str = 'gemini-1.5-flash'

  @staticmethod
  @override
  def supported_models() -> list[str]:
    return [r'gemini-.*']

  def generate_content(
      self, llm_request: LlmRequest, stream: bool = False
  ) -> Generator[types.GenerateContentResponse, None, None]:
    # Last content must be from the user, otherwise the model won't respond.
    if not llm_request.contents or llm_request.contents[-1].role != 'user':
      # TODO: expose this sentence to be configuration to user.
      llm_request.contents.append(
          types.Content(
              role='user',
              parts=[
                  types.Part(
                      text=(
                          'Continue output. DO NOT look at this line. ONLY look'
                          ' at the content before this line and system'
                          ' instruction. '
                      )
                  )
              ],
          )
      )

    logger.info(_build_request_log(llm_request))

    logger.info(
        'Sending out request, model: %s, backend: %s, stream: %s',
        llm_request.model,
        self._api_backend,
        stream,
    )

    if stream:
      responses = self.api_client.models.generate_content_stream(
          model=llm_request.model,
          contents=llm_request.contents,
          config=llm_request.config,
      )
      for response in responses:
        logger.info(_build_response_log(response))
        # TODO: Handle partial response.
        yield LlmResponse.create(response)
    else:
      response = self.api_client.models.generate_content(
          model=llm_request.model,
          contents=llm_request.contents,
          config=llm_request.config,
      )
      logger.info(_build_response_log(response))
      yield LlmResponse.create(response)

  @cached_property
  def api_client(self) -> Client:
    return Client()

  @cached_property
  def _api_backend(self) -> str:
    return 'vertex' if self.api_client.vertexai else 'ml_dev'

  @contextlib.asynccontextmanager
  async def connect(self, llm_request: LlmRequest) -> BaseLlmConnection:
    client = Client(http_options={'api_version': 'v1alpha'})
    config = types.LiveConnectConfig(
        response_modalities=llm_request.config.response_modalities,
        system_instruction=types.Content(
            role='system',
            parts=[
                types.Part.from_text(text=llm_request.config.system_instruction)
            ],
        ),
        tools=llm_request.config.tools,
    )
    async with client.aio.live.connect(
        # We use Gemini 2.0 regardless of the agent's model config.
        model='gemini-2.0-flash-exp',
        config=config,
    ) as live_session:
      yield GeminiLlmConnection(live_session)


def _build_function_declaration_log(
    func_decl: types.FunctionDeclaration,
) -> str:
  param_str = '{}'
  if func_decl.parameters and func_decl.parameters.properties:
    param_str = str({
        k: v.model_dump(exclude_none=True)
        for k, v in func_decl.parameters.properties.items()
    })
  return_str = 'None'
  if func_decl.response:
    return_str = str(func_decl.response.model_dump(exclude_none=True))
  return f'{func_decl.name}: {param_str} -> {return_str}'


def _build_request_log(req: LlmRequest) -> str:
  function_decls: list[types.FunctionDeclaration] = cast(
      list[types.FunctionDeclaration],
      req.config.tools[0].function_declarations if req.config.tools else [],
  )
  function_logs = (
      [
          _build_function_declaration_log(func_decl)
          for func_decl in function_decls
      ]
      if function_decls
      else []
  )
  contents_logs = [
      content.model_dump_json(
          exclude_none=True,
          exclude={
              'parts': {
                  i: _EXCLUDED_PART_FIELD for i in range(len(content.parts))
              }
          },
      )
      for content in req.contents
  ]

  return f"""Request:
-----------------------------------------------------------
System Instruction:
{req.config.system_instruction}
-----------------------------------------------------------
Contents:
{line_break.join(contents_logs)}
-----------------------------------------------------------
Functions:
{line_break.join(function_logs)}
-----------------------------------------------------------
"""


def _build_response_log(resp: types.GenerateContentResponse) -> str:
  return f"""Response:
-----------------------------------------------------------
{resp.model_dump_json(exclude_none=True)}
-----------------------------------------------------------
"""


================================================
File: models/llm_request.py
================================================
from typing import Callable
from google.genai import types
from pydantic import BaseModel
from pydantic import ConfigDict
from pydantic import Field
from ..tools.base_tool import BaseTool
from ..tools.function_tool import FunctionTool


class LlmRequest(BaseModel):
  model_config = ConfigDict(arbitrary_types_allowed=True)

  model: str | None = None
  contents: list[types.Content] = Field(default_factory=list)
  config: types.GenerateContentConfig | None = None
  """Additional config for the generate content request.

  tools in generate_content_config should not be set.
  """
  tools_dict: dict[str, BaseTool] = Field(default_factory=dict, exclude=True)

  def append_instructions(self, instructions: list[str]) -> None:
    if self.config.system_instruction:
      self.config.system_instruction += '\n\n' + '\n\n'.join(instructions)
    else:
      self.config.system_instruction = '\n\n'.join(instructions)

  def append_tools(self, tools: list[BaseTool | Callable]) -> None:
    if not tools:
      return
    declarations = []
    for tool in tools:
      if isinstance(tool, Callable):
        tool = FunctionTool(tool)
      self.tools_dict[tool.name] = tool
      declaration = tool.get_declaration()
      if declaration:
        declarations.append(declaration)
    if declarations:
      self.config.tools.append(types.Tool(function_declarations=declarations))

  def set_output_schema(self, base_model: type[BaseModel]) -> None:
    self.config.response_schema = base_model
    self.config.response_mime_type = 'application/json'


================================================
File: models/llm_response.py
================================================
from __future__ import annotations

from google.genai import types
from pydantic import BaseModel
from pydantic import ConfigDict


class LlmResponse(BaseModel):
  model_config = ConfigDict(extra='forbid')

  content: types.Content | None = None
  grounding_metadata: types.GroundingMetadata | None = None

  partial: bool | None = None
  """Indicates whether the text content is part of a unfinished text stream.

  Only used for streaming mode and when the content is plain text.
  """

  turn_complete: bool | None = None
  """Indicates whether the response from the model is complete.

  Only used for streaming mode.
  """

  error_code: str | None = None
  """Error code if the response is an error. Code varies by model."""

  error_message: str | None = None
  """Error message if the response is an error."""

  @staticmethod
  def create(
      generate_content_response: types.GenerateContentResponse,
  ) -> 'LlmResponse':
    if generate_content_response.candidates:
      candidate = generate_content_response.candidates[0]
      if candidate.content:
        return LlmResponse(
            content=candidate.content,
            grounding_metadata=candidate.grounding_metadata,
        )
      else:
        return LlmResponse(
            error_code=candidate.finish_reason,
            error_message=candidate.finish_message,
        )
    else:
      if generate_content_response.prompt_feedback:
        prompt_feedback = generate_content_response.prompt_feedback
        return LlmResponse(
            error_code=prompt_feedback.block_reason,
            error_message=prompt_feedback.block_reason_message,
        )
      else:
        return LlmResponse(
            error_code='UNKNOWN_ERROR',
            error_message='Unknown error.',
        )


================================================
File: models/registry.py
================================================
"""The registry class for model."""

from __future__ import annotations

import logging
import re
from functools import lru_cache
from typing import TYPE_CHECKING

if TYPE_CHECKING:
  from .base_llm import BaseLlm

logger = logging.getLogger(__name__)


_llm_registry_dict: dict[str, type[BaseLlm]] = {}
"""Registry for LLMs.

Key is the regex that matches the model name.
Value is the class that implements the model.
"""


class LLMRegistry:
  """Registry for LLMs."""

  @staticmethod
  def new_llm(model: str) -> BaseLlm:
    return LLMRegistry.resolve(model)(model=model)

  @staticmethod
  def _register(model_name_regex: str, llm_cls: type[BaseLlm]):
    if model_name_regex in _llm_registry_dict:
      logger.info(
          'Updating LLM class for %s from %s to %s',
          model_name_regex,
          _llm_registry_dict[model_name_regex],
          llm_cls,
      )

    _llm_registry_dict[model_name_regex] = llm_cls

  @staticmethod
  def register(llm_cls: type[BaseLlm]):
    for regex in llm_cls.supported_models():
      LLMRegistry._register(regex, llm_cls)

  @lru_cache(maxsize=32)
  @staticmethod
  def resolve(model: str) -> type[BaseLlm]:
    for regex, llm_class in _llm_registry_dict.items():
      if re.compile(regex).fullmatch(model):
        return llm_class

    raise ValueError(f'Model {model} not found.')


================================================
File: sessions/__init__.py
================================================
from .base_session_service import BaseSessionService
from .in_memory_session_service import InMemorySessionService
from .session import Session
from .state import State

__all__ = [
    'BaseSessionService',
    'InMemorySessionService',
    'Session',
]


================================================
File: sessions/base_session_service.py
================================================
import abc
from typing import Any
from typing import Optional

from pydantic import BaseModel
from pydantic import Field

from ..events import Event
from .session import Session


class PendingEvent(BaseModel):
  """Represents a pending function call event and its associated response events."""

  function_call_event: Event
  function_response_events: list[Event] = Field(default_factory=list)


class GetSessionConfig(BaseModel):
  num_recent_events: int | None = None
  after_timestamp: float | None = None


class ListEventsConfig(BaseModel):
  query: str | None = None
  page_size: int | None = None
  page_token: str | None = None


class ListSessionsResponse(BaseModel):
  session_ids: list[str] = []


class ListEventsResponse(BaseModel):
  events: list[Event] = []
  next_page_token: str | None = None


class BaseSessionService(abc.ABC):

  @abc.abstractmethod
  def create(
      self,
      app_name: str,
      user_id: str,
      state: Optional[dict[str, Any]] = None,
      *,
      session_id: Optional[str] = None,
  ) -> Session:
    """Creates a new session.

    Args:
      app_name: the name of the app.
      user_id: the id of the user.
      state: the initial state of the session.
      session_id: the client-provided id of the session. If not provided, a
        server-side generated uuid will be used.

    Returns:
      session: The newly created session instance.
    """
    pass

  @abc.abstractmethod
  def get(
      self,
      app_name: str,
      user_id: str,
      session_id: str,
      config: GetSessionConfig | None = None,
  ) -> Session | None:
    """Gets a session."""
    pass

  @abc.abstractmethod
  def list_sessions(self, app_name: str, user_id: str) -> ListSessionsResponse:
    """Lists all the session IDs."""
    pass

  @abc.abstractmethod
  def delete(self, app_name: str, user_id: str, session_id: str) -> None:
    """Deletes a session."""
    pass

  @abc.abstractmethod
  def list_events(
      self,
      app_name: str,
      user_id: str,
      session_id: str,
      config: ListEventsConfig,
  ) -> ListEventsResponse:
    """Lists events in a session."""
    pass

  @abc.abstractmethod
  def list_pending_events(
      self, app_name: str, user_id: str, session_id: str
  ) -> list[PendingEvent]:
    """Lists pending events in a session."""
    pass

  def append_event(self, session: Session, event: Event) -> Event:
    """Appends an event to a session."""
    if event.actions:
      if event.actions.state_delta:
        session.state.update(event.actions.state_delta)

    session.events.append(event)
    return event


================================================
File: sessions/in_memory_session_service.py
================================================
import copy
import time
from typing import Any
from typing import Optional
import uuid

from typing_extensions import override

from ..events import Event
from .base_session_service import BaseSessionService
from .base_session_service import GetSessionConfig
from .base_session_service import ListEventsConfig
from .base_session_service import ListEventsResponse
from .base_session_service import ListSessionsResponse
from .base_session_service import PendingEvent
from .session import Session


class InMemorySessionService(BaseSessionService):

  # A map from app name to a map from user Id to a map from user Id to a map
  # from session ID to a map from function call event ID to pending event.
  pending_events: dict[str, dict[str, dict[str, dict[str, PendingEvent]]]] = {}
  # A map from app name to a map from user ID to a map from session ID to session.
  sessions: dict[str, dict[str, dict[str, Session]]] = {}

  @override
  def create(
      self,
      app_name: str,
      user_id: str,
      state: Optional[dict[str, Any]] = None,
      *,
      session_id: Optional[str] = None,
  ) -> Session:
    session_id = (
        session_id.strip()
        if session_id and session_id.strip()
        else str(uuid.uuid4())
    )
    session = Session(
        app_name=app_name,
        user_id=user_id,
        id=session_id,
        state=state or {},
        last_update_time=time.time(),
    )

    if app_name not in self.sessions:
      self.sessions[app_name] = {}
    if user_id not in self.sessions[app_name]:
      self.sessions[app_name][user_id] = {}
    self.sessions[app_name][user_id][session_id] = session
    return copy.deepcopy(session)

  def exists(self, app_name: str, user_id: str, session_id: str) -> bool:
    if app_name not in self.sessions:
      return False
    if user_id not in self.sessions[app_name]:
      return False
    return session_id in self.sessions[app_name][user_id]

  @override
  def get(
      self,
      app_name: str,
      user_id: str,
      session_id: str,
      config: GetSessionConfig | None = None,
  ) -> Session:
    if app_name not in self.sessions:
      return None
    if user_id not in self.sessions[app_name]:
      return None
    if session_id not in self.sessions[app_name][user_id]:
      return None

    session = self.sessions[app_name][user_id].get(session_id)
    if config:
      if config.num_recent_events:
        session.events = session.events[-config.num_recent_events :]
      elif config.after_timestamp:
        i = len(session.events) - 1
        while i >= 0:
          if session.events[i].timestamp < config.after_timestamp:
            break
          i -= 1
        if i >= 0:
          session.events = session.events[i:]
    return copy.deepcopy(session)

  def list_sessions(self, app_name: str, user_id: str) -> ListSessionsResponse:
    empty_response = ListSessionsResponse()
    if app_name not in self.sessions:
      return empty_response
    if user_id not in self.sessions[app_name]:
      return empty_response
    return ListSessionsResponse(
        session_ids=list(self.sessions[app_name][user_id])
    )

  def delete(self, app_name: str, user_id: str, session_id: str) -> None:
    if self.get(app_name, user_id, session_id) is None:
      return None

    self.sessions[app_name][user_id].pop(session_id)

  @override
  def append_event(self, session: Session, event: Event) -> Event:
    # Update the in-memory session.
    super().append_event(session, event)
    session.last_update_time = event.timestamp

    # Update the storage session
    app_name = session.app_name
    user_id = session.user_id
    session_id = session.id
    if app_name not in self.sessions:
      return event
    if user_id not in self.sessions[app_name]:
      return event
    if session_id not in self.sessions[app_name][user_id]:
      return event

    storage_session = self.sessions[app_name][user_id].get(session_id)
    super().append_event(storage_session, event)

    if event.actions:
      if event.get_function_calls() and event.actions.pending:
        pending_event = PendingEvent(function_call_event=event)
        if session.app_name not in self.pending_events:
          self.pending_events[session.app_name] = {}
        if session.user_id not in self.pending_events[session.app_name]:
          self.pending_events[session.app_name][session.user_id] = {}
        if (
            session.id
            not in self.pending_events[session.app_name][session.user_id]
        ):
          self.pending_events[session.app_name][session.user_id][
              session.id
          ] = {}
        self.pending_events[session.app_name][session.user_id][session.id][
            event.id
        ] = pending_event
      if event.get_function_responses() and not event.actions.pending:
        # TODO: Properly handle parallel function calls.
        if (
            session.app_name in self.pending_events
            and session.user_id in self.pending_events[session.app_name]
            and session.id
            in self.pending_events[session.app_name][session.user_id]
        ):
          self.pending_events[session.app_name][session.user_id][
              session.id
          ].pop(event.function_call_event_id, None)

    storage_session.last_update_time = event.timestamp
    return event

  @override
  def list_events(
      self,
      app_name: str,
      user_id: str,
      session_id: str,
      config: ListEventsConfig,
  ) -> ListEventsResponse:
    raise NotImplementedError()

  def list_pending_events(
      self, app_name: str, user_id: str, session_id: str
  ) -> list[PendingEvent]:
    if (
        app_name in self.pending_events
        and user_id in self.pending_events[app_name]
        and session_id in self.pending_events[app_name][user_id]
    ):
      return list(self.pending_events[app_name][user_id][session_id].values())
    else:
      return []


================================================
File: sessions/postgres_session_service.py
================================================
import logging
import uuid
from datetime import datetime
from typing import Any
from typing import Optional

from sqlalchemy import ForeignKey
from sqlalchemy import delete
from sqlalchemy import func
from sqlalchemy import select
from sqlalchemy.dialects import postgresql
from sqlalchemy.engine import Engine
from sqlalchemy.engine import create_engine
from sqlalchemy.ext.mutable import MutableDict
from sqlalchemy.inspection import inspect
from sqlalchemy.orm import Mapped
from sqlalchemy.orm import Session as DatabaseSessionFactory
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import mapped_column
from sqlalchemy.orm import relationship
from sqlalchemy.orm import sessionmaker
from sqlalchemy.schema import MetaData
from sqlalchemy.types import DateTime
from sqlalchemy.types import PickleType
from sqlalchemy.types import String
from typing_extensions import override
from tzlocal import get_localzone

from ..events import Event
from .base_session_service import BaseSessionService
from .base_session_service import GetSessionConfig
from .base_session_service import ListEventsConfig
from .base_session_service import ListEventsResponse
from .base_session_service import ListSessionsResponse
from .base_session_service import PendingEvent
from .session import Session

logger = logging.getLogger(__name__)

Base = declarative_base()


class StorageSession(Base):
  __tablename__ = "sessions"

  id: Mapped[str] = mapped_column(
      String, primary_key=True, default=lambda: str(uuid.uuid4())
  )

  state: Mapped[dict] = mapped_column(
      MutableDict.as_mutable(postgresql.JSONB), default={}
  )

  create_time: Mapped[DateTime] = mapped_column(DateTime(), default=func.now())
  update_time: Mapped[DateTime] = mapped_column(
      DateTime(), default=func.now(), onupdate=func.now()
  )

  storage_events: Mapped[list["StorageEvent"]] = relationship(
      "StorageEvent", back_populates="storage_session"
  )

  pending_events: Mapped[dict[str, list]] = mapped_column(
      MutableDict.as_mutable(postgresql.JSONB), default={}
  )

  def __repr__(self):
    return (
        f"<StorageSession(id={self.id}, update_time={self.update_time},"
        f" pending_events={self.pending_events})>"
    )


class StorageEvent(Base):
  __tablename__ = "events"

  id: Mapped[str] = mapped_column(String, primary_key=True)
  session_id = mapped_column(
      ForeignKey("sessions.id", ondelete="CASCADE"), primary_key=True
  )

  invocation_id: Mapped[str] = mapped_column(String)
  author: Mapped[str] = mapped_column(String)
  timestamp: Mapped[DateTime] = mapped_column(DateTime(), default=func.now())
  content: Mapped[str] = mapped_column(postgresql.JSONB)
  actions: Mapped[dict] = mapped_column(PickleType)

  storage_session: Mapped[StorageSession] = relationship(
      "StorageSession", back_populates="storage_events"
  )


class PostgresSessionService(BaseSessionService):

  def __init__(
      self,
      db_url: Optional[str] = None,
  ):
    """
    Args:
            db_url (Optional[str]): The database URL to connect to.
    """
    # 1. Create DB engine for db connection
    # 2. Create all tables based on schema
    # 3. Initialize all properies
    if db_url is None:
      raise ValueError("db_url can not be None")

    # Get the local timezone
    local_timezone = get_localzone()
    logger.info(f"Local timezone: {local_timezone}")

    self.db_engine: Engine = create_engine(
        db_url, connect_args={"options": f"-c timezone={local_timezone}"}
    )
    self.metadata: MetaData = MetaData()
    self.inspector = inspect(self.db_engine)

    # DB session factory method
    self.DatabaseSessionFactory: sessionmaker[DatabaseSessionFactory] = (
        sessionmaker(bind=self.db_engine)
    )

    # Uncomment to recreate DB every time
    # Base.metadata.drop_all(self.db_engine)
    Base.metadata.create_all(self.db_engine)

  @override
  def create(
      self,
      state: Optional[dict[str, Any]] = None,
      *,
      session_id: Optional[str] = None,
  ) -> Session:
    # 1. Build storage session object
    # 2. Add the object to the table
    # 3. Build the session object with generated id
    # 4. Return the session
    with self.DatabaseSessionFactory() as sessionFactory:
      storage_session = StorageSession(id=session_id, state=state or {})
      sessionFactory.add(storage_session)
      sessionFactory.commit()

      sessionFactory.refresh(storage_session)
      session = Session(
          id=str(storage_session.id),
          state=state or {},
          last_update_time=storage_session.update_time.timestamp(),
      )
      return session
    return None

  @override
  def get(
      self, session_id: str, config: GetSessionConfig | None = None
  ) -> Session | None:
    # 1. Get the storage session entry from session table
    # 2. Get all the events based on session id and filtering config
    # 3. Convert and return the session
    session: Session = None
    with self.DatabaseSessionFactory() as sessionFactory:
      storage_session = sessionFactory.get(StorageSession, session_id)
      if storage_session is None:
        return None

      storage_events = (
          sessionFactory.query(StorageEvent)
          .filter(StorageEvent.session_id == storage_session.id)
          .filter(
              StorageEvent.timestamp < config.after_timestamp
              if config
              else True
          )
          .limit(config.num_recent_events if config else None)
          .all()
      )

      # Convert storage session to session
      session = Session(
          id=session_id,
          state=storage_session.state,
          last_update_time=storage_session.update_time.timestamp(),
      )
      session.events = [
          Event(
              id=e.id,
              author=e.author,
              invocation_id=e.invocation_id,
              content=e.content,
              actions=e.actions,
              timestamp=e.timestamp.timestamp(),
          )
          for e in storage_events
      ]

    return session

  @override
  def list_sessions(self) -> ListSessionsResponse:
    with self.DatabaseSessionFactory() as sessionFactory:
      results = sessionFactory.scalars(select(StorageSession.id)).all()
      return ListSessionsResponse(session_ids=results)
    raise ValueError("Failed to retrieve sessions.")

  @override
  def delete(self, session_id: str) -> None:
    with self.DatabaseSessionFactory() as sessionFactory:
      stmt = delete(StorageSession).where(StorageSession.id == session_id)
      sessionFactory.execute(stmt)
      sessionFactory.commit()

  @override
  def append_event(self, session: Session, event: Event) -> Event:
    logger.info(f"Append event: {event} to session {session.id}")

    if event.partial:
      return event

    # 1. Check if timestamp is stale
    # 2. Update session attributes based on event config
    # 3. Handle pending events case
    # 4. Store event to table
    with self.DatabaseSessionFactory() as sessionFactory:
      storage_session = sessionFactory.get(StorageSession, session.id)

      if storage_session.update_time.timestamp() > session.last_update_time:
        raise ValueError(
            f"Session last_update_time {session.last_update_time} is later than"
            f" the upate_time in storage {storage_session.update_time}"
        )

      if event.actions:
        if event.actions.state_delta:
          state_to_update = storage_session.state
          state_to_update.update(event.actions.state_delta)
          storage_session.state = state_to_update

        if event.get_function_calls() and event.actions.pending:
          storage_session.pending_events[event.id] = []

        if event.get_function_responses() and not event.actions.pending:
          storage_session.pending_events.pop(event.function_call_event_id, None)

      encoded_content = event.content.model_dump(exclude_none=True)
      storage_event = StorageEvent(
          id=event.id,
          invocation_id=event.invocation_id,
          author=event.author,
          content=encoded_content,
          actions=event.actions,
          session_id=session.id,
          timestamp=datetime.fromtimestamp(event.timestamp),
      )

      sessionFactory.add(storage_event)

      sessionFactory.commit()
      sessionFactory.refresh(storage_session)

      # Update timestamp with commit time
      session.last_update_time = storage_session.update_time.timestamp()

    # Also update the in-memory session
    super().append_event(session, event)
    return event

  @override
  def list_events(
      self, session_id: str, config: ListEventsConfig
  ) -> ListEventsResponse:
    pass

  def list_pending_events(self, session_id: str) -> list[PendingEvent]:
    response_list = []
    with self.DatabaseSessionFactory() as sessionFactory:
      # Read the session from storage
      storage_session = sessionFactory.get(StorageSession, session_id)
      if storage_session is None:
        raise ValueError("Failed to retrieve session.")

      pending_events = storage_session.pending_events
      for function_call_event_id in pending_events.keys():
        pending_event = None
        # Fetch the FunctionCall Event from Event table
        function_call_event = sessionFactory.get(
            StorageEvent, (function_call_event_id, session_id)
        )
        if function_call_event:
          pending_event = PendingEvent(
              function_call_event=convert_event(function_call_event)
          )

        # Fetch the FunctionResponse Event from Event table
        function_response_event_ids = pending_events[function_call_event_id]
        for function_response_event_id in function_response_event_ids:
          function_response_event = sessionFactory.get(
              StorageEvent, (function_response_event_id, session_id)
          )
          if function_response_event:
            pending_event.function_response_events.append(
                convert_event(function_response_event)
            )

        if pending_event:
          response_list.append(pending_event)
      return response_list


def convert_event(event: StorageEvent) -> Event:
  return Event(
      id=event.id,
      author=event.author,
      invocation_id=event.invocation_id,
      content=event.content,
      actions=event.actions,
      timestamp=event.timestamp.timestamp(),
  )


================================================
File: sessions/session.py
================================================
from typing import Any

from pydantic import BaseModel
from pydantic import ConfigDict
from pydantic import Field

from ..events import Event


class Session(BaseModel):
  model_config = ConfigDict(
      extra='forbid',
      arbitrary_types_allowed=True,
  )
  # TODO: Makes this required.
  id: str = ''
  app_name: str
  user_id: str
  state: dict[str, Any] = Field(default_factory=dict)
  """The state of the session."""

  events: list[Event] = Field(default_factory=list)
  """The events of the session, e.g. user input, model response, function
  call/response, etc
  """

  last_update_time: float = 0.0


================================================
File: sessions/state.py
================================================
from typing import Any


class State:
  """A state dict that maintain the current value and the pending-commit delta."""

  def __init__(self, value: dict[str, Any], delta: dict[str, Any]):
    """
    Args:
      value: The current value of the state dict.
      delta: The delta change to the current value that hasn't been commited.
    """
    self._value = value
    self._delta = delta

  def __getitem__(self, key: str) -> Any:
    if key in self._delta:
      return self._delta[key]
    return self._value[key]

  def __setitem__(self, key: str, value: Any):
    # TODO: make new change only store in delta, so that self._value is only
    #   updated at the storage commit time.
    self._value[key] = value
    self._delta[key] = value

  def __contains__(self, key: str) -> bool:
    return key in self._value or key in self._delta

  def has_delta(self) -> bool:
    """Whether the state has pending detla."""
    return bool(self._delta)

  def get(self, key: str, default: Any = None) -> Any:
    if key not in self:
      return default
    return self[key]

  def update(self, delta: dict[str, Any]):
    self._value.update(delta)
    self._delta.update(delta)

  def to_dict(self) -> dict[str, Any]:
    result = {}
    result.update(self._value)
    result.update(self._delta)
    return result


================================================
File: tools/__init__.py
================================================
from .async_function_tool import AsyncFunctionTool
from .base_tool import BaseTool
from .function_tool import FunctionTool
from .get_user_choice_tool import get_user_choice_tool as get_user_choice
from .google_search_tool import google_search_tool as google_search
from .load_artifacts_tool import load_artifacts_tool as load_artifacts
from .load_memory_tool import load_memory_tool as load_memory
from .preload_memory_tool import preload_memory_tool as preload_memory
from .tool_context import ToolContext


def exit_loop(tool_context: ToolContext):
  """Exits the loop.

  Call this function only when you are instructed to do so.
  """
  tool_context.actions.escalate = True


def transfer_to_agent(agent_name: str, tool_context: ToolContext):
  """Transfer the question to another agent."""
  tool_context.actions.transfer_to_agent = agent_name


================================================
File: tools/_automatic_function_calling_util.py
================================================
"""Forked from google3/third_party/py/google/genai/_automatic_function_calling_util.py temporarily."""

import inspect
from typing import Any
from typing import Callable
from typing import Dict
from typing import Literal

import pydantic
from google.genai import types
from pydantic import BaseModel, create_model
from pydantic import fields as pydantic_fields

from types import FunctionType

from . import function_parameter_parse_util

_py_type_2_schema_type = {
    'str': 'STRING',
    'int': 'INTEGER',
    'float': 'NUMBER',
    'bool': 'BOOLEAN',
    'string': 'STRING',
    'integer': 'INTEGER',
    'number': 'NUMBER',
    'boolean': 'BOOLEAN',
    'list': 'ARRAY',
    'array': 'ARRAY',
    'tuple': 'ARRAY',
    'object': 'OBJECT',
    'Dict': 'OBJECT',
    'List': 'ARRAY',
    'Tuple': 'ARRAY',
    'Any': 'TYPE_UNSPECIFIED',
}


def _get_fields_dict(func: Callable) -> Dict:
  param_signature = dict(inspect.signature(func).parameters)
  fields_dict = {
      name: (
          # 1. We infer the argument type here: use Any rather than None so
          # it will not try to auto-infer the type based on the default value.
          (
              param.annotation
              if param.annotation != inspect.Parameter.empty
              else Any
          ),
          pydantic.Field(
              # 2. We do not support default values for now.
              default=(
                  param.default
                  if param.default != inspect.Parameter.empty
                  # ! Need to use Undefined instead of None
                  else pydantic_fields.PydanticUndefined
              ),
              # 3. Do not support parameter description for now.
              description=None,
          ),
      )
      for name, param in param_signature.items()
      # We do not support *args or **kwargs
      if param.kind
      in (
          inspect.Parameter.POSITIONAL_OR_KEYWORD,
          inspect.Parameter.KEYWORD_ONLY,
          inspect.Parameter.POSITIONAL_ONLY,
      )
  }
  return fields_dict


def _annotate_nullable_fields(schema: Dict):
  for _, property_schema in schema.get('properties', {}).items():
    # for Optional[T], the pydantic schema is:
    # {
    #   "type": "object",
    #   "properties": {
    #     "anyOf": [
    #       {
    #         "type": "null"
    #       },
    #       {
    #         "type": "T"
    #       }
    #     ]
    #   }
    # }
    for type_ in property_schema.get('anyOf', []):
      if type_.get('type') == 'null':
        property_schema['nullable'] = True
        property_schema['anyOf'].remove(type_)
        break


def _annotate_required_fields(schema: Dict):
  required = [
      field_name
      for field_name, field_schema in schema.get('properties', {}).items()
      if not field_schema.get('nullable') and 'default' not in field_schema
  ]
  schema['required'] = required


def _remove_any_of(schema: Dict):
  for _, property_schema in schema.get('properties', {}).items():
    union_types = property_schema.pop('anyOf', None)
    # Take the first non-null type.
    if union_types:
      for type_ in union_types:
        if type_.get('type') != 'null':
          property_schema.update(type_)


def _remove_default(schema: Dict):
  for _, property_schema in schema.get('properties', {}).items():
    property_schema.pop('default', None)


def _remove_nullable(schema: Dict):
  for _, property_schema in schema.get('properties', {}).items():
    property_schema.pop('nullable', None)


def _remove_title(schema: Dict):
  for _, property_schema in schema.get('properties', {}).items():
    property_schema.pop('title', None)


def _get_pydantic_schema(func: Callable) -> Dict:
  fields_dict = _get_fields_dict(func)
  if 'tool_context' in fields_dict.keys():
    fields_dict.pop('tool_context')
  return pydantic.create_model(func.__name__, **fields_dict).model_json_schema()


def _process_pydantic_schema(vertexai: bool, schema: Dict) -> Dict:
  _annotate_nullable_fields(schema)
  _annotate_required_fields(schema)
  if not vertexai:
    _remove_any_of(schema)
    _remove_default(schema)
    _remove_nullable(schema)
    _remove_title(schema)
  return schema


def _map_pydantic_type_to_property_schema(property_schema: Dict):
  if 'type' in property_schema:
    property_schema['type'] = _py_type_2_schema_type.get(
        property_schema['type'], 'TYPE_UNSPECIFIED'
    )
    if property_schema['type'] == 'ARRAY':
      _map_pydantic_type_to_property_schema(property_schema['items'])
  for type_ in property_schema.get('anyOf', []):
    if 'type' in type_:
      type_['type'] = _py_type_2_schema_type.get(
          type_['type'], 'TYPE_UNSPECIFIED'
      )
      # TODO: To investigate. Unclear why a Type is needed with 'anyOf' to
      # avoid google.genai.errors.ClientError: 400 INVALID_ARGUMENT.
      property_schema['type'] = type_['type']


def _map_pydantic_type_to_schema_type(schema: Dict):
  for _, property_schema in schema.get('properties', {}).items():
    _map_pydantic_type_to_property_schema(property_schema)


def _get_return_type(func: Callable) -> Any:
  return _py_type_2_schema_type.get(
      inspect.signature(func).return_annotation.__name__,
      inspect.signature(func).return_annotation.__name__,
  )


def build_function_declaration(
    func: Callable | BaseModel,
    ignore_params: list[str] = [],
    variant:Literal["GOOGLE_AI", "VERTEX_AI", "DEFAULT"] = "GOOGLE_AI"
) -> types.FunctionDeclaration:
  signature = inspect.signature(func)
  should_update_signature = False
  new_func = None
  for name, _ in signature.parameters.items():
    if name in ignore_params:
      should_update_signature = True
      break
  if should_update_signature:
    new_params = [
        param
        for name, param in signature.parameters.items()
        if name not in ignore_params
    ]
    if isinstance(func, type):
      fields = {
          name: (param.annotation, param.default)
          for name, param in signature.parameters.items()
          if name not in ignore_params
      }
      new_func = create_model(func.__name__, **fields)
    else:
      new_sig = signature.replace(parameters=new_params)
      new_func = FunctionType(
          func.__code__,
          func.__globals__,
          func.__name__,
          func.__defaults__,
          func.__closure__,
      )
      new_func.__signature__ = new_sig

  return (
      from_function_with_options(func, variant)
      if not should_update_signature
      else from_function_with_options(
          new_func, variant
      )
  )


def build_function_declaration_for_langchain(
    vertexai: bool, name, description, func, param_pydantic_schema
) -> types.FunctionDeclaration:
  param_pydantic_schema = _process_pydantic_schema(
      vertexai, param_pydantic_schema
  )
  param_copy = param_pydantic_schema.copy()
  required_fields = param_copy.pop('required', [])
  before_param_pydantic_schema = {
      'properties': param_copy,
      'required': required_fields,
  }
  return build_function_declaration_util(
      vertexai, name, description, func, before_param_pydantic_schema
  )


def build_function_declaration_for_params_for_crewai(
    vertexai: bool, name, description, func, param_pydantic_schema
) -> types.FunctionDeclaration:
  param_pydantic_schema = _process_pydantic_schema(
      vertexai, param_pydantic_schema
  )
  param_copy = param_pydantic_schema.copy()
  return build_function_declaration_util(
      vertexai, name, description, func, param_copy
  )

def build_function_declaration_util(
    vertexai: bool, name, description, func, before_param_pydantic_schema
) -> types.FunctionDeclaration:
  _map_pydantic_type_to_schema_type(before_param_pydantic_schema)
  properties = before_param_pydantic_schema.get('properties', {})
  function_declaration = types.FunctionDeclaration(
      parameters=types.Schema(
          type='OBJECT',
          properties=properties,
      )
      if properties
      else None,
      description=description,
      name=name,
  )
  if vertexai and isinstance(func, Callable):
    return_pydantic_schema = _get_return_type(func)
    function_declaration.response = types.Schema(
        type=return_pydantic_schema,
    )
  return function_declaration

def from_function_with_options(
      func: Callable,
      variant: Literal['GOOGLE_AI', 'VERTEX_AI', 'DEFAULT'] = 'GOOGLE_AI',
  ) -> 'types.FunctionDeclaration':


  supported_variants = ['GOOGLE_AI', 'VERTEX_AI', 'DEFAULT']
  if variant not in supported_variants:
    raise ValueError(
        f'Unsupported variant: {variant}. Supported variants are:'
        f' {", ".join(supported_variants)}'
    )

  parameters_properties = {}
  for name, param in inspect.signature(func).parameters.items():
    if param.kind in (
        inspect.Parameter.POSITIONAL_OR_KEYWORD,
        inspect.Parameter.KEYWORD_ONLY,
        inspect.Parameter.POSITIONAL_ONLY,
    ):
      schema = function_parameter_parse_util._parse_schema_from_parameter(
          variant, param, func.__name__
      )
      parameters_properties[name] = schema
  declaration = types.FunctionDeclaration(
      name=func.__name__,
      description=func.__doc__,
  )
  if parameters_properties:
    declaration.parameters = types.Schema(
        type='OBJECT',
        properties=parameters_properties,
    )
    if variant == 'VERTEX_AI':
      declaration.parameters.required = (
          function_parameter_parse_util._get_required_fields(
              declaration.parameters
          )
      )
  if not variant == 'VERTEX_AI':
    return declaration

  return_annotation = inspect.signature(func).return_annotation
  if return_annotation is inspect._empty:
    return declaration

  declaration.response = (
      function_parameter_parse_util._parse_schema_from_parameter(
          variant,
          inspect.Parameter(
              'return_value',
              inspect.Parameter.POSITIONAL_OR_KEYWORD,
              annotation=return_annotation,
          ),
          func.__name__,
      )
  )
  return declaration




================================================
File: tools/agent_tool.py
================================================
from typing import Any

import os

from google.genai import types
from pydantic import model_validator

from ..agents.agent import Agent
from ..runners import InMemoryRunner
from . import _automatic_function_calling_util
from .base_tool import BaseTool
from .tool_context import ToolContext


class AgentTool(BaseTool):

  def __init__(self, agent: Agent):
    self.agent = agent
    self.skip_summarization: bool = False
    """Whether to skip summarization of the agent output."""

    super().__init__(agent.name, agent.description)

  @model_validator(mode='before')
  @classmethod
  def populate_name(cls, data: Any) -> Any:
    data['name'] = data['agent'].name
    return data

  def get_declaration(self) -> types.FunctionDeclaration:
    use_vertexai = os.environ.get('GOOGLE_GENAI_USE_VERTEXAI', '0').lower() in [
        'true',
        '1',
    ]
    if self.agent.input_schema:
      result = _automatic_function_calling_util.build_function_declaration(
          func=self.agent.input_schema,
          variant='VERTEX_AI' if use_vertexai else 'GOOGLE_AI',
      )
    else:
      result = types.FunctionDeclaration(
          parameters=types.Schema(
              type='OBJECT',
              properties={
                  'request': types.Schema(
                      type='STRING',
                  ),
              },
              required=['request'],
          ),
          description=self.agent.description,
          name=self.name,
      )
    result.name = self.name
    return result

  def call(
      self,
      *,
      args: dict[str, Any],
      tool_context: ToolContext,
  ) -> Any:
    if self.agent.input_schema:
      input_value = self.agent.input_schema.model_validate(args)
    else:
      input_value = args['request']

    if self.agent.input_schema:
      if isinstance(input_value, dict):
        input_value = self.agent.input_schema.model_validate(input_value)
      if not isinstance(input_value, self.agent.input_schema):
        raise ValueError(
            f'Input value {input_value} is not of type {self.input_schema}.'
        )
      content = types.Content(
          role='user',
          parts=[
              types.Part.from_text(
                  text=input_value.model_dump_json(exclude_none=True)
              )
          ],
      )
    else:
      content = types.Content(
          role='user',
          parts=[types.Part.from_text(text=input_value)],
      )
    runner = InMemoryRunner(self.agent)
    session = runner.session_service.create(
        'tmp_app', 'tmp_user', state=tool_context.state.to_dict()
    )
    # TODO(ybo): Remove this hack to let agent_tool to chagne parent's session state.
    session.state = tool_context.state
    events = list(runner.run(session=session, new_message=content))
    last_event = events[-1]

    if runner.artifact_service:
      for artifact_name in runner.artifact_service.list_keys(session.id):
        tool_context.save_artifact(
            artifact_name,
            runner.artifact_service.load(session.id, artifact_name),
        )

    if self.agent.output_schema:
      tool_result = self.agent.output_schema.model_validate_json(
          last_event.content.parts[0].text
      ).model_dump(exclude_none=True)
    else:
      tool_result = last_event.content.parts[0].text
    return tool_result


================================================
File: tools/async_function_tool.py
================================================
from typing import Callable
from pydantic import Field
from .function_tool import FunctionTool


class AsyncFunctionTool(FunctionTool):
  """A function tool that returns the result asynchronously."""

  def __init__(self, func: Callable):
    super().__init__(func)
    self.is_async = True


================================================
File: tools/base_tool.py
================================================
import abc
from typing import Any
from typing import TYPE_CHECKING

from google.genai import types

from .tool_context import ToolContext

if TYPE_CHECKING:
  from ..models.llm_request import LlmRequest


# We use regular class instead of Pydantic to have better control on computed
# properties.
class BaseTool:
  """Base class for all tools."""

  def __init__(self, name: str, description: str):
    self.name = name
    self.description = description
    self.is_async = False

  @abc.abstractmethod
  def get_declaration(self) -> types.FunctionDeclaration:
    pass

  def process_llm_request(
      self, tool_context: ToolContext, llm_request: 'LlmRequest'
  ):
    pass

  @abc.abstractmethod
  def call(self, *, args: dict[str, Any], tool_context: 'ToolContext') -> Any:
    """Call the tool."""
    pass


================================================
File: tools/crewai_tool.py
================================================
from __future__ import annotations

from typing import TYPE_CHECKING

from google.genai import types

from . import _automatic_function_calling_util
from .function_tool import FunctionTool

if TYPE_CHECKING:
  from crewai_tools.tools.base_tool import BaseTool as CrewaiBaseTool


class CrewaiTool(FunctionTool):
  """Use this class to wrap a CrewAI tool.

  If the original tool name and description are not suitable, you can override
  them in the constructor.
  """

  tool: CrewaiBaseTool

  def __init__(self, tool: CrewaiBaseTool, *, name: str, description: str):
    super().__init__(tool.run)
    self.tool = tool
    if name:
      self.name = name
    elif tool.name:
      # Right now, CrewAI tool name contains white spaces. White spaces are
      # not supported in our framework. So we replace them with "_".
      self.name = tool.name.replace(" ", "_").lower()
    if description:
      self.description = description
    elif tool.description:
      self.description = tool.description

  def get_declaration(self) -> types.FunctionDeclaration:
    """Build the function declaration for the tool."""
    function_declaration = _automatic_function_calling_util.build_function_declaration_for_params_for_crewai(
        False,
        self.name,
        self.description,
        self.func,
        self.tool.args_schema.model_json_schema(),
    )
    return function_declaration


================================================
File: tools/function_parameter_parse_util.py
================================================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import inspect
import types as typing_types
from typing import Any, Callable, Literal, Union, _GenericAlias, get_args, get_origin
import pydantic
from google.genai import types

_py_builtin_type_to_schema_type = {
    str: 'STRING',
    int: 'INTEGER',
    float: 'NUMBER',
    bool: 'BOOLEAN',
    list: 'ARRAY',
    dict: 'OBJECT',
}


def _is_builtin_primitive_or_compound(
    annotation: inspect.Parameter.annotation,
) -> bool:
  return annotation in _py_builtin_type_to_schema_type.keys()


def _raise_for_any_of_if_mldev(schema: types.Schema):
  if schema.any_of:
    raise ValueError(
        'AnyOf is not supported in function declaration schema for Google AI.'
    )


def _raise_for_default_if_mldev(schema: types.Schema):
  if schema.default is not None:
    raise ValueError(
        'Default value is not supported in function declaration schema for'
        ' Google AI.'
    )


def _raise_for_nullable_if_mldev(schema: types.Schema):
  if schema.nullable:
    raise ValueError(
        'Nullable is not supported in function declaration schema for'
        ' Google AI.'
    )


def _raise_if_schema_unsupported(variant: str, schema: types.Schema):
  if not variant == 'VERTEX_AI':
    _raise_for_any_of_if_mldev(schema)
    _raise_for_default_if_mldev(schema)
    _raise_for_nullable_if_mldev(schema)


def _is_default_value_compatible(
    default_value: Any, annotation: inspect.Parameter.annotation
) -> bool:
  # None type is expected to be handled external to this function
  if _is_builtin_primitive_or_compound(annotation):
    return isinstance(default_value, annotation)

  if (
      isinstance(annotation, _GenericAlias)
      or isinstance(annotation, typing_types.GenericAlias)
      or isinstance(annotation, typing_types.UnionType)
  ):
    origin = get_origin(annotation)
    if origin in (Union, typing_types.UnionType):
      return any(
          _is_default_value_compatible(default_value, arg)
          for arg in get_args(annotation)
      )

    if origin is dict:
      return isinstance(default_value, dict)

    if origin is list:
      if not isinstance(default_value, list):
        return False
      # most tricky case, element in list is union type
      # need to apply any logic within all
      # see test case test_generic_alias_complex_array_with_default_value
      # a: typing.List[int | str | float | bool]
      # default_value: [1, 'a', 1.1, True]
      return all(
          any(
              _is_default_value_compatible(item, arg)
              for arg in get_args(annotation)
          )
          for item in default_value
      )

    if origin is Literal:
      return default_value in get_args(annotation)

  # return False for any other unrecognized annotation
  # let caller handle the raise
  return False


def _parse_schema_from_parameter(
    variant: str, param: inspect.Parameter, func_name: str
) -> types.Schema:
  """parse schema from parameter.

  from the simplest case to the most complex case.
  """
  schema = types.Schema()
  default_value_error_msg = (
      f'Default value {param.default} of parameter {param} of function'
      f' {func_name} is not compatible with the parameter annotation'
      f' {param.annotation}.'
  )
  if _is_builtin_primitive_or_compound(param.annotation):
    if param.default is not inspect.Parameter.empty:
      if not _is_default_value_compatible(param.default, param.annotation):
        raise ValueError(default_value_error_msg)
      schema.default = param.default
    schema.type = _py_builtin_type_to_schema_type[param.annotation]
    _raise_if_schema_unsupported(variant, schema)
    return schema
  if (
      isinstance(param.annotation, typing_types.UnionType)
      # only parse simple UnionType, example int | str | float | bool
      # complex types.UnionType will be invoked in raise branch
      and all(
          (_is_builtin_primitive_or_compound(arg) or arg is type(None))
          for arg in get_args(param.annotation)
      )
  ):
    schema.type = 'OBJECT'
    schema.any_of = []
    unique_types = set()
    for arg in get_args(param.annotation):
      if arg.__name__ == 'NoneType':  # Optional type
        schema.nullable = True
        continue
      schema_in_any_of = _parse_schema_from_parameter(
          variant,
          inspect.Parameter(
              'item', inspect.Parameter.POSITIONAL_OR_KEYWORD, annotation=arg
          ),
          func_name,
      )
      if (
          schema_in_any_of.model_dump_json(exclude_none=True)
          not in unique_types
      ):
        schema.any_of.append(schema_in_any_of)
        unique_types.add(schema_in_any_of.model_dump_json(exclude_none=True))
    if len(schema.any_of) == 1:  # param: list | None -> Array
      schema.type = schema.any_of[0].type
      schema.any_of = None
    if (
        param.default is not inspect.Parameter.empty
        and param.default is not None
    ):
      if not _is_default_value_compatible(param.default, param.annotation):
        raise ValueError(default_value_error_msg)
      schema.default = param.default
    _raise_if_schema_unsupported(variant, schema)
    return schema
  if isinstance(param.annotation, _GenericAlias) or isinstance(
      param.annotation, typing_types.GenericAlias
  ):
    origin = get_origin(param.annotation)
    args = get_args(param.annotation)
    if origin is dict:
      schema.type = 'OBJECT'
      if param.default is not inspect.Parameter.empty:
        if not _is_default_value_compatible(param.default, param.annotation):
          raise ValueError(default_value_error_msg)
        schema.default = param.default
      _raise_if_schema_unsupported(variant, schema)
      return schema
    if origin is Literal:
      if not all(isinstance(arg, str) for arg in args):
        raise ValueError(
            f'Literal type {param.annotation} must be a list of strings.'
        )
      schema.type = 'STRING'
      schema.enum = list(args)
      if param.default is not inspect.Parameter.empty:
        if not _is_default_value_compatible(param.default, param.annotation):
          raise ValueError(default_value_error_msg)
        schema.default = param.default
      _raise_if_schema_unsupported(variant, schema)
      return schema
    if origin is list:
      schema.type = 'ARRAY'
      schema.items = _parse_schema_from_parameter(
          variant,
          inspect.Parameter(
              'item',
              inspect.Parameter.POSITIONAL_OR_KEYWORD,
              annotation=args[0],
          ),
          func_name,
      )
      if param.default is not inspect.Parameter.empty:
        if not _is_default_value_compatible(param.default, param.annotation):
          raise ValueError(default_value_error_msg)
        schema.default = param.default
      _raise_if_schema_unsupported(variant, schema)
      return schema
    if origin is Union:
      schema.any_of = []
      schema.type = 'OBJECT'
      unique_types = set()
      for arg in args:
        if arg.__name__ == 'NoneType':  # Optional type
          schema.nullable = True
          continue
        schema_in_any_of = _parse_schema_from_parameter(
            variant,
            inspect.Parameter(
                'item',
                inspect.Parameter.POSITIONAL_OR_KEYWORD,
                annotation=arg,
            ),
            func_name,
        )
        if (
            len(param.annotation.__args__) == 2
            and type(None) in param.annotation.__args__
        ):  # Optional type
          for optional_arg in param.annotation.__args__:
            if hasattr(optional_arg, '__origin__') and optional_arg.__origin__ is list:
              # Optional type with list, for example Optional[list[str]]
              schema.items = schema_in_any_of.items
        if (
            schema_in_any_of.model_dump_json(exclude_none=True)
            not in unique_types
        ):
          schema.any_of.append(schema_in_any_of)
          unique_types.add(schema_in_any_of.model_dump_json(exclude_none=True))
      if len(schema.any_of) == 1:  # param: Union[List, None] -> Array
        schema.type = schema.any_of[0].type
        schema.any_of = None
      if (
          param.default is not None
          and param.default is not inspect.Parameter.empty
      ):
        if not _is_default_value_compatible(param.default, param.annotation):
          raise ValueError(default_value_error_msg)
        schema.default = param.default
      _raise_if_schema_unsupported(variant, schema)
      return schema
      # all other generic alias will be invoked in raise branch
  if (
      inspect.isclass(param.annotation)
      # for user defined class, we only support pydantic model
      and issubclass(param.annotation, pydantic.BaseModel)
  ):
    if (
        param.default is not inspect.Parameter.empty
        and param.default is not None
    ):
      schema.default = param.default
    schema.type = 'OBJECT'
    schema.properties = {}
    for field_name, field_info in param.annotation.model_fields.items():
      schema.properties[field_name] = _parse_schema_from_parameter(
          variant,
          inspect.Parameter(
              field_name,
              inspect.Parameter.POSITIONAL_OR_KEYWORD,
              annotation=field_info.annotation,
          ),
          func_name,
      )
    _raise_if_schema_unsupported(variant, schema)
    return schema
  raise ValueError(
      f'Failed to parse the parameter {param} of function {func_name} for'
      ' automatic function calling.Automatic function calling works best with'
      ' simpler function signature schema,consider manually parse your'
      f' function declaration for function {func_name}.'
  )


def _get_required_fields(schema: types.Schema) -> list[str]:
  if not schema.properties:
    return
  return [
      field_name
      for field_name, field_schema in schema.properties.items()
      if not field_schema.nullable and field_schema.default is None
  ]

================================================
File: tools/function_tool.py
================================================
import inspect
from typing import Any
from typing import Callable

from google.genai import types

from ._automatic_function_calling_util import build_function_declaration
from .base_tool import BaseTool
from .tool_context import ToolContext


class FunctionTool(BaseTool):

  def __init__(self, func: Callable[..., Any]):
    super().__init__(func.__name__, func.__doc__)
    self.func = func

  def get_declaration(self) -> types.FunctionDeclaration:
    function_decl = types.FunctionDeclaration.model_validate(
        build_function_declaration(
            func=self.func,
            # The model doesn't understand the function context.
            ignore_params=['tool_context'],
        )
    )

    return function_decl

  def call(
      self,
      *,
      args: dict[str, Any],
      tool_context: ToolContext,
  ) -> Any:
    args_to_call = args.copy()
    signature = inspect.signature(self.func)
    if 'tool_context' in signature.parameters:
      args_to_call['tool_context'] = tool_context
    return self.func(**args_to_call) or {}


================================================
File: tools/get_user_choice_tool.py
================================================
from .async_function_tool import AsyncFunctionTool
from .tool_context import ToolContext


def get_user_choice(
    options: list[str], tool_context: ToolContext
) -> str | None:
  """Provides the options to the user and asks them to choose one.
  """
  tool_context.actions.skip_summarization = True
  return None


get_user_choice_tool = AsyncFunctionTool(func=get_user_choice)


================================================
File: tools/google_search_tool.py
================================================
from typing import TYPE_CHECKING

from google.genai import types

from .base_tool import BaseTool
from .tool_context import ToolContext

if TYPE_CHECKING:
  from ..models import LlmRequest


class GoogleSearchTool(BaseTool):

  def __init__(self):
    # Name and description are not used because this is a model built-in tool.
    super().__init__(name='google_search', description='google_search')

  def process_llm_request(
      self,
      tool_context: ToolContext,
      llm_request: 'LlmRequest',
  ):
    if llm_request.model.startswith('gemini-1'):
      if llm_request.config.tools:
        print(llm_request.config.tools)
        raise ValueError(
            'Google search tool can not be used with other tools in Gemini 1.x.'
        )
      llm_request.config.tools.append(
          types.Tool(google_search_retrieval=types.GoogleSearchRetrieval())
      )
    elif llm_request.model.startswith('gemini-2'):
      llm_request.config.tools.append(
          types.Tool(google_search=types.GoogleSearch())
      )
    else:
      raise ValueError(
          f'Google search tool is not supported for model {llm_request.model}'
      )

google_search_tool = GoogleSearchTool()


================================================
File: tools/langchain_tool.py
================================================
from typing import Any

from google.genai import types
from pydantic import model_validator

from . import _automatic_function_calling_util
from .function_tool import FunctionTool


class LangchainTool(FunctionTool):
  """Use this class to wrap a langchain tool.

  If the original tool name and description are not suitable, you can override
  them in the constructor.
  """

  tool: Any

  def __init__(self, tool: Any):
    super().__init__(tool.run)
    self.tool = tool
    if tool.name:
      self.name = tool.name
    if tool.description:
      self.description = tool.description

  @model_validator(mode='before')
  @classmethod
  def populate_name(cls, data: Any) -> Any:
    # Override this to not use function's signature name as it's
    # mostly "run" or "invoke" for thir-party tools.
    return data

  def get_declaration(self) -> types.FunctionDeclaration:
    """Build the function declaration for the tool."""
    from langchain.agents import Tool
    from langchain_core.tools import BaseTool

    # There are two types of tools:
    # 1. BaseTool: the tool is defined in langchain.tools.
    # 2. Other tools: the tool doesn't inherit any class but follow some
    #    conventions, like having a "run" method.
    if isinstance(self.tool, BaseTool):
      tool_wrapper = Tool(
          name=self.name,
          func=self.func,
          description=self.description,
      )
      function_declaration = _automatic_function_calling_util.build_function_declaration_for_langchain(
          False,
          self.name,
          self.description,
          tool_wrapper.func,
          tool_wrapper.args,
      )
      return function_declaration
    else:
      # Need to provide a way to override the function names and descriptions
      # as the original function names are mostly ".run" and the descriptions
      # may not meet users' needs.
      function_declaration = (
          _automatic_function_calling_util.build_function_declaration(
              func=self.tool.run,
          )
      )
      return function_declaration


================================================
File: tools/load_artifacts_tool.py
================================================
import json
from typing import Any
from typing import Callable
from typing import TYPE_CHECKING

from google.genai import types

from .function_tool import FunctionTool
from .tool_context import ToolContext

if TYPE_CHECKING:
  from ..models import LlmRequest


def load_artifacts(artifact_names: list[str]):
  """Loads the artifacts and adds them to the session."""
  return {'artifact_names': artifact_names}


class LoadArtifactsTool(FunctionTool):

  def __init__(self):
    super().__init__(load_artifacts)

  def process_llm_request(
      self,
      tool_context: ToolContext,
      llm_request: 'LlmRequest',
  ):
    artifact_names = tool_context.list_artifacts()
    if not artifact_names:
      return

    # Tell the model about the available artifacts.
    llm_request.append_instructions([f"""You have a list of artifacts:
  {json.dumps(artifact_names)}

  When the user asks questions about any of the artifacts, you should call the
  `load_artifacts` function to load the artifact. Do not generate any text other
  than the function call.
  """])

    # Attache the content of the artifacts if the model requests them.
    # This only adds the content to the model request, instead of the session.
    if llm_request.contents:
      function_response = llm_request.contents[-1].parts[0].function_response
      if function_response and function_response.name == 'load_artifacts':
        artifact_names = function_response.response['artifact_names']
        for artifact_name in artifact_names:
          artifact = tool_context.load_artifact(artifact_name)
          llm_request.contents.append(
              types.Content(
                  role='user',
                  parts=[
                      types.Part.from_text(
                          text=f'Artifact {artifact_name} is:'
                      ),
                      artifact,
                  ],
              )
          )

load_artifacts_tool = LoadArtifactsTool()


================================================
File: tools/load_memory_tool.py
================================================
from typing import TYPE_CHECKING

from .function_tool import FunctionTool
from .tool_context import ToolContext

if TYPE_CHECKING:
  from ..models import LlmRequest
  from ..memory.base_memory_service import MemoryResult


def load_memory(query: str, tool_context: ToolContext) -> 'list[MemoryResult]':
  """Loads the memory for the current user."""
  response = tool_context.search_memory(query)
  return response.memories


class LoadMemoryTool(FunctionTool):

  def __init__(self):
    super().__init__(load_memory)

  def process_llm_request(
      self,
      tool_context: ToolContext,
      llm_request: 'LlmRequest',
  ):
    # Tell the model about the memory.
    llm_request.append_instructions([f"""
You have memory. You can use it to answer questions. If any questions need
you to look up the memory, you should call load_memory function with a query.
"""])


load_memory_tool = LoadMemoryTool()


================================================
File: tools/load_web_page.py
================================================
"""Tool for web browse."""

import requests


def load_web_page(url: str) -> str:
  """Fetches the content in the url and returns the text in it.

  Args:
      url (str): The url to browse.

  Returns:
      str: The text content of the url.
  """
  from bs4 import BeautifulSoup

  response = requests.get(url)

  if response.status_code == 200:
    soup = BeautifulSoup(response.content, 'lxml')
    text = soup.get_text(separator='\n', strip=True)
  else:
    text = f'Failed to fetch url: {url}'

  # Split the text into lines, filtering out very short lines
  # (e.g., single words or short subtitles)
  return '\n'.join(line for line in text.splitlines() if len(line.split()) > 3)


================================================
File: tools/preload_memory_tool.py
================================================
from datetime import datetime
from typing import TYPE_CHECKING

from .base_tool import BaseTool
from .tool_context import ToolContext

if TYPE_CHECKING:
  from ..models import LlmRequest


class PreloadMemoryTool(BaseTool):

  def __init__(self):
    # Name and description are not used because this tool only changes llm_request.
    super().__init__(name='preload_memory', description='preload_memory')

  def process_llm_request(
      self,
      tool_context: ToolContext,
      llm_request: 'LlmRequest',
  ):
    parts = tool_context.user_content.parts
    if not parts or not parts[0].text:
      return
    query = parts[0].text
    response = tool_context.search_memory(query)
    if not response.memories:
      return
    memory_text = ''
    for memory in response.memories:
      time_str = datetime.fromtimestamp(memory.events[0].timestamp).isoformat()
      memory_text += f'Time: {time_str}\n'
      for event in memory.events:
        # TODO: support multi-part content.
        if event.content.parts[0].text:
          memory_text += f'{event.author}: {event.content.parts[0].text}\n'
    si = f"""The following content is from your previous conversations with the user.
They may be useful for answering the user's current query.
<PAST_CONVERSATIONS>
{memory_text}
</PAST_CONVERSATIONS>
"""
    llm_request.append_instructions([si])


preload_memory_tool = PreloadMemoryTool()


================================================
File: tools/tool_context.py
================================================
from __future__ import annotations

from typing import TYPE_CHECKING

from ..agents.callback_context import CallbackContext

if TYPE_CHECKING:
  from ..agents.invocation_context import InvocationContext
  from ..events.event_actions import EventActions
  from ..memory.base_memory_service import SearchMemoryResponse


class ToolContext(CallbackContext):

  def __init__(
      self,
      invocation_context: InvocationContext,
      *,
      function_call_event_id: str | None = None,
      function_call_id: str | None = None,
      event_actions: EventActions | None = None,
  ):
    super().__init__(invocation_context, event_actions=event_actions)
    self.function_call_event_id = function_call_event_id
    self.function_call_id = function_call_id

  @property
  def actions(self) -> EventActions:
    return self._event_actions

  def list_artifacts(self) -> list[str]:
    """Lists the filenames of the artifacts attached to the current session."""
    if self._invocation_context.artifact_service is None:
      raise ValueError('Artifact service is not initialized.')
    return self._invocation_context.artifact_service.list_keys(
        self._invocation_context.session.id
    )

  def search_memory(self, query: str) -> 'SearchMemoryResponse':
    """Searches the memory of the current user."""
    if self._invocation_context.memory_service is None:
      raise ValueError('Memory service is not available.')
    return self._invocation_context.memory_service.search(
        self._invocation_context.app_name,
        self._invocation_context.user_id,
        query,
    )


================================================
File: tools/toolbox_tool.py
================================================
import asyncio
from typing import Any

from . import _automatic_function_calling_util
from .langchain_tool import LangchainTool


class ToolboxTool:
  """Use this class to wrap a toolbox tool."""

  toolbox_client: Any

  def __init__(self, url: str):
    from toolbox_langchain_sdk import ToolboxClient

    self.toolbox_client = ToolboxClient(url)

  def get_tool(self, tool_name: str) -> LangchainTool:
    tool = asyncio.run(self.toolbox_client.load_tool(tool_name))
    return LangchainTool(tool)

  def get_toolset(self, toolset_name: str) -> list[LangchainTool]:
    tools = asyncio.run(self.toolbox_client.load_toolset(toolset_name))
    return [LangchainTool(tool) for tool in tools]


================================================
File: tools/retrieval/__init__.py
================================================
import logging
from .base_retrieval_tool import BaseRetrievalTool
from .files_retrieval import FilesRetrieval
from .llama_index_retrieval import LlamaIndexRetrieval

logger = logging.getLogger(__name__)

__all__ = [
    'BaseRetrievalTool',
    'FilesRetrieval',
    'LlamaIndexRetrieval',
]

try:
  from .vertex_rag_retrieval import VertexRagRetrieval

  __all__.append('VertexRagRetrieval')
except ImportError:
  logger.warning(
      'The Vertex sdk is not installed. If you want to use the Vertex RAG with'
      ' agents, please install it. If not, you can ignore this warning.'
  )


================================================
File: tools/retrieval/base_retrieval_tool.py
================================================
from google.genai import types

from .. import BaseTool


class BaseRetrievalTool(BaseTool):

  def __init__(self, *, name: str, description: str):
    super().__init__(name, description)

  def get_declaration(self) -> types.FunctionDeclaration:
    return types.FunctionDeclaration(
        name=self.name,
        description=self.description,
        parameters=types.Schema(
            type='OBJECT',
            properties={
                'query': types.Schema(
                    type='STRING',
                    description='The query to retrieve.',
                ).model_dump(exclude_none=True),
            },
        ),
    )


================================================
File: tools/retrieval/files_retrieval.py
================================================
"""Provides data for the agent."""

from typing import Any

from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.base.base_retriever import BaseRetriever
from pydantic import Field, model_validator

from .llama_index_retrieval import LlamaIndexRetrieval


class FilesRetrieval(LlamaIndexRetrieval):

  def __init__(self, *, name: str, description: str, input_dir: str):
    self.input_dir = input_dir

    print(f'Loading data from {input_dir}')
    retriever = VectorStoreIndex.from_documents(
        SimpleDirectoryReader(input_dir).load_data()
    ).as_retriever()
    super().__init__(name, description, retriever)


================================================
File: tools/retrieval/llama_index_retrieval.py
================================================
"""Provides data for the agent."""

from typing import Any
from typing import TYPE_CHECKING

from ...events import Event
from ..tool_context import ToolContext
from .base_retrieval_tool import BaseRetrievalTool

if TYPE_CHECKING:
  from llama_index.core.base.base_retriever import BaseRetriever


class LlamaIndexRetrieval(BaseRetrievalTool):

  def __init__(self, *, name: str, description: str, retriever: 'BaseRetriever'):
    super().__init__(name, description)
    self.retriever = retriever

  def call(
      self,
      *,
      args: dict[str, Any],
      tool_context: ToolContext,
  ) -> Event:
    return self.retriever.retrieve(args['query'])[0].text


================================================
File: tools/retrieval/vertex_rag_retrieval.py
================================================
"""A retrieval tool that uses Vertex RAG to retrieve data."""

import logging
from typing import Any

from vertexai.preview import rag

from ..tool_context import ToolContext
from . import BaseRetrievalTool

logger = logging.getLogger(__name__)


class VertexRagRetrieval(BaseRetrievalTool):
  """A retrieval tool that uses Vertex RAG (Retrieval-Augmented Generation) to retrieve data."""

  def __init__(
      self,
      *,
      name: str,
      description: str,
      retrieval_config: dict[str, Any] = {},
  ):
    super().__init__(name=name, description=description)
    self.retrieval_config = retrieval_config

  def call(
      self,
      *,
      args: dict[str, Any],
      tool_context: ToolContext,
  ) -> Any:

    response = rag.retrieval_query(text=args["query"], **self.retrieval_config)

    logging.debug("RAG raw response: %s", response)

    return (
        f"No matching result found with the config: {self.retrieval_config}"
        if not response.contexts.contexts
        else [context.text for context in response.contexts.contexts]
    )


